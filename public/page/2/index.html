<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head>
	<meta name="generator" content="Hugo 0.152.2"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>My work notes</title>

<meta name="description" content="è®°å½•è®ºæ–‡é˜…è¯»ä¸ä»£ç å®è·µ">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
  <meta property="og:description" content=" ğŸ“š æˆ‘çš„å·¥ä½œç¬”è®° è¿™é‡Œè®°å½•æˆ‘åœ¨äººå·¥æ™ºèƒ½ã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼€å‘å·¥å…·æ–¹é¢çš„å­¦ä¹ ä¸å®è·µã€‚ æ¬¢è¿ä¸€èµ·æ¢ç´¢æŠ€æœ¯çš„è¾¹ç•Œï¼ ğŸ§  RL ğŸ’¬ LLM ğŸ§° å·¥å…·ç®± ">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
<meta name="twitter:description" content="è®°å½•è®ºæ–‡é˜…è¯»ä¸ä»£ç å®è·µ">

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "My work notes",
  "url": "http://localhost:1313/",
  "description": "è®°å½•è®ºæ–‡é˜…è¯»ä¸ä»£ç å®è·µ",
  "logo": "http://localhost:1313/favicon.ico",
  "sameAs": [
      
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="å¤§è¯­è¨€æ¨¡å‹ (LLM)">
                    <span>å¤§è¯­è¨€æ¨¡å‹ (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="å·¥å…·ç®± (toolbox)">
                    <span>å·¥å…·ç®± (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="å¼ºåŒ–å­¦ä¹  (RL)">
                    <span>å¼ºåŒ–å­¦ä¹  (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
                    <span class="active">æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<div class="post-content"><div style="text-align: center; max-width: 700px; margin: 2rem auto; padding: 0 1rem;">
  <h2>ğŸ“š æˆ‘çš„å·¥ä½œç¬”è®°</h2>
  <p style="font-size: 1.1em; color: #555; line-height: 1.6;">
    è¿™é‡Œè®°å½•æˆ‘åœ¨äººå·¥æ™ºèƒ½ã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼€å‘å·¥å…·æ–¹é¢çš„å­¦ä¹ ä¸å®è·µã€‚
    æ¬¢è¿ä¸€èµ·æ¢ç´¢æŠ€æœ¯çš„è¾¹ç•Œï¼
  </p>
<div style="display: flex; justify-content: center; gap: 2rem; margin-top: 2rem; flex-wrap: wrap;">
  <a href="/rl/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    ğŸ§  RL
  </a>

  <a href="/llm/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    ğŸ’¬ LLM
  </a>

  <a href="/toolbox/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    ğŸ§° å·¥å…·ç®±
  </a>

</div>
  </div>
</div>


</div>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">ä½¿ç”¨ Mindspeed-LLMè¿›è¡Œæ¨ç†
    </h2>
  </header>
  <div class="entry-content">
    <p>æœ¬æ–‡è®°å½•äº† mindspeed-llm æ¡†æ¶æä¾›çš„æ¨ç†æ–¹æ³•ï¼Œä»¥ Qwen2.5-7B æ¨¡å‹ä¸ºä¾‹è¯´æ˜æ¨ç†è¿‡ç¨‹ã€‚
å‡†å¤‡å·¥ä½œ æ¨ç†æ¨¡å‹çš„æƒé‡éœ€è¦è½¬æ¢ä¸º mcore æ ¼å¼ï¼Œæ–¹æ³•ä¸å¾®è°ƒæ—¶ç›¸åŒï¼Œå‚è€ƒè¿™é‡Œ
å¼€å¯æ¨ç†æµ‹è¯• å‡†å¤‡å¥½æ¨¡å‹æƒé‡åï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ mindspeed-llm æä¾›çš„æ¨ç†è„šæœ¬è¿›è¡Œæ¨ç†æµ‹è¯•ï¼Œè„šæœ¬ä½äºexamples/mcore/qwen25/generate_qwen25_7b_ptd.sh
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 # please fill these path configurations CHECKPOINT=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B/mcore_tp1_pp1/&#34; TOKENIZER_PATH=&#34;/home/fdd/workspace/models/Qwen/Qwen2.5-7B-Instruct/&#34; # Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 NPUS_PER_NODE=1 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES)) TP=1 PP=1 SEQ_LENGTH=32768 ... torchrun $DISTRIBUTED_ARGS inference.py \ --use-mcore-models \ ... å¯ä»¥çœ‹åˆ°å¯¹äº 7B æ¨¡å‹ï¼Œé‡‡ç”¨å•å¡å°±å¯ä»¥è¿›è¡Œæ¨ç†ã€‚å¯åŠ¨æ–¹å¼ä¾ç„¶ä¸º torchrunï¼Œå¯åŠ¨è„šæœ¬ä¸º inference.pyã€‚
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-11-14 00:00:00 +0000 UTC'>November 14, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to ä½¿ç”¨ Mindspeed-LLMè¿›è¡Œæ¨ç†" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_inference/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">ä½¿ç”¨ Mindspeed-LLMè¿›è¡Œè®­ç»ƒ
    </h2>
  </header>
  <div class="entry-content">
    <p>æœ¬æŒ‡å—è®°å½•é‡‡ç”¨mindspeed-llmè¿›è¡Œè®­ç»ƒçš„æ­¥éª¤ï¼Œä»¥å¾®è°ƒä¸ºä¾‹ã€‚
å‡†å¤‡æ•°æ® å‡†å¤‡æ•°æ®é›†çš„jsonlæ–‡ä»¶ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œè‡³å°‘è¦åŒ…å«å…³é”®å­—promptå’Œresponseï¼Œå…¶ä¸­promptä¸ºè¾“å…¥ï¼Œresponseä¸ºè¾“å‡ºã€‚
æ³¨æ„ï¼šå¦‚æœæä¾›çš„jsonlæ–‡ä»¶é‡Œé¢æ²¡æœ‰åŒ…å«chat templateï¼Œé‚£ä¹ˆåœ¨è½¬æ¢æ•°æ®çš„æ—¶å€™è¦æ·»åŠ prompt-typeå‚æ•°æ¥æä¾›æ¨¡æ¿ã€‚ä¸‹é¢æä¾›ä¸€ä¸ªå¸¦äº†chat templatedçš„jsonlæ–‡ä»¶æ ·ä¾‹ï¼š
{&#34;prompt&#34;:&#34;&lt;|im_start|&gt;system\nYou are a helpful assistant. To answer the user\&#39;s question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;\/think&gt; and &lt;answer&gt; &lt;\/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;\/think&gt; &lt;answer&gt; answer here &lt;\/answer&gt;.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nLet $\\triangle ABC$ have circumcenter $O$ and incenter $I$ with $\\overline{IA}\\perp\\overline{OI}$, circumradius $13$, and inradius $6$. Find $AB\\cdot AC$.\nPlease reason step by step, and put your final answer within \\boxed{}.\nPlease reason step by step, and put your final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#34;,&#34;response&#34;:&#34;&lt;think&gt;\nI have a geometry problem. We have a triangle ABC with circumcenter O and incenter I....&#34;} {&#34;prompt&#34;:&#34;&lt;|im_start|&gt;system\nYou are a helpful assistant. To answer the user\&#39;s question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;\/think&gt; and &lt;answer&gt; &lt;\/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;\/think&gt; &lt;answer&gt; answer here &lt;\/answer&gt;.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nLet \\(b\\ge 2\\) be an integer. Call a positive integer \\(n\\) \\(b\\text-\\textit{eautiful}\\) if it has exactly two digits when expressed in base \\(b\\) and these two digits sum to \\(\\sqrt n\\). For example, \\(81\\) is \\(13\\text-\\textit{eautiful}\\) because \\(81 = \\underline{6} \\ \\underline{3}_{13} \\) and \\(6 &#43; 3 = \\sqrt{81}\\). Find the least integer \\(b\\ge 2\\) for which there are more than ten \\(b\\text-\\textit{eautiful}\\) integers.\nPlease reason step by step, and put your final answer within \\boxed{}.\nPlease reason step by step, and put your final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#34;,&#34;response&#34;:&#34;&lt;think&gt;\nThe problem defines a \&#34;b-beautiful\&#34; number as a positive integer...&#34;} å‡†å¤‡å¥½jsonlæ–‡ä»¶åï¼Œåˆ©ç”¨mindspeed-llmè‡ªå¸¦çš„è„šæœ¬preprocess_data.pyæ¥é¢„å¤„ç†æ•°æ®ï¼š
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-11-14 00:00:00 +0000 UTC'>November 14, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to ä½¿ç”¨ Mindspeed-LLMè¿›è¡Œè®­ç»ƒ" href="http://localhost:1313/llm/mindspeed_llm/mindspeed_llm_finetune/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ
    </h2>
  </header>
  <div class="entry-content">
    <p>æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚
ç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯
Mindspeed commit ID: 89f4632d Megatron åˆ†æ”¯: core_v0.12.1 CANN: 8.2.RC1 PyTorch: 2.5.1 1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import torch import mindspeed.megatron_adaptor # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹ from megatron.core import parallel_state def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1): # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰ parallel_state.destroy_model_parallel() # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½® rank = int(os.environ.get(&#39;LOCAL_RANK&#39;, 0)) world_size = int(os.environ.get(&#34;WORLD_SIZE&#34;, 1)) torch.cuda.set_device(rank) torch.distributed.init_process_group(world_size=world_size, rank=rank) # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€ parallel_state.initialize_model_parallel( tensor_model_parallel_size=tensor_model_parallel_size, pipeline_model_parallel_size=pipeline_model_parallel_size ) å…³é”®ç‚¹è¯´æ˜ï¼š å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚ é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚ éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-11-11 00:00:00 +0000 UTC'>November 11, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ" href="http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">MindspeedåŸºæœ¬ä»‹ç»
    </h2>
  </header>
  <div class="entry-content">
    <p>MindSpeedçš„æ ¸å¿ƒç›®çš„æ˜¯é«˜æ•ˆåœ°é€‚é…å’ŒåŠ é€ŸåŸºäºMegatron-LMçš„å¤§æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åä¸ºæ˜‡è…¾AIç¡¬ä»¶ä¸Šè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚å…¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
æ¡†æ¶å…¼å®¹æ€§ï¼šå°†åŸç”ŸMegatron-LMä»£ç é€‚é…åˆ°æ˜‡è…¾NPUæ¶æ„ï¼Œç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§ã€‚ æ€§èƒ½ä¼˜åŒ–ï¼šé€šè¿‡ç®—å­èåˆã€å†…å­˜ä¼˜åŒ–å’Œé€šä¿¡åŠ é€Ÿç­‰æŠ€æœ¯æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚ å¤šçº§åŠ é€Ÿï¼šæä¾›å¯é…ç½®çš„ä¼˜åŒ–å±‚çº§ï¼ˆå¦‚åŸºç¡€å…¼å®¹æ€§ã€äº²å’Œæ€§å¢å¼ºã€å…¨é¢åŠ é€Ÿï¼‰ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€æ±‚å¯ç”¨ã€‚ ç”Ÿæ€é›†æˆï¼šä¸CANNå’ŒMindSporeç­‰æ˜‡è…¾è½¯ä»¶æ ˆæ·±åº¦é›†æˆï¼Œå®ç°æ— ç¼çš„è½¯ç¡¬ä»¶ååŒä¼˜åŒ–ã€‚ ç®€è€Œè¨€ä¹‹ï¼ŒMindSpeedä½¿åŸºäºMegatronçš„å¤§æ¨¡å‹èƒ½å¤Ÿåœ¨æ˜‡è…¾è®¾å¤‡ä¸Šæ­£ç¡®è¿è¡Œã€å¿«é€Ÿè¿è¡Œå’Œå¯é è¿è¡Œã€‚
ä¸€è¡Œä»£ç é€‚é…Megatronä»£ç  1 2 import torch import mindspeed.megatron_adaptor # æ–°å¢ä»£ç è¡Œ åŠ é€Ÿç‰¹æ€§å±‚çº§è¯´æ˜ MindSpeed CoreåŠ é€Ÿç‰¹æ€§åˆ†ä¸ºä¸‰ä¸ªå±‚çº§ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡åœ¨å¯åŠ¨è„šæœ¬ä¸­è®¾ç½®--optimization-level {level}å‚æ•°æ¥æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©è¦å¯ç”¨çš„ä¼˜åŒ–çº§åˆ«ã€‚è¯¥å‚æ•°æ”¯æŒä»¥ä¸‹é…ç½®ï¼š
ç­‰çº§ ç­‰çº§åç§° æè¿° 0 åŸºç¡€åŠŸèƒ½å…¼å®¹æ€§ æä¾›Megatron-LMæ¡†æ¶ä¸NPUçš„åŸºç¡€åŠŸèƒ½å…¼å®¹æ€§ã€‚ 1 äº²å’Œæ€§å¢å¼º ğŸ”¥ åœ¨L0åŸºç¡€ä¸Šï¼Œå¯ç”¨éƒ¨åˆ†èåˆç®—å­å’Œæ˜‡è…¾å‹å¥½çš„è®¡ç®—é‡å†™ã€‚ 2 åŠ é€Ÿç‰¹æ€§å¢å¼º ğŸ”¥ğŸ”¥ é»˜è®¤å€¼ã€‚åœ¨L0å’ŒL1åŸºç¡€ä¸Šå¯ç”¨æ›´ä¸°å¯Œçš„åŠ é€Ÿç‰¹æ€§ï¼ŒåŠ é€Ÿç‰¹æ€§é€šå¸¸é€šè¿‡ç‰¹å®šå‚æ•°å¯ç”¨ã€‚è¯¦æƒ…è¯·å‚é˜…&#34;ç‰¹æ€§ä»‹ç»&#34;éƒ¨åˆ†ã€‚ MindSpeed Coreç”Ÿæ€ç³»ç»Ÿ åœ¨MindSpeed CoreåŠ é€Ÿåº“çš„åŸºç¡€ä¸Šï¼Œè¿˜æä¾›äº†ä»¥ä¸‹ä¸“ä¸šåº“ï¼š
å¤§è¯­è¨€æ¨¡å‹åº“: MindSpeed LLM å¤šæ¨¡æ€æ¨¡å‹åº“: MindSpeed MM å¼ºåŒ–å­¦ä¹ åŠ é€Ÿåº“: MindSpeed RL </p>
  </div>
  <footer class="entry-footer"><span title='2025-11-10 00:00:00 +0000 UTC'>November 10, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to MindspeedåŸºæœ¬ä»‹ç»" href="http://localhost:1313/llm/mindspeed/introduction_with_mindspeed/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³
    </h2>
  </header>
  <div class="entry-content">
    <p>KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚
ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ 1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨ å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š
è¾“å…¥: &#34;ä»Šå¤©å¤©æ°”&#34; ç¬¬1æ­¥è¾“å‡º: &#34;çœŸ&#34; è¾“å…¥å˜ä¸º: &#34;ä»Šå¤©å¤©æ°”çœŸ&#34; ç¬¬2æ­¥è¾“å‡º: &#34;å¥½&#34; ... 2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚
å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t&#43;1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t&#43;1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š
ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰ â€¦ ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼ è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚
3. KV Cache çš„æå‡º ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-13 00:00:00 +0000 UTC'>January 13, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³" href="http://localhost:1313/llm/architecture/kv-cache/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Mamba - A Transformer-like Architecture for Long Sequence Modeling
    </h2>
  </header>
  <div class="entry-content">
    <p>ä¸€ã€Mamba çš„è®¾è®¡æ€æƒ³ èƒŒæ™¯ï¼šTransformer çš„å±€é™æ€§ è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(L^2)$ï¼ˆL ä¸ºåºåˆ—é•¿åº¦ï¼‰ï¼Œå¯¹é•¿åºåˆ—ï¼ˆå¦‚åŸºå› ç»„ã€é«˜åˆ†è¾¨ç‡éŸ³é¢‘ã€é•¿æ–‡æœ¬ï¼‰æ•ˆç‡ä½ã€‚ çº¿æ€§æ³¨æ„åŠ›ç­‰è¿‘ä¼¼æ–¹æ³•ç‰ºç‰²äº†å»ºæ¨¡èƒ½åŠ›ã€‚ RNN / SSMï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰ å…·æœ‰çº¿æ€§å¤æ‚åº¦ $O(L)$ï¼Œä½†ä¼ ç»Ÿ SSMï¼ˆå¦‚ Linear Time-Invariant SSM, LTI-SSMï¼‰æ˜¯æ—¶ä¸å˜çš„ï¼Œæ— æ³•åƒ Transformer é‚£æ ·æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€è°ƒæ•´è¡Œä¸ºã€‚ Mamba çš„æ ¸å¿ƒåˆ›æ–° Selective State Space Model (SSM) â€”â€” å°† SSM ä¸è¾“å…¥ç›¸å…³ï¼ˆinput-dependentï¼‰ï¼Œä½¿å…¶å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§å¤æ‚åº¦ã€‚
å…³é”®ç‚¹ï¼š
é€‰æ‹©æ€§ï¼ˆSelectivityï¼‰ï¼šSSM çš„å‚æ•°ï¼ˆå¦‚ A, B, Cï¼‰ä¸å†æ˜¯å›ºå®šæˆ–ä»…æ—¶é—´ç›¸å…³çš„ï¼Œè€Œæ˜¯ç”±å½“å‰è¾“å…¥ token åŠ¨æ€ç”Ÿæˆã€‚ ç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ï¼šåˆ©ç”¨ç°ä»£ GPU çš„å¹¶è¡Œç‰¹æ€§ï¼Œé€šè¿‡â€œæ‰«æï¼ˆscanï¼‰&#43; å¹¶è¡Œå‰ç¼€â€å®ç°é«˜æ•ˆè®­ç»ƒã€‚ ç»“æ„ç®€å•ï¼šæ²¡æœ‰ attentionï¼Œåªæœ‰ MLP &#43; SSM blockã€‚ äºŒã€Mamba æ¶æ„æ¦‚è§ˆ Mamba Block æ›¿ä»£äº† Transformer ä¸­çš„ Attention &#43; MLP å­å±‚ï¼š
Input â†’ LayerNorm â†’ SSM (Selective SSM) â†’ Residual Add â†’ LayerNorm â†’ MLP â†’ Residual Add â†’ Output å…¶ä¸­æœ€æ ¸å¿ƒçš„æ˜¯ Selective SSM æ¨¡å—ã€‚
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-13 00:00:00 +0000 UTC'>January 13, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to Mamba - A Transformer-like Architecture for Long Sequence Modeling" href="http://localhost:1313/llm/architecture/mamba/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GDN - Qwen3-Nextä¸­å¼•å…¥çš„ä¸€ç§çŠ¶æ€ç©ºé—´æ¨¡å‹å˜ä½“
    </h2>
  </header>
  <div class="entry-content">
    <p>Ref: çŸ¥ä¹
Gated DeltaNet æ˜¯ Qwen3-Nextï¼ˆé€šä¹‰åƒé—® 3 çš„ä¸‹ä¸€ä»£æ¨¡å‹ï¼‰ä¸­å¼•å…¥çš„ä¸€ç§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆState Space Model, SSMï¼‰å˜ä½“ï¼Œç”¨äºå¢å¼º Transformer æ¶æ„åœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­çš„èƒ½åŠ›ã€‚å®ƒèåˆäº† Delta Ruleï¼ˆå¢é‡å­¦ä¹ è§„åˆ™ï¼‰ã€é—¨æ§æœºåˆ¶ï¼ˆGatingï¼‰ å’Œ çŠ¶æ€ç©ºé—´å»ºæ¨¡æ€æƒ³ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡æ—¶çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚
ä¸‹é¢æˆ‘ä»¬å°†ä»ç”±æ¥ã€æ¼”å˜ã€é›†æˆæ–¹å¼ã€æ•°å­¦å½¢å¼åˆ° PyTorch å®ç° ä¸€æ­¥æ­¥è§£æ Gated DeltaNetã€‚
ä¸€ã€ç”±æ¥ä¸åŠ¨æœº 1.1 èƒŒæ™¯ï¼šTransformer çš„å±€é™ æ ‡å‡† Transformer ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶å¤æ‚åº¦ä¸º $O(L^2)$ï¼Œå…¶ä¸­ $L$ æ˜¯åºåˆ—é•¿åº¦ã€‚ å¯¹äºè¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ 100K&#43; tokensï¼‰ï¼Œæ³¨æ„åŠ›æœºåˆ¶å˜å¾—ä¸å¯æ‰©å±•ã€‚ 1.2 çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„å…´èµ· SSMï¼ˆå¦‚ S4ã€S5ã€Mambaï¼‰æä¾›äº†ä¸€ç§çº¿æ€§å¤æ‚åº¦ $O(L)$ çš„åºåˆ—å»ºæ¨¡èŒƒå¼ã€‚ å…¶æ ¸å¿ƒæ˜¯ç»´æŠ¤ä¸€ä¸ªéšçŠ¶æ€ $h_t$ï¼Œé€šè¿‡é€’æ¨æ›´æ–°ï¼š $$ h_t = \bar{A} h_{t-1} &#43; \bar{B} x_t $$ $$ y_t = C h_t $$ å…¶ä¸­ $\bar{A}, \bar{B}$ æ˜¯ç¦»æ•£åŒ–åçš„ç³»ç»ŸçŸ©é˜µã€‚ 1.3 Delta Rule ä¸ Hebbian å­¦ä¹  Delta Rule æ˜¯ä¸€ç§å¢é‡å­¦ä¹ è§„åˆ™ï¼Œå½¢å¼ä¸ºï¼š $$ \Delta W \propto (y_{\text{target}} - y) x^\top $$ åœ¨ç¥ç»è®°å¿†æ¨¡å‹ä¸­ï¼ŒDelta Rule å¯è¢«è§£é‡Šä¸ºå¿«é€Ÿè”æƒ³è®°å¿†æ›´æ–°ï¼šæ–°è¾“å…¥ $x_t$ ä¸å½“å‰è¾“å‡ºè¯¯å·®å…±åŒé©±åŠ¨æƒé‡æ›´æ–°ã€‚ 1.4 Gated DeltaNet çš„æå‡º Gated DeltaNet å°† Delta Rule çš„æ€æƒ³åµŒå…¥åˆ° SSM æ¡†æ¶ä¸­ï¼Œå½¢æˆä¸€ç§å¯å­¦ä¹ çš„ã€é—¨æ§æ§åˆ¶çš„è®°å¿†æ›´æ–°æœºåˆ¶ã€‚ å®ƒä¸æ˜¯ç›´æ¥æ›´æ–°æƒé‡ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªé”®å€¼è®°å¿†çŸ©é˜µ $M_t$ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶è¿›è¡Œå¢é‡æ›´æ–°ã€‚ äºŒã€Gated DeltaNet çš„æ•°å­¦å½¢å¼ Gated DeltaNet çš„æ ¸å¿ƒæ˜¯ç»´æŠ¤ä¸€ä¸ªå¤–éƒ¨è®°å¿†çŸ©é˜µ $M_t \in \mathbb{R}^{d_k \times d_v}$ï¼Œå¹¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥é€šè¿‡è¾“å…¥ $x_t$ æ›´æ–°å®ƒã€‚
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-09 00:00:00 +0000 UTC'>January 9, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to GDN - Qwen3-Nextä¸­å¼•å…¥çš„ä¸€ç§çŠ¶æ€ç©ºé—´æ¨¡å‹å˜ä½“" href="http://localhost:1313/llm/architecture/gdn/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">mHC - Manifold-Constrained Hyper-Connections æµå½¢çº¦æŸè¶…é“¾æ¥
    </h2>
  </header>
  <div class="entry-content">
    <p>Ref: mHC: Manifold-Constrained Hyper-Connections Introduction è‡ªä»ResNetsçš„æå‡ºï¼Œæ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ç»å†äº†å¿«é€Ÿçš„æ¼”è¿›ã€‚ä¸€ä¸ªå¸¸è§„çš„ResNetå±‚ç»“æœå¦‚ä¸Šé¢å›¾(a)æ‰€ç¤ºï¼Œå…¶æ•°å­¦è¡¨è¾¾å¼æè¿°ä¸ºï¼š
$$ \mathbf{x}_{l&#43;1} = \mathbf{x}_l &#43; \mathcal{F}(\mathbf{x}_l, W_l) \tag{1} $$
å…¶ä¸­$\mathbf{x}l$å’Œ$\mathbf{x}{l&#43;1}$è¡¨ç¤ºè¾“å…¥å’Œè¾“å‡ºå‘é‡ï¼Œæ˜¯ä¸€ä¸ª$C$ç»´åº¦çš„å‘é‡ã€‚$\mathcal{F}$è¡¨ç¤ºæ®‹å·®å‡½æ•°ï¼Œ$W_l$è¡¨ç¤ºæƒé‡çŸ©é˜µã€‚å°½ç®¡æ®‹å·®å‡½æ•°Fåœ¨è¿‡å»åå¹´ä¸­å·²å‘å±•å‡ºå·ç§¯ã€æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œç­‰å¤šç§æ“ä½œï¼Œä½†æ®‹å·®è¿æ¥çš„èŒƒå¼ä»ä¿æŒå…¶åŸå§‹å½¢å¼ã€‚éšç€Transformeræ¶æ„çš„å‘å±•ï¼Œè¯¥èŒƒå¼ç›®å‰å·²ç¡®ç«‹ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºæœ¬è®¾è®¡å…ƒç´ ã€‚
è¿™ä¸€æˆåŠŸä¸»è¦å½’åŠŸäºæ®‹å·®è¿æ¥çš„ç®€æ´å½¢å¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ—©æœŸç ”ç©¶æ­ç¤ºäº†æ®‹å·®è¿æ¥çš„æ’ç­‰æ˜ å°„ç‰¹æ€§åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä¿æŒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡é€’å½’æ‰©å±•æ®‹å·®è¿æ¥è·¨è¶Šå¤šå±‚ï¼Œç­‰å¼ï¼ˆ1ï¼‰å¾—å‡ºï¼š
$$ \mathbf{x}_{L} = \mathbf{x}l &#43; \sum{i=l}^{L-1} \mathcal{F}(\mathbf{x}_l, W_l) \tag{2} $$
å…¶ä¸­$L$å’Œ$l$åˆ†åˆ«å¯¹åº”è¾ƒæ·±å’Œè¾ƒæµ…çš„å±‚ã€‚æ’ç­‰æ˜ å°„é¡¹æŒ‡çš„æ˜¯ç»„ä»¶$x_l$æœ¬èº«ï¼Œå¼ºè°ƒäº†æ¥è‡ªè¾ƒæµ…å±‚çš„ä¿¡å·æœªç»ä»»ä½•ä¿®æ”¹ç›´æ¥æ˜ å°„åˆ°è¾ƒæ·±å±‚çš„ç‰¹æ€§ã€‚
è¿‘æœŸï¼Œä»¥è¶…è¿æ¥ï¼ˆHyper-Connectionsï¼ŒHCï¼‰ï¼ˆZhu ç­‰äººï¼Œ2024ï¼‰ä¸ºä»£è¡¨çš„ç ”ç©¶ä¸ºæ®‹å·®è¿æ¥å¼•å…¥äº†æ–°ç»´åº¦ï¼Œå¹¶é€šè¿‡å®è¯éªŒè¯äº†å…¶æ€§èƒ½æ½œåŠ›ã€‚HCçš„å•å±‚æ¶æ„å¦‚å›¾1(b)æ‰€ç¤ºã€‚é€šè¿‡æ‰©å±•æ®‹å·®æµå®½åº¦å¹¶å¢å¼ºè¿æ¥å¤æ‚åº¦ï¼ŒHCåœ¨ä¸æ”¹å˜å•ä¸ªå•å…ƒæµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰è®¡ç®—å¼€é”€çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ‹“æ‰‘å¤æ‚åº¦ã€‚å½¢å¼åŒ–åœ°ï¼ŒHCä¸­çš„å•å±‚ä¼ æ’­å®šä¹‰ä¸ºï¼š
$$ \mathbf{x}_{l&#43;1} = \mathcal{H}_l^{\text{res}} \mathbf{x}_l &#43; \mathcal{H}_l^{\text{post}}{}^\top \mathcal{F}(\mathcal{H}_l^{\text{pre}} \mathbf{x}_l, W_l) \tag{3} $$
å…¶ä¸­ï¼Œ$\mathbf{x}l$ å’Œ $\mathbf{x}{l&#43;1}$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $l$ å±‚çš„è¾“å…¥å’Œè¾“å‡ºã€‚ä¸å…¬å¼ (1) ä¸­çš„è¡¨è¿°ä¸åŒï¼Œ$\mathbf{x}l$ å’Œ $\mathbf{x}{l&#43;1}$ çš„ç‰¹å¾ç»´åº¦ä» $C$ æ‰©å±•åˆ°äº† $n \times C$ï¼Œå…¶ä¸­ $n$ æ˜¯æ‰©å±•å€ç‡ï¼ˆexpansion rateï¼‰ã€‚é¡¹ $\mathcal{H}_l^{\text{res}} \in \mathbb{R}^{n \times n}$ è¡¨ç¤ºä¸€ä¸ªå¯å­¦ä¹ çš„æ˜ å°„ï¼Œç”¨äºåœ¨æ®‹å·®æµï¼ˆresidual streamï¼‰å†…éƒ¨æ··åˆç‰¹å¾ã€‚åŒæ ·ä½œä¸ºå¯å­¦ä¹ æ˜ å°„ï¼Œ$\mathcal{H}_l^{\text{pre}} \in \mathbb{R}^{1 \times n}$ å°†æ¥è‡ª $nC$ ç»´æµä¸­çš„ç‰¹å¾èšåˆä¸ºä¸€ä¸ª $C$ ç»´çš„å±‚è¾“å…¥ï¼›åä¹‹ï¼Œ$\mathcal{H}_l^{\text{post}} \in \mathbb{R}^{1 \times n}$ å°†å±‚çš„è¾“å‡ºé‡æ–°æ˜ å°„å›åŸå§‹æµä¸­ã€‚
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-08 00:00:00 +0000 UTC'>January 8, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to mHC - Manifold-Constrained Hyper-Connections æµå½¢çº¦æŸè¶…é“¾æ¥" href="http://localhost:1313/llm/architecture/mhc/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Deploying vLLM in Production
    </h2>
  </header>
  <div class="entry-content">
    <p>Getting Started with vLLM Deployment Deploying large language models efficiently requires careful consideration of hardware resources and configuration parameters.
Installation Installing vLLM is straightforward using pip:
1 pip install vllm </p>
  </div>
  <footer class="entry-footer"><span title='2024-01-20 00:00:00 +0000 UTC'>January 20, 2024</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to Deploying vLLM in Production" href="http://localhost:1313/llm/vllm/deployment/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">vLLM Optimization Techniques
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction In this post, weâ€™ll explore the key optimization techniques that make vLLM one of the most efficient inference engines for large language models.
Key Optimizations PagedAttention PagedAttention is vLLMâ€™s core innovation that efficiently manages the GPU memory for attention keys and values. This technique significantly reduces memory fragmentation and allows for serving more requests simultaneously.
Continuous Batching Unlike traditional static batching, vLLM employs continuous batching which dynamically adds new requests to the batch as they arrive, improving throughput without increasing latency.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-01-15 00:00:00 +0000 UTC'>January 15, 2024</span>&nbsp;Â·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to vLLM Optimization Techniques" href="http://localhost:1313/llm/vllm/optimization/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/">
      Â«&nbsp;Prev&nbsp;
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
