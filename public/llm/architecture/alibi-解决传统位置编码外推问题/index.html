<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜ | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜
åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰ æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ è®­ç»ƒé•¿åº¦ &lt; æ¨ç†é•¿åº¦ æ—¶è¡¨ç°ä¸ä½³ï¼š

ç»å¯¹ä½ç½®ç¼–ç ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼•
RoPEï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚


ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼šä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ

2. ALiBi çš„æå‡º

è®ºæ–‡ï¼šã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹ï¼ˆICLR 2022ï¼‰
ä½œè€…ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰
æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨ç§»é™¤ä½ç½®ç¼–ç ï¼Œæ”¹ç”¨ ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰ ç›´æ¥åŠ åˆ° attention score ä¸Š


âœ… ä¼˜åŠ¿ï¼š

æ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰
æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰
åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
è¢« BLOOMï¼ˆ176Bï¼‰ ç­‰å¤§æ¨¡å‹é‡‡ç”¨



äºŒã€åŸºæœ¬åŸç†
ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š

äººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³ã€‚
å› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚

ä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª çº¿æ€§åç½®é¡¹ï¼š">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜">
  <meta property="og:description" content="ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ 1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜ åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰ æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ è®­ç»ƒé•¿åº¦ &lt; æ¨ç†é•¿åº¦ æ—¶è¡¨ç°ä¸ä½³ï¼š
ç»å¯¹ä½ç½®ç¼–ç ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼• RoPEï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚ ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼šä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ
2. ALiBi çš„æå‡º è®ºæ–‡ï¼šã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹ï¼ˆICLR 2022ï¼‰ ä½œè€…ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰ æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨ç§»é™¤ä½ç½®ç¼–ç ï¼Œæ”¹ç”¨ ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰ ç›´æ¥åŠ åˆ° attention score ä¸Š âœ… ä¼˜åŠ¿ï¼š
æ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰ æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰ åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ è¢« BLOOMï¼ˆ176Bï¼‰ ç­‰å¤§æ¨¡å‹é‡‡ç”¨ äºŒã€åŸºæœ¬åŸç† ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š
äººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³ã€‚
å› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚
ä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª çº¿æ€§åç½®é¡¹ï¼š">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-04T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜">
<meta name="twitter:description" content="ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜
åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰ æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ è®­ç»ƒé•¿åº¦ &lt; æ¨ç†é•¿åº¦ æ—¶è¡¨ç°ä¸ä½³ï¼š

ç»å¯¹ä½ç½®ç¼–ç ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼•
RoPEï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚


ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼šä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ

2. ALiBi çš„æå‡º

è®ºæ–‡ï¼šã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹ï¼ˆICLR 2022ï¼‰
ä½œè€…ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰
æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨ç§»é™¤ä½ç½®ç¼–ç ï¼Œæ”¹ç”¨ ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰ ç›´æ¥åŠ åˆ° attention score ä¸Š


âœ… ä¼˜åŠ¿ï¼š

æ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰
æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰
åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
è¢« BLOOMï¼ˆ176Bï¼‰ ç­‰å¤§æ¨¡å‹é‡‡ç”¨



äºŒã€åŸºæœ¬åŸç†
ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š

äººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³ã€‚
å› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚

ä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª çº¿æ€§åç½®é¡¹ï¼š">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "å¤§è¯­è¨€æ¨¡å‹ (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜",
      "item": "http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜",
  "name": "ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜",
  "description": "ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ 1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜ åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰ æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ è®­ç»ƒé•¿åº¦ \u0026lt; æ¨ç†é•¿åº¦ æ—¶è¡¨ç°ä¸ä½³ï¼š\nç»å¯¹ä½ç½®ç¼–ç ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼• RoPEï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚ ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼šä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ\n2. ALiBi çš„æå‡º è®ºæ–‡ï¼šã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹ï¼ˆICLR 2022ï¼‰ ä½œè€…ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰ æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨ç§»é™¤ä½ç½®ç¼–ç ï¼Œæ”¹ç”¨ ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰ ç›´æ¥åŠ åˆ° attention score ä¸Š âœ… ä¼˜åŠ¿ï¼š\næ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰ æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰ åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ è¢« BLOOMï¼ˆ176Bï¼‰ ç­‰å¤§æ¨¡å‹é‡‡ç”¨ äºŒã€åŸºæœ¬åŸç† ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š\näººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³ã€‚\nå› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚\nä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª çº¿æ€§åç½®é¡¹ï¼š\n",
  "keywords": [
    
  ],
  "articleBody": "ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ 1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜ åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰ æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ è®­ç»ƒé•¿åº¦ \u003c æ¨ç†é•¿åº¦ æ—¶è¡¨ç°ä¸ä½³ï¼š\nç»å¯¹ä½ç½®ç¼–ç ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼• RoPEï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚ ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼šä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ\n2. ALiBi çš„æå‡º è®ºæ–‡ï¼šã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹ï¼ˆICLR 2022ï¼‰ ä½œè€…ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰ æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨ç§»é™¤ä½ç½®ç¼–ç ï¼Œæ”¹ç”¨ ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰ ç›´æ¥åŠ åˆ° attention score ä¸Š âœ… ä¼˜åŠ¿ï¼š\næ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰ æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰ åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ è¢« BLOOMï¼ˆ176Bï¼‰ ç­‰å¤§æ¨¡å‹é‡‡ç”¨ äºŒã€åŸºæœ¬åŸç† ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š\näººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³ã€‚\nå› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚\nä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª çº¿æ€§åç½®é¡¹ï¼š\n[ \\text{score}_{ij} = \\frac{Q_i K_j^\\top}{\\sqrt{d}} - m_h \\cdot |i - j| ]\nå…¶ä¸­ï¼š\n(i, j) æ˜¯ token ä½ç½®ï¼ˆ(i) ä¸º query ä½ç½®ï¼Œ(j) ä¸º key ä½ç½®ï¼‰ (m_h \u003e 0) æ˜¯ç¬¬ (h) ä¸ª head çš„ è¡°å‡æ–œç‡ï¼ˆslopeï¼‰ æ³¨æ„ï¼šå³ä½¿ (i \u003c j)ï¼ˆæœªæ¥ tokenï¼‰ï¼Œåç½®ä»ä¸ºè´Ÿï¼ˆä½†åœ¨ decoder-only ä¸­ä¼šè¢« causal mask å±è”½ï¼‰ ğŸ”‘ å…³é”®ç‚¹ï¼šä¸ä¾èµ–ä»»ä½•ä½ç½®åµŒå…¥ï¼Œä»…é è·ç¦» (|i-j|) å’Œå¯å­¦ä¹ ï¼ˆæˆ–é¢„è®¾ï¼‰çš„æ–œç‡æ§åˆ¶æ³¨æ„åŠ›èŒƒå›´ã€‚\nä¸‰ã€æ•°å­¦ç»†èŠ‚ 1. æ³¨æ„åŠ›è®¡ç®—ï¼ˆä»¥ decoder-only ä¸ºä¾‹ï¼‰ æ ‡å‡† causal attentionï¼š [ A_{ij} = \\begin{cases} \\text{softmax}\\left( \\frac{Q_i K_j^\\top}{\\sqrt{d}} \\right), \u0026 j \\leq i \\ 0, \u0026 j \u003e i \\end{cases} ]\nALiBi ä¿®æ”¹ä¸ºï¼š [ A_{ij} = \\begin{cases} \\text{softmax}\\left( \\frac{Q_i K_j^\\top}{\\sqrt{d}} - m_h \\cdot (i - j) \\right), \u0026 j \\leq i \\ 0, \u0026 j \u003e i \\end{cases} ]\nğŸ“Œ å› ä¸º (j \\leq i)ï¼Œæ‰€ä»¥ (|i - j| = i - j)ï¼Œåç½®ä¸º (-m_h (i - j))\n2. æ–œç‡ (m_h) çš„è®¾ç½® è®ºæ–‡å‘ç°ï¼šä¸åŒ head åº”å…³æ³¨ä¸åŒå°ºåº¦çš„ä¸Šä¸‹æ–‡ï¼ˆæœ‰çš„çœ‹è¿‘ï¼Œæœ‰çš„çœ‹è¿œï¼‰ã€‚\nå› æ­¤ï¼Œå°† heads åˆ†ç»„ï¼ŒæŒ‰æŒ‡æ•°è¡°å‡åˆ†é…æ–œç‡ï¼š\n[ m_h = 2^{-\\frac{8h}{H}}, \\quad h = 1, 2, â€¦, H ]\nä¾‹å¦‚ï¼Œå½“ (H = 8)ï¼š\nhead 0: (m = 2^{-1} = 0.5) head 1: (m = 2^{-2} = 0.25) â€¦ head 7: (m = 2^{-8} \\approx 0.0039) âœ… è¿™æ ·ï¼Œéƒ¨åˆ† head å…³æ³¨å±€éƒ¨ï¼ˆå¤§æ–œç‡ï¼‰ï¼Œéƒ¨åˆ† head å…³æ³¨å…¨å±€ï¼ˆå°æ–œç‡ï¼‰\nå››ã€PyTorch å®ç°ï¼ˆå¯è¿è¡Œæ¼”ç¤ºï¼‰ import torch import torch.nn as nn import torch.nn.functional as F def build_alibi_bias(num_heads: int, seq_len: int, dtype=torch.float32): \"\"\" æ„å»º ALiBi åç½®çŸ©é˜µã€‚ Args: num_heads: æ³¨æ„åŠ›å¤´æ•° seq_len: åºåˆ—é•¿åº¦ dtype: æ•°æ®ç±»å‹ Returns: bias: (num_heads, seq_len, seq_len) \"\"\" # é¢„å®šä¹‰æ–œç‡ m_h = 2^(-8h/H) slopes = torch.pow(2.0, -torch.arange(1, num_heads + 1, dtype=torch.float32) * 8.0 / num_heads) slopes = slopes.view(num_heads, 1, 1) # (H, 1, 1) # æ„å»ºè·ç¦»çŸ©é˜µ: d[i, j] = i - j (ä»…å¯¹ j \u003c= i æœ‰æ•ˆ) position_ids = torch.arange(seq_len, dtype=torch.float32) relative_position = position_ids[None, :] - position_ids[:, None] # (L, L) relative_position = relative_position.abs().unsqueeze(0) # (1, L, L) # ALiBi åç½® = -m_h * |i - j| alibi_bias = -slopes * relative_position # (H, L, L) # å¯¹äº decoder-onlyï¼Œåº”ç”¨ causal maskï¼ˆä¸Šä¸‰è§’è®¾ä¸º -infï¼‰ causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() alibi_bias.masked_fill_(causal_mask, float('-inf')) return alibi_bias.to(dtype) class ALiBiAttention(nn.Module): def __init__(self, embed_dim, num_heads, dropout=0.0): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\" self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False) self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False) self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False) self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False) self.dropout = dropout def forward(self, x): B, L, D = x.shape H = self.num_heads Dh = self.head_dim # æŠ•å½± Q, K, V q = self.q_proj(x).view(B, L, H, Dh).transpose(1, 2) # (B, H, L, Dh) k = self.k_proj(x).view(B, L, H, Dh).transpose(1, 2) # (B, H, L, Dh) v = self.v_proj(x).view(B, L, H, Dh).transpose(1, 2) # (B, H, L, Dh) # Scaled dot-product scores = torch.matmul(q, k.transpose(-2, -1)) / (Dh ** 0.5) # (B, H, L, L) # æ·»åŠ  ALiBi åç½® alibi_bias = build_alibi_bias(H, L, dtype=scores.dtype).to(scores.device) scores = scores + alibi_bias.unsqueeze(0) # å¹¿æ’­ batch ç»´åº¦ # Softmax + Dropout attn_weights = F.softmax(scores, dim=-1) attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training) # åŠ æƒæ±‚å’Œ output = torch.matmul(attn_weights, v) # (B, H, L, Dh) output = output.transpose(1, 2).contiguous().view(B, L, D) output = self.out_proj(output) return output # ------------------ æ¼”ç¤º ------------------ if __name__ == \"__main__\": torch.manual_seed(0) B, L, D = 2, 8, 128 H = 8 x = torch.randn(B, L, D) model = ALiBiAttention(embed_dim=D, num_heads=H) out = model(x) print(\"Input shape:\", x.shape) # [2, 8, 128] print(\"Output shape:\", out.shape) # [2, 8, 128] # å¯è§†åŒ– ALiBi åç½®ï¼ˆç¬¬ä¸€ä¸ª headï¼‰ bias = build_alibi_bias(H, L) print(\"\\nALiBi bias for head 0 (first 4x4):\") print(bias[0, :4, :4]) è¾“å‡ºç¤ºä¾‹ï¼ˆåç½®éƒ¨åˆ†ï¼‰ï¼š ALiBi bias for head 0 (first 4x4): tensor([[ 0.0000, -inf, -inf, -inf], [-0.5000, 0.0000, -inf, -inf], [-1.0000, -0.5000, 0.0000, -inf], [-1.5000, -1.0000, -0.5000, 0.0000]]) ğŸ” å¯è§ï¼š\nå¯¹è§’çº¿ä¸º 0ï¼ˆè‡ªå·±å¯¹é½è‡ªå·±ï¼‰ å·¦ä¸‹æ–¹ä¸ºè´Ÿå€¼ï¼Œä¸”éšè·ç¦»çº¿æ€§å‡å° ä¸Šä¸‰è§’ä¸º -infï¼ˆcausal maskï¼‰ äº”ã€ALiBi vs RoPEï¼šå…³é”®åŒºåˆ« ç‰¹æ€§ RoPE ALiBi æ˜¯å¦éœ€è¦ä½ç½®ç¼–ç  âœ… æ˜¯ï¼ˆæ—‹è½¬çŸ©é˜µï¼‰ âŒ å¦ å¤–æ¨èƒ½åŠ› ä¾èµ–æ’å€¼ï¼ˆå¦‚ YaRNï¼‰ å¤©ç„¶æ”¯æŒä»»æ„é•¿åº¦ è®¡ç®—å¼€é”€ éœ€è¦å¤æ•°ä¹˜æ³• ä»…åŠ åç½®ï¼ˆæä½ï¼‰ é€‚ç”¨åœºæ™¯ ä¸»æµ LLMï¼ˆLLaMA, Qwenï¼‰ BLOOMã€é•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹ å¯¹ç§°æ€§ æ”¯æŒåŒå‘ï¼ˆencoderï¼‰ é€šå¸¸ç”¨äºå•å‘ï¼ˆdecoderï¼‰ å…­ã€æ€»ç»“ ALiBi = æ— ä½ç½®ç¼–ç  + è·ç¦»çº¿æ€§åç½® å…¬å¼ï¼š(\\text{score}_{ij} = \\frac{Q_i K_j^\\top}{\\sqrt{d}} - m_h \\cdot |i - j|) ä¼˜åŠ¿ï¼šè®­ç»ƒçŸ­ã€æµ‹è¯•é•¿ï¼›æ¶æ„ç®€æ´ï¼›æ¨ç†é«˜æ•ˆ åº”ç”¨ï¼šBLOOMï¼ˆ176B å‚æ•°ï¼‰ã€AI21 çš„ Jurassic æ¨¡å‹ ğŸ’¡ å¦‚æœä½ æ­£åœ¨è®¾è®¡ä¸€ä¸ªéœ€è¦ è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ\u003e32K tokensï¼‰ çš„æ¨¡å‹ï¼ŒALiBi æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚\n",
  "wordCount" : "706",
  "inLanguage": "en",
  "datePublished": "2026-01-04T00:00:00Z",
  "dateModified": "2026-01-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="å¤§è¯­è¨€æ¨¡å‹ (LLM)">
                    <span>å¤§è¯­è¨€æ¨¡å‹ (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="å·¥å…·ç®± (toolbox)">
                    <span>å·¥å…·ç®± (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="å¼ºåŒ–å­¦ä¹  (RL)">
                    <span>å¼ºåŒ–å­¦ä¹  (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
                    <span>æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false }
            ],
            trust: true,
            throwOnError: false
        });
        });
    </script>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜
    </h1>
    <div class="post-meta"><span title='2026-01-04 00:00:00 +0000 UTC'>January 4, 2026</span>&nbsp;Â·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><h2 id="ä¸€alibi-çš„ç”±æ¥ä¸ºä»€ä¹ˆéœ€è¦å®ƒ">ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#ä¸€alibi-çš„ç”±æ¥ä¸ºä»€ä¹ˆéœ€è¦å®ƒ">#</a></h2>
<h3 id="1-ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜">1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜<a hidden class="anchor" aria-hidden="true" href="#1-ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜">#</a></h3>
<p>åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ <strong>ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰</strong> æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ <strong>è®­ç»ƒé•¿åº¦ &lt; æ¨ç†é•¿åº¦</strong> æ—¶è¡¨ç°ä¸ä½³ï¼š</p>
<ul>
<li><strong>ç»å¯¹ä½ç½®ç¼–ç </strong>ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼•</li>
<li><strong>RoPE</strong>ï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚</li>
</ul>
<blockquote>
<p>ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼š<strong>ä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ</strong></p>
</blockquote>
<h3 id="2-alibi-çš„æå‡º">2. ALiBi çš„æå‡º<a hidden class="anchor" aria-hidden="true" href="#2-alibi-çš„æå‡º">#</a></h3>
<ul>
<li><strong>è®ºæ–‡</strong>ï¼šã€Š<a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>ã€‹ï¼ˆICLR 2022ï¼‰</li>
<li><strong>ä½œè€…</strong>ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰</li>
<li><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š<strong>å®Œå…¨ç§»é™¤ä½ç½®ç¼–ç </strong>ï¼Œæ”¹ç”¨ <strong>ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰</strong> ç›´æ¥åŠ åˆ° attention score ä¸Š</li>
</ul>
<blockquote>
<p>âœ… ä¼˜åŠ¿ï¼š</p>
<ul>
<li>æ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰</li>
<li>æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰</li>
<li>åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚</li>
<li>è¢« <strong>BLOOMï¼ˆ176Bï¼‰</strong> ç­‰å¤§æ¨¡å‹é‡‡ç”¨</li>
</ul>
</blockquote>
<hr>
<h2 id="äºŒåŸºæœ¬åŸç†">äºŒã€åŸºæœ¬åŸç†<a hidden class="anchor" aria-hidden="true" href="#äºŒåŸºæœ¬åŸç†">#</a></h2>
<p>ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š</p>
<blockquote>
<p><strong>äººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³</strong>ã€‚<br>
å› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº <strong>å±€éƒ¨æ€§ï¼ˆlocalityï¼‰</strong>ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚</p>
</blockquote>
<p>ä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª <strong>çº¿æ€§åç½®é¡¹</strong>ï¼š</p>
<p>[
\text{score}_{ij} = \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot |i - j|
]</p>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>(i, j) æ˜¯ token ä½ç½®ï¼ˆ(i) ä¸º query ä½ç½®ï¼Œ(j) ä¸º key ä½ç½®ï¼‰</li>
<li>(m_h &gt; 0) æ˜¯ç¬¬ (h) ä¸ª head çš„ <strong>è¡°å‡æ–œç‡ï¼ˆslopeï¼‰</strong></li>
<li><strong>æ³¨æ„</strong>ï¼šå³ä½¿ (i &lt; j)ï¼ˆæœªæ¥ tokenï¼‰ï¼Œåç½®ä»ä¸ºè´Ÿï¼ˆä½†åœ¨ decoder-only ä¸­ä¼šè¢« causal mask å±è”½ï¼‰</li>
</ul>
<blockquote>
<p>ğŸ”‘ å…³é”®ç‚¹ï¼š<strong>ä¸ä¾èµ–ä»»ä½•ä½ç½®åµŒå…¥</strong>ï¼Œä»…é è·ç¦» (|i-j|) å’Œå¯å­¦ä¹ ï¼ˆæˆ–é¢„è®¾ï¼‰çš„æ–œç‡æ§åˆ¶æ³¨æ„åŠ›èŒƒå›´ã€‚</p>
</blockquote>
<hr>
<h2 id="ä¸‰æ•°å­¦ç»†èŠ‚">ä¸‰ã€æ•°å­¦ç»†èŠ‚<a hidden class="anchor" aria-hidden="true" href="#ä¸‰æ•°å­¦ç»†èŠ‚">#</a></h2>
<h3 id="1-æ³¨æ„åŠ›è®¡ç®—ä»¥-decoder-only-ä¸ºä¾‹">1. æ³¨æ„åŠ›è®¡ç®—ï¼ˆä»¥ decoder-only ä¸ºä¾‹ï¼‰<a hidden class="anchor" aria-hidden="true" href="#1-æ³¨æ„åŠ›è®¡ç®—ä»¥-decoder-only-ä¸ºä¾‹">#</a></h3>
<p>æ ‡å‡† causal attentionï¼š
[
A_{ij} =
\begin{cases}
\text{softmax}\left( \frac{Q_i K_j^\top}{\sqrt{d}} \right), &amp; j \leq i \
0, &amp; j &gt; i
\end{cases}
]</p>
<p>ALiBi ä¿®æ”¹ä¸ºï¼š
[
A_{ij} =
\begin{cases}
\text{softmax}\left( \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot (i - j) \right), &amp; j \leq i \
0, &amp; j &gt; i
\end{cases}
]</p>
<blockquote>
<p>ğŸ“Œ å› ä¸º (j \leq i)ï¼Œæ‰€ä»¥ (|i - j| = i - j)ï¼Œåç½®ä¸º (-m_h (i - j))</p>
</blockquote>
<h3 id="2-æ–œç‡-m_h-çš„è®¾ç½®">2. æ–œç‡ (m_h) çš„è®¾ç½®<a hidden class="anchor" aria-hidden="true" href="#2-æ–œç‡-m_h-çš„è®¾ç½®">#</a></h3>
<p>è®ºæ–‡å‘ç°ï¼š<strong>ä¸åŒ head åº”å…³æ³¨ä¸åŒå°ºåº¦çš„ä¸Šä¸‹æ–‡</strong>ï¼ˆæœ‰çš„çœ‹è¿‘ï¼Œæœ‰çš„çœ‹è¿œï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œå°† heads åˆ†ç»„ï¼ŒæŒ‰æŒ‡æ•°è¡°å‡åˆ†é…æ–œç‡ï¼š</p>
<p>[
m_h = 2^{-\frac{8h}{H}}, \quad h = 1, 2, &hellip;, H
]</p>
<p>ä¾‹å¦‚ï¼Œå½“ (H = 8)ï¼š</p>
<ul>
<li>head 0: (m = 2^{-1} = 0.5)</li>
<li>head 1: (m = 2^{-2} = 0.25)</li>
<li>&hellip;</li>
<li>head 7: (m = 2^{-8} \approx 0.0039)</li>
</ul>
<blockquote>
<p>âœ… è¿™æ ·ï¼Œéƒ¨åˆ† head å…³æ³¨å±€éƒ¨ï¼ˆå¤§æ–œç‡ï¼‰ï¼Œéƒ¨åˆ† head å…³æ³¨å…¨å±€ï¼ˆå°æ–œç‡ï¼‰</p>
</blockquote>
<hr>
<h2 id="å››pytorch-å®ç°å¯è¿è¡Œæ¼”ç¤º">å››ã€PyTorch å®ç°ï¼ˆå¯è¿è¡Œæ¼”ç¤ºï¼‰<a hidden class="anchor" aria-hidden="true" href="#å››pytorch-å®ç°å¯è¿è¡Œæ¼”ç¤º">#</a></h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">build_alibi_bias</span><span class="p">(</span><span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    æ„å»º ALiBi åç½®çŸ©é˜µã€‚
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_heads: æ³¨æ„åŠ›å¤´æ•°
</span></span></span><span class="line"><span class="cl"><span class="s2">        seq_len: åºåˆ—é•¿åº¦
</span></span></span><span class="line"><span class="cl"><span class="s2">        dtype: æ•°æ®ç±»å‹
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        bias: (num_heads, seq_len, seq_len)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># é¢„å®šä¹‰æ–œç‡ m_h = 2^(-8h/H)</span>
</span></span><span class="line"><span class="cl">    <span class="n">slopes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">8.0</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">slopes</span> <span class="o">=</span> <span class="n">slopes</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (H, 1, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># æ„å»ºè·ç¦»çŸ©é˜µ: d[i, j] = i - j (ä»…å¯¹ j &lt;= i æœ‰æ•ˆ)</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">relative_position</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">position_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># (L, L)</span>
</span></span><span class="line"><span class="cl">    <span class="n">relative_position</span> <span class="o">=</span> <span class="n">relative_position</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># ALiBi åç½® = -m_h * |i - j|</span>
</span></span><span class="line"><span class="cl">    <span class="n">alibi_bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">slopes</span> <span class="o">*</span> <span class="n">relative_position</span>  <span class="c1"># (H, L, L)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># å¯¹äº decoder-onlyï¼Œåº”ç”¨ causal maskï¼ˆä¸Šä¸‰è§’è®¾ä¸º -infï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">alibi_bias</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">alibi_bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ALiBiAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&#34;embed_dim must be divisible by num_heads&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="n">Dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># æŠ•å½± Q, K, V</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Dh</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, H, L, Dh)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Dh</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, H, L, Dh)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Dh</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, H, L, Dh)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Scaled dot-product</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Dh</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># (B, H, L, L)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># æ·»åŠ  ALiBi åç½®</span>
</span></span><span class="line"><span class="cl">        <span class="n">alibi_bias</span> <span class="o">=</span> <span class="n">build_alibi_bias</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">alibi_bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># å¹¿æ’­ batch ç»´åº¦</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax + Dropout</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># åŠ æƒæ±‚å’Œ</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (B, H, L, Dh)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ------------------ æ¼”ç¤º ------------------</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl">    <span class="n">H</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">ALiBiAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">H</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Input shape:&#34;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>       <span class="c1"># [2, 8, 128]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Output shape:&#34;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>     <span class="c1"># [2, 8, 128]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># å¯è§†åŒ– ALiBi åç½®ï¼ˆç¬¬ä¸€ä¸ª headï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="n">bias</span> <span class="o">=</span> <span class="n">build_alibi_bias</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">ALiBi bias for head 0 (first 4x4):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="p">:</span><span class="mi">4</span><span class="p">])</span>
</span></span></code></pre></div><h3 id="è¾“å‡ºç¤ºä¾‹åç½®éƒ¨åˆ†">è¾“å‡ºç¤ºä¾‹ï¼ˆåç½®éƒ¨åˆ†ï¼‰ï¼š<a hidden class="anchor" aria-hidden="true" href="#è¾“å‡ºç¤ºä¾‹åç½®éƒ¨åˆ†">#</a></h3>
<pre tabindex="0"><code>ALiBi bias for head 0 (first 4x4):
tensor([[ 0.0000,    -inf,    -inf,    -inf],
        [-0.5000,  0.0000,    -inf,    -inf],
        [-1.0000, -0.5000,  0.0000,    -inf],
        [-1.5000, -1.0000, -0.5000,  0.0000]])
</code></pre><blockquote>
<p>ğŸ” å¯è§ï¼š</p>
<ul>
<li>å¯¹è§’çº¿ä¸º 0ï¼ˆè‡ªå·±å¯¹é½è‡ªå·±ï¼‰</li>
<li>å·¦ä¸‹æ–¹ä¸ºè´Ÿå€¼ï¼Œä¸”éšè·ç¦»çº¿æ€§å‡å°</li>
<li>ä¸Šä¸‰è§’ä¸º <code>-inf</code>ï¼ˆcausal maskï¼‰</li>
</ul>
</blockquote>
<hr>
<h2 id="äº”alibi-vs-ropeå…³é”®åŒºåˆ«">äº”ã€ALiBi vs RoPEï¼šå…³é”®åŒºåˆ«<a hidden class="anchor" aria-hidden="true" href="#äº”alibi-vs-ropeå…³é”®åŒºåˆ«">#</a></h2>
<table>
  <thead>
      <tr>
          <th>ç‰¹æ€§</th>
          <th>RoPE</th>
          <th>ALiBi</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>æ˜¯å¦éœ€è¦ä½ç½®ç¼–ç </td>
          <td>âœ… æ˜¯ï¼ˆæ—‹è½¬çŸ©é˜µï¼‰</td>
          <td>âŒ å¦</td>
      </tr>
      <tr>
          <td>å¤–æ¨èƒ½åŠ›</td>
          <td>ä¾èµ–æ’å€¼ï¼ˆå¦‚ YaRNï¼‰</td>
          <td><strong>å¤©ç„¶æ”¯æŒä»»æ„é•¿åº¦</strong></td>
      </tr>
      <tr>
          <td>è®¡ç®—å¼€é”€</td>
          <td>éœ€è¦å¤æ•°ä¹˜æ³•</td>
          <td>ä»…åŠ åç½®ï¼ˆæä½ï¼‰</td>
      </tr>
      <tr>
          <td>é€‚ç”¨åœºæ™¯</td>
          <td>ä¸»æµ LLMï¼ˆLLaMA, Qwenï¼‰</td>
          <td>BLOOMã€é•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹</td>
      </tr>
      <tr>
          <td>å¯¹ç§°æ€§</td>
          <td>æ”¯æŒåŒå‘ï¼ˆencoderï¼‰</td>
          <td>é€šå¸¸ç”¨äºå•å‘ï¼ˆdecoderï¼‰</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="å…­æ€»ç»“">å…­ã€æ€»ç»“<a hidden class="anchor" aria-hidden="true" href="#å…­æ€»ç»“">#</a></h2>
<ul>
<li><strong>ALiBi = æ— ä½ç½®ç¼–ç  + è·ç¦»çº¿æ€§åç½®</strong></li>
<li><strong>å…¬å¼</strong>ï¼š(\text{score}_{ij} = \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot |i - j|)</li>
<li><strong>ä¼˜åŠ¿</strong>ï¼šè®­ç»ƒçŸ­ã€æµ‹è¯•é•¿ï¼›æ¶æ„ç®€æ´ï¼›æ¨ç†é«˜æ•ˆ</li>
<li><strong>åº”ç”¨</strong>ï¼šBLOOMï¼ˆ176B å‚æ•°ï¼‰ã€AI21 çš„ Jurassic æ¨¡å‹</li>
</ul>
<blockquote>
<p>ğŸ’¡ å¦‚æœä½ æ­£åœ¨è®¾è®¡ä¸€ä¸ªéœ€è¦ <strong>è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ&gt;32K tokensï¼‰</strong> çš„æ¨¡å‹ï¼ŒALiBi æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
