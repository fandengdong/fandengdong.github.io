<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLM architecture | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="大语言模型的架构解析">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/llm/architecture/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="LLM architecture">
  <meta property="og:description" content="大语言模型的架构解析">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LLM architecture">
<meta name="twitter:description" content="大语言模型的架构解析">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "大语言模型 (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span>欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    LLM architecture
  </h1>
  <div class="post-description">
    大语言模型的架构解析
  </div>
</header>
<div class="post-content"><p>本节尝试将大语言模型的架构进行解析，并希望同时给出代码实现，已帮助更高的理解大语言模型的构建。此外，基于transformer架构，我们希望能够将架构的解读延续到当前的多模态大模型。</p>


</div>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RoPE - 旋转位置编码解读
    </h2>
  </header>
  <div class="entry-content">
    <p>旋转位置编码（RoPE, Rotary Position Embedding）是一种在Transformer模型中引入相对位置信息的方法，由苏剑林等人在2021年提出。相比传统的绝对位置编码（如BERT中的可学习位置嵌入或正弦位置编码），RoPE通过将位置信息以旋转变换的方式融入注意力机制，使得模型天然具备对相对位置的感知能力。
回顾原始位置编码 在Transformer模型中，位置编码（Position Encoding）用于将输入序列中的每个位置映射为向量，以表示其相对位置信息。原始位置编码的实现方式有很多种，如可学习位置嵌入（Learnable Position Embedding）或正弦位置编码（Sinusoidal Position Encoding）。
假设我们的词汇表大小为[vocab_size, embedding_dim]，输入句子的token长度为seq_len，每个token会被转换成一个vector向量，维度为[embedding_dim]，则整个输入句子的shape为[ seq_len, embedding_dim]。
但是，为了让模型理解序列的顺序，还需要加入位置编码(Positional Encoding)。这就是著名的公式：
$$ PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
$$ PE_{(pos,2i&#43;1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
pos是位置索引，即token在序列中的位置，从0开始； i是维度索引，从0开始到$d_{model}-1$结束，表示词嵌入的维度； $d_{model}$是词嵌入（word embedding）的维度embedding_dim 观察输入句子的shape: [seq_len, embedding_dim]，位置编码的时候，pos对应的是token在seq_len维度的位置，而维度索引i对应词嵌入embedding_dim的维度。
pytorch代码实现:
class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1).float() div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) self.register_buffer(&#39;pe&#39;, pe.unsqueeze(0)) def forward(self, x): return x &#43; self.pe[:, :x.size(1)] RoPE 的核心思想 相对位置 vs 绝对位置 在传统Transformer中，位置编码是加到词嵌入上的（如 x &#43; pos_emb），这种方式只保留了绝对位置信息。 而 RoPE 则通过将查询（Q）和键（K）向量进行旋转变换，使得注意力分数自然包含两个 token 之间的相对距离。 旋转操作 对于维度为 d 的向量，RoPE 将其分成若干二维子空间（通常是相邻两个维度一组），每组应用一个二维旋转矩阵：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to RoPE - 旋转位置编码解读" href="http://localhost:1313/llm/architecture/rope%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">一切的开始 - Transformer架构
    </h2>
  </header>
  <div class="entry-content">
    <p>Transformer的诞生背景 在深度学习的世界里，序列建模一直是个老大难问题。传统的循环神经网络(RNN)虽然能够处理序列数据，但在处理长序列时存在梯度消失和计算效率低下的问题。而卷积神经网络(CNN)虽然并行化程度高，但在捕捉长距离依赖关系方面力不从心。
就在这个关键时刻，Google的大神们在2017年扔出了一个&#34;核弹级&#34;的解决方案——《Attention Is All You Need》。这篇论文彻底颠覆了人们对序列建模的认知，提出了完全基于注意力机制的Transformer架构。
翻译任务：Transformer的试验场 Transformer最初是为了机器翻译任务而设计的。想象一下，你要把一句中文翻译成英文：
中文输入: “我喜欢学习人工智能”
英文输出: “I like studying artificial intelligence”
在这个过程中，模型需要理解每个词的含义，并找到它们之间的对应关系。传统的Seq2Seq模型依赖RNN来编码输入序列，但效果有限。而Transformer通过自注意力机制，能够同时关注输入序列中的所有位置，大大提升了翻译质量。
Transformer的基本输入输出 输入的秘密 Transformer的输入其实很简单，就是一个个token（可以是单词、子词或者字符）。假设我们的词汇表大小为[vocab_size, embedding_dim]，输入句子的token长度为seq_len，每个token会被转换成一个vector向量，维度为[embedding_dim]，则整个输入句子的shape为[ seq_len, embedding_dim]。
但是，为了让模型理解序列的顺序，还需要加入位置编码(Positional Encoding)。这就是著名的公式：
$$ PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
$$ PE_{(pos,2i&#43;1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
pos是位置索引，即token在序列中的位置，从0开始； i是维度索引，从0开始到$d_{model}-1$结束，表示词嵌入的维度； $d_{model}$是词嵌入（word embedding）的维度embedding_dim 观察输入句子的shape: [seq_len, embedding_dim]，位置编码的时候，pos对应的是token在seq_len维度的位置，而维度索引i对应词嵌入embedding_dim的维度。
输出的魅力 Transformer的输出同样是一系列token，但它还有一个特殊之处——它是一个概率分布。对于每个时间步，模型会输出词汇表中每个词的概率，然后通过贪婪搜索或束搜索来选择最合适的词。这些搜索算法构成了推理时扩展算法的基础。
补充贪婪搜索和束搜索的算法：
贪婪搜索是一种简单直接的解码策略。在每个时间步，它总是选择具有最高概率的token作为下一个输出，不考虑后续可能的影响。优点是计算简单，速度快，内存占用少；缺点则是容易陷入局部最优，生成结果缺乏多样性。 束搜索是对贪婪搜索的改进，在每个时间步保留beam_width个最有希望的候选序列，而不是只保留一个。相比贪婪搜索能找到更优的序列，在合理的时间内探索多个可能性；缺点则是计算复杂度较高，需要调整beam_width参数，仍可能错过全局最优解。 核心模块：Encoder和Decoder Transformer的核心由两大部分组成：Encoder和Decoder。
Encoder：信息的加工厂 Encoder负责将输入序列转换成一系列隐藏表示。它由N个相同的层堆叠而成，每一层都包含两个子层：
多头自注意力机制(Multi-Head Self-Attention) 位置全连接前馈网络(Position-wise Feed-Forward Networks) 每层都有残差连接和层归一化，这使得深层网络也能稳定训练。
Decoder：创作的艺术家 Decoder则负责根据Encoder的输出和之前生成的token来预测下一个token。它的结构比Encoder稍复杂一些，包含三个子层：
掩码多头自注意力机制(Masked Multi-Head Self-Attention) 多头注意力机制(Multi-Head Attention) 位置全连接前馈网络(Position-wise Feed-Forward Networks) 其中掩码机制确保在生成当前位置的输出时，只能看到之前的位置，不能&#34;偷看&#34;未来的信息。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 一切的开始 - Transformer架构" href="http://localhost:1313/llm/architecture/transformer/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">综述 - Transformer架构的演进
    </h2>
  </header>
  <div class="entry-content">
    <p>Transformer架构演进：从自注意力到长序列与多模态的革命性突破 Transformer自2017年提出以来，已成为深度学习领域的基石架构，其演进历程反映了AI研究者对计算效率、长序列处理能力和跨模态应用的不懈追求。从最初的O(n²)计算复杂度到如今的O(n)线性复杂度，从单模态文本处理到原生多模态架构，Transformer的演进不仅解决了自身局限，更推动了大语言模型、视觉模型和多模态系统的革命性发展。这些演进主要体现在注意力机制创新、位置编码改进、模型架构变体与扩展、训练推理效率优化四大方向，共同构建了现代AI系统的计算基础。
一、注意力机制的演进：从全局到稀疏再到状态空间模型 Transformer原始架构的核心是自注意力机制，它通过计算查询(Q)、键(K)和值(V)之间的相似度来捕捉序列内部的长距离依赖关系 。标准自注意力的计算公式为：
$$ \text{Attention}(Q,K,V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V $$
其中，$Q∈ℝ^{n×d_k}$，$K∈ℝ^{n×d_k}$，$V∈ℝ^{n×d_v}$，d_k是键向量的维度。这一机制虽然强大，但计算复杂度为O(n²)，随着序列长度n的增加呈平方增长，严重限制了处理长文本的能力。
为解决这一瓶颈，研究者从多个角度对注意力机制进行了创新：
1. 稀疏注意力机制
稀疏注意力通过限制每个token只能关注序列中的局部区域，将计算复杂度从O(n²)降至O(n)。主要代表包括：
Longformer：采用滑动窗口注意力机制，每个token仅关注其周围固定窗口内的token，同时保留少数全局token以维持上下文连贯性 。其窗口注意力公式为： $$ \text{WindowAttention}(Q,K,V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V_{\text{window}} $$
其中，V_window是窗口内值向量的子集，窗口大小通常为512或1024个token 。
BigBird：结合三种注意力模式——随机注意力（通过Erdős-Rényi图采样）、滑动窗口注意力和全局注意力 。随机注意力使模型能够捕捉长距离依赖，同时保持O(n)的线性复杂度。其随机注意力矩阵可以表示为： $$ \text{RandomAttention}(Q,K,V) = \text{softmax}\left( \frac{QW_{\text{random}}K^\top}{\sqrt{d_k}} \right)V_{\text{random}} $$
其中，$W_{random}$是随机掩码矩阵，$V_{random}$是随机采样值向量的子集 。
局部增强Mamba（LEVM）：在Mamba架构中，通过动态选择性处理，对重要信息保留完整状态，对不重要信息进行压缩，实现O(n)计算复杂度和O(1)显存占用 。其核心状态方程为： $$ h_t = \overline{A}h_{t-1} &#43; \overline{B}x_t $$
其中，$\overline{A}$和$\overline{B}$是动态生成的参数矩阵，允许模型根据输入选择性地保留信息 。
2. 线性注意力机制
线性注意力通过数学近似技术，将注意力计算的复杂度降至O(n)，同时保持模型的表达能力。主要代表包括：
Performer：采用核函数近似方法，将标准注意力公式中的softmax(QKᵀ)替换为核函数K(Q,K)，使得计算可以分解为O(n)的线性操作 。其核心公式为： $$ \text{LinearAttention}(Q,K,V) = \text{核函数}(Q,K)V $$
其中，核函数可以是高斯核、多项式核或其他类型的核函数 。
Mamba：将注意力机制替换为结构化状态空间模型（SSM），通过递归计算实现长序列处理 。Mamba在S4的基础上改进，引入选择性处理机制，允许模型根据输入动态生成参数$\overline{A}$, $\overline{B}$, Δ，从而实现对信息的选择性保留和压缩 。其状态方程为： $$ h_t = \overline{A}h_{t-1} &#43; \overline{B}x_t,\quad y_t = Ch_t $$
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 综述 - Transformer架构的演进" href="http://localhost:1313/llm/architecture/transformer%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
