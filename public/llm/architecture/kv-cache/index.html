<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³ | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚
ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ
1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨
å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š
è¾“å…¥: &#34;ä»Šå¤©å¤©æ°”&#34;
ç¬¬1æ­¥è¾“å‡º: &#34;çœŸ&#34;
è¾“å…¥å˜ä¸º: &#34;ä»Šå¤©å¤©æ°”çœŸ&#34;
ç¬¬2æ­¥è¾“å‡º: &#34;å¥½&#34;
...
2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜
Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚
å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t&#43;1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t&#43;1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š

ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV
ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰
&hellip;
ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼

è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚
3. KV Cache çš„æå‡º
ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/kv-cache/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/kv-cache/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/kv-cache/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³">
  <meta property="og:description" content="KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚
ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ 1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨ å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š
è¾“å…¥: &#34;ä»Šå¤©å¤©æ°”&#34; ç¬¬1æ­¥è¾“å‡º: &#34;çœŸ&#34; è¾“å…¥å˜ä¸º: &#34;ä»Šå¤©å¤©æ°”çœŸ&#34; ç¬¬2æ­¥è¾“å‡º: &#34;å¥½&#34; ... 2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚
å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t&#43;1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t&#43;1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š
ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰ â€¦ ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼ è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚
3. KV Cache çš„æå‡º ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-01-13T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-13T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³">
<meta name="twitter:description" content="KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚
ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ
1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨
å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š
è¾“å…¥: &#34;ä»Šå¤©å¤©æ°”&#34;
ç¬¬1æ­¥è¾“å‡º: &#34;çœŸ&#34;
è¾“å…¥å˜ä¸º: &#34;ä»Šå¤©å¤©æ°”çœŸ&#34;
ç¬¬2æ­¥è¾“å‡º: &#34;å¥½&#34;
...
2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜
Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚
å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t&#43;1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t&#43;1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š

ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV
ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰
&hellip;
ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼

è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚
3. KV Cache çš„æå‡º
ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "å¤§è¯­è¨€æ¨¡å‹ (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³",
      "item": "http://localhost:1313/llm/architecture/kv-cache/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³",
  "name": "KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³",
  "description": "KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚\nä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ 1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨ å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š\nè¾“å…¥: \u0026#34;ä»Šå¤©å¤©æ°”\u0026#34; ç¬¬1æ­¥è¾“å‡º: \u0026#34;çœŸ\u0026#34; è¾“å…¥å˜ä¸º: \u0026#34;ä»Šå¤©å¤©æ°”çœŸ\u0026#34; ç¬¬2æ­¥è¾“å‡º: \u0026#34;å¥½\u0026#34; ... 2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚\nå‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t+1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t+1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š\nç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰ \u0026hellip; ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼ è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚\n3. KV Cache çš„æå‡º ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š\n",
  "keywords": [
    
  ],
  "articleBody": "KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚\nä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ 1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨ å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š\nè¾“å…¥: \"ä»Šå¤©å¤©æ°”\" ç¬¬1æ­¥è¾“å‡º: \"çœŸ\" è¾“å…¥å˜ä¸º: \"ä»Šå¤©å¤©æ°”çœŸ\" ç¬¬2æ­¥è¾“å‡º: \"å¥½\" ... 2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ Transformer ä½¿ç”¨ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚\nå‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t+1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t+1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š\nç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰ â€¦ ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼ è¿™å¯¼è‡´ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚\n3. KV Cache çš„æå‡º ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡ã€‚å› ä¸ºï¼š\nåœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œå†å² token ä¸ä¼šæ”¹å˜ï¼› Attention å…¬å¼ä¸­ï¼Œå½“å‰ token çš„ Q åªéœ€ä¸æ‰€æœ‰å†å² Kã€V è®¡ç®—å³å¯ï¼› Q æ˜¯å½“å‰ token çš„è¡¨ç¤ºï¼Œå¿…é¡»å®æ—¶è®¡ç®—ï¼›ä½† Kã€V å¯ä»¥æå‰ç¼“å­˜ã€‚ äºæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥åªéœ€ï¼š\nè®¡ç®—å½“å‰ token çš„ Qï¼› å°†å†å²ç¼“å­˜çš„ Kã€V ä¸å½“å‰ Kã€V æ‹¼æ¥ï¼› æ‰§è¡Œä¸€æ¬¡ attentionã€‚ è¿™æ ·ï¼Œæ¯æ­¥è®¡ç®—é‡æ’å®šï¼ˆ$O(1)$ per stepï¼‰ï¼Œæ€»å¤æ‚åº¦ä» $O(n^2)$ é™ä¸º $O(n)$ã€‚\nâœ… KV Cache çš„æ ¸å¿ƒæ€æƒ³ï¼šç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œé¿å…é‡å¤è®¡ç®— Kã€Vã€‚\näºŒã€æŠ€æœ¯åŸç†è¯¦è§£ 1. æ ‡å‡† Self-Attention å…¬å¼å›é¡¾ å¯¹äºè¾“å…¥åºåˆ— $X \\in \\mathbb{R}^{n \\times d}$ï¼š\n$$ Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V $$ $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n2. è‡ªå›å½’ç”Ÿæˆä¸­çš„ KV Cache åº”ç”¨ è®¾å½“å‰å·²å¤„ç† token æ•°ä¸º $t$ï¼Œç¼“å­˜äº†ï¼š\n$K_{\\text{cache}} \\in \\mathbb{R}^{t \\times d_k}$ $V_{\\text{cache}} \\in \\mathbb{R}^{t \\times d_v}$ å½“è¾“å…¥æ–° token $x_{t+1}$ï¼ˆæˆ–åˆå§‹ prompt çš„ä¸‹ä¸€ä¸ª tokenï¼‰ï¼š\nè®¡ç®—å…¶ Qã€Kã€Vï¼š $$ q_{t+1} = x_{t+1} W_Q,\\quad k_{t+1} = x_{t+1} W_K,\\quad v_{t+1} = x_{t+1} W_V $$ æ›´æ–°ç¼“å­˜ï¼š $$ K_{\\text{new}} = [K_{\\text{cache}}; k_{t+1}],\\quad V_{\\text{new}} = [V_{\\text{cache}}; v_{t+1}] $$ è®¡ç®— attentionï¼š $$ \\text{attn} = \\text{softmax}\\left(\\frac{q_{t+1} K_{\\text{new}}^T}{\\sqrt{d_k}}\\right) V_{\\text{new}} $$ æ³¨æ„ï¼šQ åªéœ€å½“å‰ token çš„ï¼ˆå› ä¸ºæ˜¯ decoder-only æ¶æ„ï¼Œå¦‚ GPTï¼‰ï¼Œè€Œ Kã€V éœ€è¦å…¨éƒ¨å†å²ã€‚\næ³¨æ„ï¼šåœ¨è‡ªå›å½’çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç½‘ç»œä¸­æµé€šçš„tensorçš„shapeä¸ºï¼š[batch_size, 1, hidden_size]ï¼Œè€Œä¸æ˜¯è®­ç»ƒå½“ä¸­çš„[batch_size, seq_len, hidden_size]ã€‚\n3. å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ KV Cache æ¯ä¸ª attention head éƒ½æœ‰è‡ªå·±çš„ $W_K, W_V$ï¼Œå› æ­¤ KV Cache é€šå¸¸æ˜¯ shape: $(batch_size, num_heads, seq_len, head_dim)$ã€‚\nåœ¨å®ç°ä¸­ï¼Œå¸¸æŒ‰ head ç»´åº¦ç»„ç»‡ç¼“å­˜ã€‚\n4. å·¥ç¨‹ä¼˜åŒ– å†…å­˜ç®¡ç†ï¼šç¼“å­˜éšåºåˆ—å¢é•¿è€Œå¢é•¿ï¼Œéœ€æ³¨æ„æ˜¾å­˜é™åˆ¶ã€‚ PagedAttentionï¼ˆvLLMï¼‰ï¼šå°† KV Cache åˆ†é¡µå­˜å‚¨ï¼Œæé«˜å†…å­˜åˆ©ç”¨ç‡ã€‚ é‡åŒ– KV Cacheï¼šç”¨ int8/float16 å­˜å‚¨ Kã€Vï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚ æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼šåªç¼“å­˜æœ€è¿‘ $N$ ä¸ª token çš„ KVï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡ã€‚ ä¸‰ã€Python ä»£ç  Demoï¼ˆç®€åŒ–ç‰ˆï¼‰ ä¸‹é¢æ˜¯ä¸€ä¸ª ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶ çš„çº¯ NumPy å®ç°ï¼Œæ¼”ç¤º KV Cache å¦‚ä½•å·¥ä½œã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 import numpy as np # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç° np.random.seed(42) class SimpleKVCacheDemo: def __init__(self, d_model=64, d_k=32, d_v=32): self.d_model = d_model self.d_k = d_k self.d_v = d_v # éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰ self.W_Q = np.random.randn(d_model, d_k) self.W_K = np.random.randn(d_model, d_k) self.W_V = np.random.randn(d_model, d_v) self.W_O = np.random.randn(d_v, d_model) # è¾“å‡ºæŠ•å½±ï¼ˆå¯é€‰ï¼‰ # åˆå§‹åŒ– KV ç¼“å­˜ self.K_cache = None # shape: (seq_len, d_k) self.V_cache = None # shape: (seq_len, d_v) def clear_cache(self): self.K_cache = None self.V_cache = None def forward_step(self, x): \"\"\" x: np.array of shape (d_model,) â€” å½“å‰è¾“å…¥ token çš„ embedding è¿”å›è¾“å‡ºè¡¨ç¤ºï¼Œå¹¶æ›´æ–° KV cache æ­£å¸¸çš„å‰å‘è¾“å…¥æ˜¯[bs, seq_len, d_model],è¿™é‡Œè€ƒè™‘bs=1çš„æƒ…å†µï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¾“å…¥ä¸ºåˆšåˆšç”Ÿæˆçš„æœ€æ–°çš„tokenï¼Œæ‰€ä»¥è¯´å½“å‰è¾“å…¥çš„tokenç»´åº¦ä¸ºï¼š[1, d_model] \"\"\" x = x.reshape(1, -1) # (1, d_model) # è®¡ç®—å½“å‰ token çš„ Q, K, V Q = x @ self.W_Q # (1, d_k) K = x @ self.W_K # (1, d_k) V = x @ self.W_V # (1, d_v) if self.K_cache is None: # ç¬¬ä¸€ä¸ª token self.K_cache = K self.V_cache = V attn_weights = np.array([[1.0]]) # softmax([0]) = [1] else: # æ‹¼æ¥ç¼“å­˜ K_full = np.vstack([self.K_cache, K]) # (seq_len+1, d_k) V_full = np.vstack([self.V_cache, V]) # (seq_len+1, d_v) # è®¡ç®— attention scores: Q @ K_full^T scores = Q @ K_full.T / np.sqrt(self.d_k) # (1, seq_len+1) attn_weights = np.exp(scores - np.max(scores)) # numerical stability attn_weights /= np.sum(attn_weights, axis=-1, keepdims=True) # æ›´æ–°ç¼“å­˜ self.K_cache = K_full self.V_cache = V_full # åŠ æƒæ±‚å’Œ output = attn_weights @ V_full # (1, d_v) output = output @ self.W_O # (1, d_model) # åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªattentionçš„ç»´åº¦å°±æ˜¯(1, d_model)ï¼Œä¸åŒäºè®­ç»ƒä¸­ï¼Œç»´åº¦ä¸º(seq_len, d_model) return output.flatten() # ------------------ Demo ------------------ # æ¨¡æ‹Ÿ token embeddingsï¼ˆæ¯”å¦‚æ¥è‡ª embedding layerï¼‰ embeddings = [ np.random.randn(64), np.random.randn(64), np.random.randn(64), np.random.randn(64) ] model = SimpleKVCacheDemo() print(\"=== Without KV Cache (naive recompute) ===\") # è¿™é‡Œæˆ‘ä»¬ä¸å®ç°æ— ç¼“å­˜ç‰ˆæœ¬ï¼Œä½†é€»è¾‘ä¸Šæ¯æ­¥éƒ½è¦é‡ç®—å…¨éƒ¨ print(\"\\n=== With KV Cache ===\") model.clear_cache() for i, emb in enumerate(embeddings): out = model.forward_step(emb) print(f\"Step {i+1}: output norm = {np.linalg.norm(out):.4f}, \" f\"cache length = {model.K_cache.shape[0]}\") # éªŒè¯ï¼šå¦‚æœé‡æ–°è¾“å…¥ç›¸åŒåºåˆ—ï¼Œç¼“å­˜ä¼šç´¯ç§¯ print(\"\\nAdding one more token...\") out = model.forward_step(np.random.randn(64)) print(f\"Step 5: cache length = {model.K_cache.shape[0]}\") è¾“å‡ºç¤ºä¾‹ï¼š === With KV Cache === Step 1: output norm = 7.8921, cache length = 1 Step 2: output norm = 8.1023, cache length = 2 Step 3: output norm = 7.9542, cache length = 3 Step 4: output norm = 8.0124, cache length = 4 Adding one more token... Step 5: cache length = 5 ğŸ’¡ æ­¤ demo è™½ç®€åŒ–ï¼ˆå•å¤´ã€æ—  batchã€æ—  LayerNorm ç­‰ï¼‰ï¼Œä½†å®Œæ•´å±•ç¤ºäº† KV Cache çš„æ ¸å¿ƒæœºåˆ¶ï¼šç¼“å­˜ Kã€Vï¼Œé¿å…é‡å¤è®¡ç®—ã€‚\nå››ã€å®é™…åº”ç”¨ä¸­çš„ KV Cacheï¼ˆè¡¥å……ï¼‰ åœ¨çœŸå® LLM æ¨ç†å¼•æ“ä¸­ï¼ˆå¦‚ HuggingFace Transformersã€vLLMã€TensorRT-LLMï¼‰ï¼š\nKV Cache æ˜¯é»˜è®¤å¯ç”¨çš„ï¼ˆpast_key_values å‚æ•°ï¼‰ï¼› æ”¯æŒ batch æ¨ç†ï¼ˆä¸åŒåºåˆ—é•¿åº¦éœ€ padding æˆ–ä½¿ç”¨ PagedAttentionï¼‰ï¼› å¯é€šè¿‡ use_cache=True æ§åˆ¶ï¼› æ˜¾å­˜å ç”¨ â‰ˆ $2 \\times \\text{num_layers} \\times \\text{num_heads} \\times \\text{seq_len} \\times \\text{head_dim} \\times \\text{bytes_per_param}$ ä¾‹å¦‚ HuggingFace ä¸­ä½¿ç”¨ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained(\"gpt2\", use_cache=True) tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") inputs = tokenizer(\"Hello, how are\", return_tensors=\"pt\") outputs = model(**inputs) # ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—å…¨éƒ¨ KV # outputs.past_key_values åŒ…å«å„å±‚çš„ (K, V) ç¼“å­˜ # ä¸‹ä¸€æ­¥ç”Ÿæˆï¼š next_input = tokenizer(\" you\", return_tensors=\"pt\").input_ids[:, -1:] outputs2 = model( input_ids=next_input, past_key_values=outputs.past_key_values # ä¼ å…¥ç¼“å­˜ï¼ ) æ€»ç»“ é¡¹ç›® è¯´æ˜ åŠ¨æœº é¿å…è‡ªå›å½’ç”Ÿæˆä¸­é‡å¤è®¡ç®— Kã€V æ ¸å¿ƒ ç¼“å­˜å†å² token çš„ Key å’Œ Value ä¼˜åŠ¿ æ¨ç†é€Ÿåº¦æå‡ï¼Œæ¯æ­¥ $O(1)$ è®¡ç®— ä»£ä»· é¢å¤–æ˜¾å­˜ï¼ˆä¸åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼‰ æ‰©å±•æŠ€æœ¯ PagedAttentionã€KV é‡åŒ–ã€æ»‘åŠ¨çª—å£ KV Cache æ˜¯ç°ä»£ LLM é«˜æ•ˆæ¨ç†çš„åŸºçŸ³ä¹‹ä¸€ï¼Œç†è§£å®ƒå¯¹ä¼˜åŒ–éƒ¨ç½²ã€è®¾è®¡æ¨ç†å¼•æ“è‡³å…³é‡è¦ã€‚\n",
  "wordCount" : "853",
  "inLanguage": "en",
  "datePublished": "2025-01-13T00:00:00Z",
  "dateModified": "2025-01-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/architecture/kv-cache/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="å¤§è¯­è¨€æ¨¡å‹ (LLM)">
                    <span>å¤§è¯­è¨€æ¨¡å‹ (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="å·¥å…·ç®± (toolbox)">
                    <span>å·¥å…·ç®± (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="å¼ºåŒ–å­¦ä¹  (RL)">
                    <span>å¼ºåŒ–å­¦ä¹  (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
                    <span>æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false }
            ],
            trust: true,
            throwOnError: false
        });
        });
    </script>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³
    </h1>
    <div class="post-meta"><span title='2025-01-13 00:00:00 +0000 UTC'>January 13, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><p>KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äº<strong>åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆ</strong>çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚</p>
<h2 id="ä¸€ä¸ºä»€ä¹ˆéœ€è¦-kv-cache">ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#ä¸€ä¸ºä»€ä¹ˆéœ€è¦-kv-cache">#</a></h2>
<h3 id="1-è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨">1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨<a hidden class="anchor" aria-hidden="true" href="#1-è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨">#</a></h3>
<p>å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥<strong>è‡ªå›å½’æ–¹å¼</strong>ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š</p>
<pre tabindex="0"><code>è¾“å…¥: &#34;ä»Šå¤©å¤©æ°”&#34;
ç¬¬1æ­¥è¾“å‡º: &#34;çœŸ&#34;
è¾“å…¥å˜ä¸º: &#34;ä»Šå¤©å¤©æ°”çœŸ&#34;
ç¬¬2æ­¥è¾“å‡º: &#34;å¥½&#34;
...
</code></pre><h3 id="2-æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜">2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜<a hidden class="anchor" aria-hidden="true" href="#2-æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜">#</a></h3>
<p>Transformer ä½¿ç”¨ <strong>è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰</strong>ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚</p>
<p>å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t+1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t+1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š</p>
<ul>
<li>ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV</li>
<li>ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰</li>
<li>&hellip;</li>
<li>ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼</li>
</ul>
<p>è¿™å¯¼è‡´ <strong>æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$</strong>ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚</p>
<h3 id="3-kv-cache-çš„æå‡º">3. KV Cache çš„æå‡º<a hidden class="anchor" aria-hidden="true" href="#3-kv-cache-çš„æå‡º">#</a></h3>
<p>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼š<strong>åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡</strong>ã€‚å› ä¸ºï¼š</p>
<ul>
<li>åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œ<strong>å†å² token ä¸ä¼šæ”¹å˜</strong>ï¼›</li>
<li>Attention å…¬å¼ä¸­ï¼Œå½“å‰ token çš„ Q åªéœ€ä¸æ‰€æœ‰å†å² Kã€V è®¡ç®—å³å¯ï¼›</li>
<li>Q æ˜¯å½“å‰ token çš„è¡¨ç¤ºï¼Œå¿…é¡»å®æ—¶è®¡ç®—ï¼›ä½† Kã€V å¯ä»¥æå‰ç¼“å­˜ã€‚</li>
</ul>
<p>äºæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥åªéœ€ï¼š</p>
<ul>
<li>è®¡ç®—å½“å‰ token çš„ Qï¼›</li>
<li>å°†å†å²ç¼“å­˜çš„ Kã€V ä¸å½“å‰ Kã€V æ‹¼æ¥ï¼›</li>
<li>æ‰§è¡Œä¸€æ¬¡ attentionã€‚</li>
</ul>
<p>è¿™æ ·ï¼Œæ¯æ­¥è®¡ç®—é‡æ’å®šï¼ˆ$O(1)$ per stepï¼‰ï¼Œæ€»å¤æ‚åº¦ä» $O(n^2)$ é™ä¸º $O(n)$ã€‚</p>
<blockquote>
<p>âœ… <strong>KV Cache çš„æ ¸å¿ƒæ€æƒ³ï¼šç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œé¿å…é‡å¤è®¡ç®— Kã€Vã€‚</strong></p>
</blockquote>
<hr>
<h2 id="äºŒæŠ€æœ¯åŸç†è¯¦è§£">äºŒã€æŠ€æœ¯åŸç†è¯¦è§£<a hidden class="anchor" aria-hidden="true" href="#äºŒæŠ€æœ¯åŸç†è¯¦è§£">#</a></h2>
<h3 id="1-æ ‡å‡†-self-attention-å…¬å¼å›é¡¾">1. æ ‡å‡† Self-Attention å…¬å¼å›é¡¾<a hidden class="anchor" aria-hidden="true" href="#1-æ ‡å‡†-self-attention-å…¬å¼å›é¡¾">#</a></h3>
<p>å¯¹äºè¾“å…¥åºåˆ— $X \in \mathbb{R}^{n \times d}$ï¼š</p>
<p>$$
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
$$
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<h3 id="2-è‡ªå›å½’ç”Ÿæˆä¸­çš„-kv-cache-åº”ç”¨">2. è‡ªå›å½’ç”Ÿæˆä¸­çš„ KV Cache åº”ç”¨<a hidden class="anchor" aria-hidden="true" href="#2-è‡ªå›å½’ç”Ÿæˆä¸­çš„-kv-cache-åº”ç”¨">#</a></h3>
<p>è®¾å½“å‰å·²å¤„ç† token æ•°ä¸º $t$ï¼Œç¼“å­˜äº†ï¼š</p>
<ul>
<li>$K_{\text{cache}} \in \mathbb{R}^{t \times d_k}$</li>
<li>$V_{\text{cache}} \in \mathbb{R}^{t \times d_v}$</li>
</ul>
<p>å½“è¾“å…¥æ–° token $x_{t+1}$ï¼ˆæˆ–åˆå§‹ prompt çš„ä¸‹ä¸€ä¸ª tokenï¼‰ï¼š</p>
<ol>
<li>è®¡ç®—å…¶ Qã€Kã€Vï¼š
$$
q_{t+1} = x_{t+1} W_Q,\quad k_{t+1} = x_{t+1} W_K,\quad v_{t+1} = x_{t+1} W_V
$$</li>
<li>æ›´æ–°ç¼“å­˜ï¼š
$$
K_{\text{new}} = [K_{\text{cache}}; k_{t+1}],\quad V_{\text{new}} = [V_{\text{cache}}; v_{t+1}]
$$</li>
<li>è®¡ç®— attentionï¼š
$$
\text{attn} = \text{softmax}\left(\frac{q_{t+1} K_{\text{new}}^T}{\sqrt{d_k}}\right) V_{\text{new}}
$$</li>
</ol>
<blockquote>
<p>æ³¨æ„ï¼šQ åªéœ€å½“å‰ token çš„ï¼ˆå› ä¸ºæ˜¯ decoder-only æ¶æ„ï¼Œå¦‚ GPTï¼‰ï¼Œè€Œ Kã€V éœ€è¦å…¨éƒ¨å†å²ã€‚</p>
</blockquote>
<blockquote>
<p>æ³¨æ„ï¼šåœ¨è‡ªå›å½’çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç½‘ç»œä¸­æµé€šçš„tensorçš„shapeä¸ºï¼š[batch_size, 1, hidden_size]ï¼Œè€Œä¸æ˜¯è®­ç»ƒå½“ä¸­çš„[batch_size, seq_len, hidden_size]ã€‚</p>
</blockquote>
<h3 id="3-å¤šå¤´æ³¨æ„åŠ›ä¸­çš„-kv-cache">3. å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ KV Cache<a hidden class="anchor" aria-hidden="true" href="#3-å¤šå¤´æ³¨æ„åŠ›ä¸­çš„-kv-cache">#</a></h3>
<p>æ¯ä¸ª attention head éƒ½æœ‰è‡ªå·±çš„ $W_K, W_V$ï¼Œå› æ­¤ KV Cache é€šå¸¸æ˜¯ shape: $(batch_size, num_heads, seq_len, head_dim)$ã€‚</p>
<p>åœ¨å®ç°ä¸­ï¼Œå¸¸æŒ‰ head ç»´åº¦ç»„ç»‡ç¼“å­˜ã€‚</p>
<h3 id="4-å·¥ç¨‹ä¼˜åŒ–">4. å·¥ç¨‹ä¼˜åŒ–<a hidden class="anchor" aria-hidden="true" href="#4-å·¥ç¨‹ä¼˜åŒ–">#</a></h3>
<ul>
<li><strong>å†…å­˜ç®¡ç†</strong>ï¼šç¼“å­˜éšåºåˆ—å¢é•¿è€Œå¢é•¿ï¼Œéœ€æ³¨æ„æ˜¾å­˜é™åˆ¶ã€‚</li>
<li><strong>PagedAttentionï¼ˆvLLMï¼‰</strong>ï¼šå°† KV Cache åˆ†é¡µå­˜å‚¨ï¼Œæé«˜å†…å­˜åˆ©ç”¨ç‡ã€‚</li>
<li><strong>é‡åŒ– KV Cache</strong>ï¼šç”¨ int8/float16 å­˜å‚¨ Kã€Vï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚</li>
<li><strong>æ»‘åŠ¨çª—å£æ³¨æ„åŠ›</strong>ï¼šåªç¼“å­˜æœ€è¿‘ $N$ ä¸ª token çš„ KVï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡ã€‚</li>
</ul>
<hr>
<h2 id="ä¸‰python-ä»£ç -demoç®€åŒ–ç‰ˆ">ä¸‰ã€Python ä»£ç  Demoï¼ˆç®€åŒ–ç‰ˆï¼‰<a hidden class="anchor" aria-hidden="true" href="#ä¸‰python-ä»£ç -demoç®€åŒ–ç‰ˆ">#</a></h2>
<p>ä¸‹é¢æ˜¯ä¸€ä¸ª <strong>ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶</strong> çš„çº¯ NumPy å®ç°ï¼Œæ¼”ç¤º KV Cache å¦‚ä½•å·¥ä½œã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleKVCacheDemo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># è¾“å‡ºæŠ•å½±ï¼ˆå¯é€‰ï¼‰</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># åˆå§‹åŒ– KV ç¼“å­˜</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># shape: (seq_len, d_k)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">V_cache</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># shape: (seq_len, d_v)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">clear_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">V_cache</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        x: np.array of shape (d_model,) â€” å½“å‰è¾“å…¥ token çš„ embedding
</span></span></span><span class="line"><span class="cl"><span class="s2">        è¿”å›è¾“å‡ºè¡¨ç¤ºï¼Œå¹¶æ›´æ–° KV cache
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        æ­£å¸¸çš„å‰å‘è¾“å…¥æ˜¯[bs, seq_len, d_model],è¿™é‡Œè€ƒè™‘bs=1çš„æƒ…å†µï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¾“å…¥ä¸ºåˆšåˆšç”Ÿæˆçš„æœ€æ–°çš„tokenï¼Œæ‰€ä»¥è¯´å½“å‰è¾“å…¥çš„tokenç»´åº¦ä¸ºï¼š[1, d_model]
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (1, d_model)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># è®¡ç®—å½“å‰ token çš„ Q, K, V</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span>  <span class="c1"># (1, d_k)</span>
</span></span><span class="line"><span class="cl">        <span class="n">K</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span>  <span class="c1"># (1, d_k)</span>
</span></span><span class="line"><span class="cl">        <span class="n">V</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span>  <span class="c1"># (1, d_v)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ç¬¬ä¸€ä¸ª token</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span> <span class="o">=</span> <span class="n">K</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">V_cache</span> <span class="o">=</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">]])</span>  <span class="c1"># softmax([0]) = [1]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># æ‹¼æ¥ç¼“å­˜</span>
</span></span><span class="line"><span class="cl">            <span class="n">K_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span><span class="p">,</span> <span class="n">K</span><span class="p">])</span>  <span class="c1"># (seq_len+1, d_k)</span>
</span></span><span class="line"><span class="cl">            <span class="n">V_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">V_cache</span><span class="p">,</span> <span class="n">V</span><span class="p">])</span>  <span class="c1"># (seq_len+1, d_v)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># è®¡ç®— attention scores: Q @ K_full^T</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K_full</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (1, seq_len+1)</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>  <span class="c1"># numerical stability</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_weights</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># æ›´æ–°ç¼“å­˜</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">K_cache</span> <span class="o">=</span> <span class="n">K_full</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">V_cache</span> <span class="o">=</span> <span class="n">V_full</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># åŠ æƒæ±‚å’Œ</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">V_full</span>  <span class="c1"># (1, d_v)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span>       <span class="c1"># (1, d_model)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªattentionçš„ç»´åº¦å°±æ˜¯(1, d_model)ï¼Œä¸åŒäºè®­ç»ƒä¸­ï¼Œç»´åº¦ä¸º(seq_len, d_model)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ------------------ Demo ------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># æ¨¡æ‹Ÿ token embeddingsï¼ˆæ¯”å¦‚æ¥è‡ª embedding layerï¼‰</span>
</span></span><span class="line"><span class="cl"><span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleKVCacheDemo</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;=== Without KV Cache (naive recompute) ===&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¿™é‡Œæˆ‘ä»¬ä¸å®ç°æ— ç¼“å­˜ç‰ˆæœ¬ï¼Œä½†é€»è¾‘ä¸Šæ¯æ­¥éƒ½è¦é‡ç®—å…¨éƒ¨</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">=== With KV Cache ===&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">emb</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Step </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: output norm = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &#34;</span>
</span></span><span class="line"><span class="cl">          <span class="sa">f</span><span class="s2">&#34;cache length = </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">K_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># éªŒè¯ï¼šå¦‚æœé‡æ–°è¾“å…¥ç›¸åŒåºåˆ—ï¼Œç¼“å­˜ä¼šç´¯ç§¯</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Adding one more token...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Step 5: cache length = </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">K_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="è¾“å‡ºç¤ºä¾‹">è¾“å‡ºç¤ºä¾‹ï¼š<a hidden class="anchor" aria-hidden="true" href="#è¾“å‡ºç¤ºä¾‹">#</a></h3>
<pre tabindex="0"><code>=== With KV Cache ===
Step 1: output norm = 7.8921, cache length = 1
Step 2: output norm = 8.1023, cache length = 2
Step 3: output norm = 7.9542, cache length = 3
Step 4: output norm = 8.0124, cache length = 4

Adding one more token...
Step 5: cache length = 5
</code></pre><blockquote>
<p>ğŸ’¡ æ­¤ demo è™½ç®€åŒ–ï¼ˆå•å¤´ã€æ—  batchã€æ—  LayerNorm ç­‰ï¼‰ï¼Œä½†å®Œæ•´å±•ç¤ºäº† KV Cache çš„æ ¸å¿ƒæœºåˆ¶ï¼š<strong>ç¼“å­˜ Kã€Vï¼Œé¿å…é‡å¤è®¡ç®—</strong>ã€‚</p>
</blockquote>
<hr>
<h2 id="å››å®é™…åº”ç”¨ä¸­çš„-kv-cacheè¡¥å……">å››ã€å®é™…åº”ç”¨ä¸­çš„ KV Cacheï¼ˆè¡¥å……ï¼‰<a hidden class="anchor" aria-hidden="true" href="#å››å®é™…åº”ç”¨ä¸­çš„-kv-cacheè¡¥å……">#</a></h2>
<p>åœ¨çœŸå® LLM æ¨ç†å¼•æ“ä¸­ï¼ˆå¦‚ HuggingFace Transformersã€vLLMã€TensorRT-LLMï¼‰ï¼š</p>
<ul>
<li>KV Cache æ˜¯é»˜è®¤å¯ç”¨çš„ï¼ˆ<code>past_key_values</code> å‚æ•°ï¼‰ï¼›</li>
<li>æ”¯æŒ batch æ¨ç†ï¼ˆä¸åŒåºåˆ—é•¿åº¦éœ€ padding æˆ–ä½¿ç”¨ PagedAttentionï¼‰ï¼›</li>
<li>å¯é€šè¿‡ <code>use_cache=True</code> æ§åˆ¶ï¼›</li>
<li>æ˜¾å­˜å ç”¨ â‰ˆ $2 \times \text{num_layers} \times \text{num_heads} \times \text{seq_len} \times \text{head_dim} \times \text{bytes_per_param}$</li>
</ul>
<p>ä¾‹å¦‚ HuggingFace ä¸­ä½¿ç”¨ï¼š</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;gpt2&#34;</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;gpt2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&#34;Hello, how are&#34;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—å…¨éƒ¨ KV</span>
</span></span><span class="line"><span class="cl"><span class="c1"># outputs.past_key_values åŒ…å«å„å±‚çš„ (K, V) ç¼“å­˜</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ä¸‹ä¸€æ­¥ç”Ÿæˆï¼š</span>
</span></span><span class="line"><span class="cl"><span class="n">next_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&#34; you&#34;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs2</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_ids</span><span class="o">=</span><span class="n">next_input</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">past_key_values</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>  <span class="c1"># ä¼ å…¥ç¼“å­˜ï¼</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h2 id="æ€»ç»“">æ€»ç»“<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“">#</a></h2>
<table>
  <thead>
      <tr>
          <th>é¡¹ç›®</th>
          <th>è¯´æ˜</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>åŠ¨æœº</strong></td>
          <td>é¿å…è‡ªå›å½’ç”Ÿæˆä¸­é‡å¤è®¡ç®— Kã€V</td>
      </tr>
      <tr>
          <td><strong>æ ¸å¿ƒ</strong></td>
          <td>ç¼“å­˜å†å² token çš„ Key å’Œ Value</td>
      </tr>
      <tr>
          <td><strong>ä¼˜åŠ¿</strong></td>
          <td>æ¨ç†é€Ÿåº¦æå‡ï¼Œæ¯æ­¥ $O(1)$ è®¡ç®—</td>
      </tr>
      <tr>
          <td><strong>ä»£ä»·</strong></td>
          <td>é¢å¤–æ˜¾å­˜ï¼ˆä¸åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼‰</td>
      </tr>
      <tr>
          <td><strong>æ‰©å±•æŠ€æœ¯</strong></td>
          <td>PagedAttentionã€KV é‡åŒ–ã€æ»‘åŠ¨çª—å£</td>
      </tr>
  </tbody>
</table>
<p>KV Cache æ˜¯ç°ä»£ LLM é«˜æ•ˆæ¨ç†çš„åŸºçŸ³ä¹‹ä¸€ï¼Œç†è§£å®ƒå¯¹ä¼˜åŒ–éƒ¨ç½²ã€è®¾è®¡æ¨ç†å¼•æ“è‡³å…³é‡è¦ã€‚</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
