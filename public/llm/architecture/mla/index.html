<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>MLAï¼šMulti-head Latent Attention | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="ä¸€ã€MLA çš„ç”±æ¥
èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ
åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚
ç›®æ ‡ï¼šå‹ç¼© KV Cache
ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š

MQAï¼ˆMulti-Query Attentionï¼‰ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚
GQAï¼ˆGrouped-Query Attentionï¼‰ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚
MLAï¼ˆMulti-head Latent Attentionï¼‰ï¼šä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/Vã€‚

æå‡ºè€…ä¸å‡ºå¤„

DeepSeek-V2ï¼ˆ2024ï¼‰ é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å MLAï¼ˆMulti-head Latent Attentionï¼‰ã€‚
æ ¸å¿ƒæ€æƒ³ï¼šç”¨ ä½ç§©æ½œåœ¨å‘é‡ &#43; å°å‹æŠ•å½±ç½‘ç»œ ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚



äºŒã€åŸºæœ¬åŸç†
MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

ä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \in \mathbb{R}^{d_z} $ï¼ˆ$ d_z \ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/mla/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/mla/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/mla/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="MLAï¼šMulti-head Latent Attention">
  <meta property="og:description" content="ä¸€ã€MLA çš„ç”±æ¥ èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚
ç›®æ ‡ï¼šå‹ç¼© KV Cache ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š
MQAï¼ˆMulti-Query Attentionï¼‰ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚ GQAï¼ˆGrouped-Query Attentionï¼‰ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚ MLAï¼ˆMulti-head Latent Attentionï¼‰ï¼šä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/Vã€‚ æå‡ºè€…ä¸å‡ºå¤„ DeepSeek-V2ï¼ˆ2024ï¼‰ é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å MLAï¼ˆMulti-head Latent Attentionï¼‰ã€‚ æ ¸å¿ƒæ€æƒ³ï¼šç”¨ ä½ç§©æ½œåœ¨å‘é‡ &#43; å°å‹æŠ•å½±ç½‘ç»œ ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚ äºŒã€åŸºæœ¬åŸç† MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
ä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \in \mathbb{R}^{d_z} $ï¼ˆ$ d_z \ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2026-01-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-07T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MLAï¼šMulti-head Latent Attention">
<meta name="twitter:description" content="ä¸€ã€MLA çš„ç”±æ¥
èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ
åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚
ç›®æ ‡ï¼šå‹ç¼© KV Cache
ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š

MQAï¼ˆMulti-Query Attentionï¼‰ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚
GQAï¼ˆGrouped-Query Attentionï¼‰ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚
MLAï¼ˆMulti-head Latent Attentionï¼‰ï¼šä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/Vã€‚

æå‡ºè€…ä¸å‡ºå¤„

DeepSeek-V2ï¼ˆ2024ï¼‰ é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å MLAï¼ˆMulti-head Latent Attentionï¼‰ã€‚
æ ¸å¿ƒæ€æƒ³ï¼šç”¨ ä½ç§©æ½œåœ¨å‘é‡ &#43; å°å‹æŠ•å½±ç½‘ç»œ ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚



äºŒã€åŸºæœ¬åŸç†
MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

ä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \in \mathbb{R}^{d_z} $ï¼ˆ$ d_z \ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "å¤§è¯­è¨€æ¨¡å‹ (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "MLAï¼šMulti-head Latent Attention",
      "item": "http://localhost:1313/llm/architecture/mla/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MLAï¼šMulti-head Latent Attention",
  "name": "MLAï¼šMulti-head Latent Attention",
  "description": "ä¸€ã€MLA çš„ç”±æ¥ èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚\nç›®æ ‡ï¼šå‹ç¼© KV Cache ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š\nMQAï¼ˆMulti-Query Attentionï¼‰ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚ GQAï¼ˆGrouped-Query Attentionï¼‰ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚ MLAï¼ˆMulti-head Latent Attentionï¼‰ï¼šä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/Vã€‚ æå‡ºè€…ä¸å‡ºå¤„ DeepSeek-V2ï¼ˆ2024ï¼‰ é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å MLAï¼ˆMulti-head Latent Attentionï¼‰ã€‚ æ ¸å¿ƒæ€æƒ³ï¼šç”¨ ä½ç§©æ½œåœ¨å‘é‡ + å°å‹æŠ•å½±ç½‘ç»œ ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚ äºŒã€åŸºæœ¬åŸç† MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\nä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \\in \\mathbb{R}^{d_z} $ï¼ˆ$ d_z \\ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚\n",
  "keywords": [
    
  ],
  "articleBody": "ä¸€ã€MLA çš„ç”±æ¥ èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚\nç›®æ ‡ï¼šå‹ç¼© KV Cache ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š\nMQAï¼ˆMulti-Query Attentionï¼‰ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚ GQAï¼ˆGrouped-Query Attentionï¼‰ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚ MLAï¼ˆMulti-head Latent Attentionï¼‰ï¼šä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/Vã€‚ æå‡ºè€…ä¸å‡ºå¤„ DeepSeek-V2ï¼ˆ2024ï¼‰ é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å MLAï¼ˆMulti-head Latent Attentionï¼‰ã€‚ æ ¸å¿ƒæ€æƒ³ï¼šç”¨ ä½ç§©æ½œåœ¨å‘é‡ + å°å‹æŠ•å½±ç½‘ç»œ ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚ äºŒã€åŸºæœ¬åŸç† MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\nä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \\in \\mathbb{R}^{d_z} $ï¼ˆ$ d_z \\ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚\nå…·ä½“æµç¨‹ï¼š\nå¯¹æ¯ä¸ªè¾“å…¥ token $ x_t $ï¼Œå…ˆè®¡ç®—ä¸€ä¸ª å…±äº«çš„æ½œåœ¨è¡¨ç¤º $ z_t $ã€‚ åœ¨ attention è®¡ç®—æ—¶ï¼Œå¯¹æ¯ä¸ª head $ i $ï¼Œç”¨ä¸¤ä¸ªå°å‹ MLPï¼ˆæˆ–çº¿æ€§å±‚ï¼‰å°† $ z_t $ æ˜ å°„ä¸ºè¯¥ head çš„ $ k_{t,i} $ å’Œ $ v_{t,i} $ã€‚ ç¼“å­˜çš„æ˜¯ $ z_t $ è€Œé $ k_t, v_t $ï¼Œæ˜¾è‘—èŠ‚çœå†…å­˜ã€‚ ç”±äº $ z_t $ ç»´åº¦è¿œå°äºåŸå§‹ K/Vï¼ˆä¾‹å¦‚ $ d_z = 128 $ï¼Œè€Œ $ d_k = d_v = 128 \\times 8 = 1024 $ï¼‰ï¼Œä¸”é‡å»ºç½‘ç»œå‚æ•°é‡å°ï¼Œæ•´ä½“æ•ˆç‡æ›´é«˜ã€‚\nä¸‰ã€è¯¦ç»†æ•°å­¦ç»†èŠ‚ è®¾ï¼š\nè¾“å…¥ token è¡¨ç¤ºï¼š$ x_t \\in \\mathbb{R}^{d_{\\text{model}}} $ æ½œåœ¨ç»´åº¦ï¼š$ d_z $ æ³¨æ„åŠ›å¤´æ•°ï¼š$ H $ æ¯ä¸ª head çš„ key/value ç»´åº¦ï¼š$ d_k = d_v = d_h $ æ­¥éª¤ 1ï¼šç”Ÿæˆæ½œåœ¨å‘é‡ é€šè¿‡ä¸€ä¸ªå…±äº«çº¿æ€§å±‚ç”Ÿæˆæ½œåœ¨å‘é‡ï¼š $$ z_t = W_z x_t + b_z \\quad \\in \\mathbb{R}^{d_z} $$ å…¶ä¸­ $ W_z \\in \\mathbb{R}^{d_z \\times d_{\\text{model}}} $\nå®é™…å®ç°ä¸­ï¼Œ$ z_t $ å¯èƒ½ç»è¿‡ LayerNorm æˆ–å…¶ä»–å½’ä¸€åŒ–ã€‚\næ­¥éª¤ 2ï¼šæŒ‰å¤´é‡å»º K å’Œ V å¯¹æ¯ä¸ª head $ h \\in {1, â€¦, H} $ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„é‡å»ºçŸ©é˜µï¼š $ k_{t,h} = W_{K,h} z_t \\quad \\in \\mathbb{R}^{d_h} \\ v_{t,h} = W_{V,h} z_t \\quad \\in \\mathbb{R}^{d_h} $ å…¶ä¸­ï¼š\n$ W_{K,h} \\in \\mathbb{R}^{d_h \\times d_z} $ $ W_{V,h} \\in \\mathbb{R}^{d_h \\times d_z} $ ä¸ºå‡å°‘å‚æ•°é‡ï¼Œé€šå¸¸å°†æ‰€æœ‰å¤´çš„æƒé‡å †å ï¼š $ W_K \\in \\mathbb{R}^{H \\cdot d_h \\times d_z}, \\quad W_V \\in \\mathbb{R}^{H \\cdot d_h \\times d_z} $\nåˆ™å¯æ‰¹é‡è®¡ç®—ï¼š $ K = (W_K z_t) \\in \\mathbb{R}^{H d_h} \\Rightarrow \\text{reshape to } (H, d_h) \\ V = (W_V z_t) \\in \\mathbb{R}^{H d_h} \\Rightarrow \\text{reshape to } (H, d_h) $\næ­¥éª¤ 3ï¼šæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®— æŸ¥è¯¢ä»ç”±ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆï¼ˆå›  Q ä¸éœ€ç¼“å­˜ï¼‰ï¼š $ Q = x_t W_Q \\in \\mathbb{R}^{H \\times d_h} $\nç„¶åè¿›è¡Œæ ‡å‡† scaled dot-product attentionï¼š $ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_h}} \\right) V $\nç¼“å­˜ç­–ç•¥ ç¼“å­˜å†…å®¹ï¼šä»…ç¼“å­˜ $ {z_1, z_2, â€¦, z_t} $ï¼Œè€Œé $ K, V $ã€‚ ç¼“å­˜å¤§å°ï¼šä» $ O(T \\cdot H \\cdot d_h) $ é™è‡³ $ O(T \\cdot d_z) $ã€‚ è‹¥ $ d_z \\ll H \\cdot d_h $ï¼ˆä¾‹å¦‚ $ d_z = 128 $, $ H=32, d_h=128 \\Rightarrow H d_h = 4096 $ï¼‰ï¼Œåˆ™å‹ç¼©æ¯”è¾¾ 32xã€‚ å››ã€PyTorch å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰ ä»¥ä¸‹æ˜¯ä¸€ä¸ª å•å±‚ MLA æ³¨æ„åŠ›æ¨¡å— çš„ PyTorch å®ç°ï¼Œç”¨äºæ¼”ç¤ºåŸç†ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 import torch import torch.nn as nn import torch.nn.functional as F class MLAttention(nn.Module): def __init__( self, d_model: int, n_heads: int, d_head: int, d_latent: int, dropout: float = 0.1 ): \"\"\" Multi-head Latent Attention (MLA) as in DeepSeek-V2. Args: d_model: model dimension (e.g., 4096) n_heads: number of attention heads d_head: dimension per head (e.g., 128) d_latent: latent dimension for KV compression (e.g., 128) \"\"\" super().__init__() self.d_model = d_model self.n_heads = n_heads self.d_head = d_head self.d_latent = d_latent self.scale = d_head ** -0.5 # Query projection (not compressed) self.wq = nn.Linear(d_model, n_heads * d_head, bias=False) # Latent vector projection (shared for all heads) self.wz = nn.Linear(d_model, d_latent, bias=False) # Reconstruction matrices for K and V (per head, but stored as big matrices) self.wk_recon = nn.Parameter(torch.randn(n_heads * d_head, d_latent)) self.wv_recon = nn.Parameter(torch.randn(n_heads * d_head, d_latent)) # Output projection self.wo = nn.Linear(n_heads * d_head, d_model, bias=False) self.dropout = nn.Dropout(dropout) # Initialize reconstruction matrices (optional: Xavier) nn.init.xavier_uniform_(self.wk_recon) nn.init.xavier_uniform_(self.wv_recon) def forward( self, x: torch.Tensor, kv_cache: dict = None # { 'z': [seq_len, d_latent] } ): \"\"\" x: [batch, seq_len, d_model] kv_cache: optional dict to store/reuse latent vectors Returns: output: [batch, seq_len, d_model] new_kv_cache: updated cache with latent vectors \"\"\" B, L, D = x.shape # Project queries q = self.wq(x) # [B, L, H * dh] q = q.view(B, L, self.n_heads, self.d_head).transpose(1, 2) # [B, H, L, dh] # Project to latent space z = self.wz(x) # [B, L, d_latent] # Update KV cache if kv_cache is not None: if 'z' in kv_cache: z = torch.cat([kv_cache['z'], z], dim=1) # prepend cached latents kv_cache['z'] = z.detach() # update cache # Reconstruct K and V from latent vectors # z: [B, T, d_latent] where T = current total length T = z.size(1) # Reconstruct all K and V at once k_flat = F.linear(z, self.wk_recon) # [B, T, H * dh] v_flat = F.linear(z, self.wv_recon) # [B, T, H * dh] k = k_flat.view(B, T, self.n_heads, self.d_head).transpose(1, 2) # [B, H, T, dh] v = v_flat.view(B, T, self.n_heads, self.d_head).transpose(1, 2) # [B, H, T, dh] # Scaled dot-product attention attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale # [B, H, L, T] attn_weights = F.softmax(attn_scores, dim=-1) attn_weights = self.dropout(attn_weights) output = torch.matmul(attn_weights, v) # [B, H, L, dh] output = output.transpose(1, 2).contiguous().view(B, L, -1) # [B, L, H*dh] output = self.wo(output) return output, kv_cache # ç¤ºä¾‹ä½¿ç”¨ if __name__ == \"__main__\": batch_size = 2 seq_len = 8 d_model = 512 n_heads = 8 d_head = 64 d_latent = 128 # \u003c\u003c n_heads * d_head = 512 x = torch.randn(batch_size, seq_len, d_model) mla = MLAttention(d_model, n_heads, d_head, d_latent) # First forward (no cache) out1, cache = mla(x, kv_cache={}) print(\"Output shape:\", out1.shape) # [2, 8, 512] print(\"Cache z shape:\", cache['z'].shape) # [2, 8, 128] # Next token (simulate autoregressive) next_token = torch.randn(batch_size, 1, d_model) out2, cache = mla(next_token, kv_cache=cache) print(\"After one more token, cache z shape:\", cache['z'].shape) # [2, 9, 128] äº”ã€ä¼˜åŠ¿ä¸å±€é™ âœ… ä¼˜åŠ¿ KV Cache å‹ç¼©ç‡é«˜ï¼šç¼“å­˜ä» $ O(H d_h) $ é™è‡³ $ O(d_z) $ï¼Œå…¸å‹å‹ç¼©æ¯” 4xâ€“32xã€‚ æ¨ç†æ˜¾å­˜å¤§å¹…é™ä½ï¼šé€‚åˆé•¿ä¸Šä¸‹æ–‡éƒ¨ç½²ã€‚ è®­ç»ƒå…¼å®¹æ€§å¥½ï¼šç«¯åˆ°ç«¯å¯è®­ç»ƒï¼Œæ— éœ€ä¿®æ”¹æŸå¤±å‡½æ•°ã€‚ âš ï¸ å±€é™ é‡å»ºè¯¯å·®ï¼šK/V æ˜¯è¿‘ä¼¼é‡å»ºï¼Œå¯èƒ½å½±å“æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼ˆä½† DeepSeek-V2 è¡¨æ˜å½±å“å¯æ§ï¼‰ã€‚ é¢å¤–è®¡ç®—å¼€é”€ï¼šæ¯æ¬¡ attention éƒ½éœ€é‡å»º K/Vï¼Œå¢åŠ  FLOPsï¼ˆä½†å†…å­˜å¸¦å®½å¾€å¾€æ˜¯ç“¶é¢ˆï¼Œè®¡ç®—æ¢å†…å­˜å€¼å¾—ï¼‰ã€‚ éœ€è°ƒæ•´è®­ç»ƒç­–ç•¥ï¼šæ½œåœ¨ç©ºé—´éœ€å……åˆ†å­¦ä¹ ï¼Œå¯èƒ½éœ€æ›´å¤§å­¦ä¹ ç‡æˆ– warmupã€‚ å…­ã€æ€»ç»“ MLAï¼ˆMulti-head Latent Attentionï¼‰ æ˜¯ä¸€ç§é¢å‘é«˜æ•ˆæ¨ç†çš„æ³¨æ„åŠ›æœºåˆ¶åˆ›æ–°ï¼Œé€šè¿‡å¼•å…¥ä½ç»´æ½œåœ¨è¡¨ç¤º + åŠ¨æ€é‡å»ºï¼Œæ˜¾è‘—å‹ç¼© KV Cacheï¼Œå·²åœ¨ DeepSeek-V2 ç­‰å·¥ä¸šçº§å¤§æ¨¡å‹ä¸­éªŒè¯æœ‰æ•ˆæ€§ã€‚å®ƒä»£è¡¨äº†â€œç”¨è®¡ç®—æ¢å†…å­˜â€ çš„ç°ä»£å¤§æ¨¡å‹ä¼˜åŒ–èŒƒå¼ã€‚\nğŸ” æ³¨ï¼šè‹¥ä½ åœ¨å…¶ä»–æ–‡çŒ®ä¸­çœ‹åˆ° â€œMLAâ€ æŒ‡ä»£ä¸åŒæ¦‚å¿µï¼ˆå¦‚ Multi-Layer Attentionã€Meta-Learning Attention ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­ã€‚ä½†åœ¨ 2024â€“2026 å¹´å¤§æ¨¡å‹æ•ˆç‡ä¼˜åŒ–è¯­å¢ƒä¸‹ï¼ŒMLA â‰ˆ Multi-head Latent Attentionï¼ˆDeepSeek-V2ï¼‰ã€‚\nå‚è€ƒèµ„æ–™ DeepSeek-V2 Technical Report (2024). https://github.com/deepseek-ai/DeepSeek-V2 â€œEfficient Transformers: A Surveyâ€ (2022+) Googleâ€™s AFTã€Metaâ€™s MQA/GQA ç­‰ç›¸å…³å·¥ä½œ ",
  "wordCount" : "1043",
  "inLanguage": "en",
  "datePublished": "2026-01-07T00:00:00Z",
  "dateModified": "2026-01-07T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/architecture/mla/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="å¤§è¯­è¨€æ¨¡å‹ (LLM)">
                    <span>å¤§è¯­è¨€æ¨¡å‹ (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="å·¥å…·ç®± (toolbox)">
                    <span>å·¥å…·ç®± (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="å¼ºåŒ–å­¦ä¹  (RL)">
                    <span>å¼ºåŒ–å­¦ä¹  (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
                    <span>æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false }
            ],
            trust: true,
            throwOnError: false
        });
        });
    </script>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      MLAï¼šMulti-head Latent Attention
    </h1>
    <div class="post-meta"><span title='2026-01-07 00:00:00 +0000 UTC'>January 7, 2026</span>&nbsp;Â·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><h2 id="ä¸€mla-çš„ç”±æ¥">ä¸€ã€MLA çš„ç”±æ¥<a hidden class="anchor" aria-hidden="true" href="#ä¸€mla-çš„ç”±æ¥">#</a></h2>
<h3 id="èƒŒæ™¯kv-cache-æˆä¸ºæ¨ç†ç“¶é¢ˆ">èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ<a hidden class="anchor" aria-hidden="true" href="#èƒŒæ™¯kv-cache-æˆä¸ºæ¨ç†ç“¶é¢ˆ">#</a></h3>
<p>åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚</p>
<h3 id="ç›®æ ‡å‹ç¼©-kv-cache">ç›®æ ‡ï¼šå‹ç¼© KV Cache<a hidden class="anchor" aria-hidden="true" href="#ç›®æ ‡å‹ç¼©-kv-cache">#</a></h3>
<p>ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š</p>
<ul>
<li><strong>MQAï¼ˆMulti-Query Attentionï¼‰</strong>ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚</li>
<li><strong>GQAï¼ˆGrouped-Query Attentionï¼‰</strong>ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚</li>
<li><strong>MLAï¼ˆMulti-head Latent Attentionï¼‰</strong>ï¼š<strong>ä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/V</strong>ã€‚</li>
</ul>
<h3 id="æå‡ºè€…ä¸å‡ºå¤„">æå‡ºè€…ä¸å‡ºå¤„<a hidden class="anchor" aria-hidden="true" href="#æå‡ºè€…ä¸å‡ºå¤„">#</a></h3>
<ul>
<li><strong>DeepSeek-V2ï¼ˆ2024ï¼‰</strong> é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å <strong>MLAï¼ˆMulti-head Latent Attentionï¼‰</strong>ã€‚</li>
<li>æ ¸å¿ƒæ€æƒ³ï¼šç”¨ <strong>ä½ç§©æ½œåœ¨å‘é‡ + å°å‹æŠ•å½±ç½‘ç»œ</strong> ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚</li>
</ul>
<img src="MLA.jpg" alt="MLA" style="width:80%;height:auto;">
<hr>
<h2 id="äºŒåŸºæœ¬åŸç†">äºŒã€åŸºæœ¬åŸç†<a hidden class="anchor" aria-hidden="true" href="#äºŒåŸºæœ¬åŸç†">#</a></h2>
<p>MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š</p>
<blockquote>
<p><strong>ä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \in \mathbb{R}^{d_z} $ï¼ˆ$ d_z \ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚</strong></p>
</blockquote>
<p>å…·ä½“æµç¨‹ï¼š</p>
<ol>
<li>å¯¹æ¯ä¸ªè¾“å…¥ token $ x_t $ï¼Œå…ˆè®¡ç®—ä¸€ä¸ª <strong>å…±äº«çš„æ½œåœ¨è¡¨ç¤º $ z_t $</strong>ã€‚</li>
<li>åœ¨ attention è®¡ç®—æ—¶ï¼Œå¯¹æ¯ä¸ª head $ i $ï¼Œç”¨ä¸¤ä¸ªå°å‹ MLPï¼ˆæˆ–çº¿æ€§å±‚ï¼‰å°† $ z_t $ æ˜ å°„ä¸ºè¯¥ head çš„ $ k_{t,i} $ å’Œ $ v_{t,i} $ã€‚</li>
<li>ç¼“å­˜çš„æ˜¯ $ z_t $ è€Œé $ k_t, v_t $ï¼Œæ˜¾è‘—èŠ‚çœå†…å­˜ã€‚</li>
</ol>
<p>ç”±äº $ z_t $ ç»´åº¦è¿œå°äºåŸå§‹ K/Vï¼ˆä¾‹å¦‚ $ d_z = 128 $ï¼Œè€Œ $ d_k = d_v = 128 \times 8 = 1024 $ï¼‰ï¼Œä¸”é‡å»ºç½‘ç»œå‚æ•°é‡å°ï¼Œæ•´ä½“æ•ˆç‡æ›´é«˜ã€‚</p>
<hr>
<h2 id="ä¸‰è¯¦ç»†æ•°å­¦ç»†èŠ‚">ä¸‰ã€è¯¦ç»†æ•°å­¦ç»†èŠ‚<a hidden class="anchor" aria-hidden="true" href="#ä¸‰è¯¦ç»†æ•°å­¦ç»†èŠ‚">#</a></h2>
<p>è®¾ï¼š</p>
<ul>
<li>è¾“å…¥ token è¡¨ç¤ºï¼š$ x_t \in \mathbb{R}^{d_{\text{model}}} $</li>
<li>æ½œåœ¨ç»´åº¦ï¼š$ d_z $</li>
<li>æ³¨æ„åŠ›å¤´æ•°ï¼š$ H $</li>
<li>æ¯ä¸ª head çš„ key/value ç»´åº¦ï¼š$ d_k = d_v = d_h $</li>
</ul>
<h3 id="æ­¥éª¤-1ç”Ÿæˆæ½œåœ¨å‘é‡">æ­¥éª¤ 1ï¼šç”Ÿæˆæ½œåœ¨å‘é‡<a hidden class="anchor" aria-hidden="true" href="#æ­¥éª¤-1ç”Ÿæˆæ½œåœ¨å‘é‡">#</a></h3>
<p>é€šè¿‡ä¸€ä¸ªå…±äº«çº¿æ€§å±‚ç”Ÿæˆæ½œåœ¨å‘é‡ï¼š
$$
z_t = W_z x_t + b_z \quad \in \mathbb{R}^{d_z}
$$
å…¶ä¸­ $ W_z \in \mathbb{R}^{d_z \times d_{\text{model}}} $</p>
<blockquote>
<p>å®é™…å®ç°ä¸­ï¼Œ$ z_t $ å¯èƒ½ç»è¿‡ LayerNorm æˆ–å…¶ä»–å½’ä¸€åŒ–ã€‚</p>
</blockquote>
<h3 id="æ­¥éª¤-2æŒ‰å¤´é‡å»º-k-å’Œ-v">æ­¥éª¤ 2ï¼šæŒ‰å¤´é‡å»º K å’Œ V<a hidden class="anchor" aria-hidden="true" href="#æ­¥éª¤-2æŒ‰å¤´é‡å»º-k-å’Œ-v">#</a></h3>
<p>å¯¹æ¯ä¸ª head $ h \in {1, &hellip;, H} $ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„é‡å»ºçŸ©é˜µï¼š
$
k_{t,h} = W_{K,h} z_t \quad \in \mathbb{R}^{d_h} \
v_{t,h} = W_{V,h} z_t \quad \in \mathbb{R}^{d_h}
$
å…¶ä¸­ï¼š</p>
<ul>
<li>$ W_{K,h} \in \mathbb{R}^{d_h \times d_z} $</li>
<li>$ W_{V,h} \in \mathbb{R}^{d_h \times d_z} $</li>
</ul>
<p>ä¸ºå‡å°‘å‚æ•°é‡ï¼Œé€šå¸¸å°†æ‰€æœ‰å¤´çš„æƒé‡å †å ï¼š
$
W_K \in \mathbb{R}^{H \cdot d_h \times d_z}, \quad
W_V \in \mathbb{R}^{H \cdot d_h \times d_z}
$</p>
<p>åˆ™å¯æ‰¹é‡è®¡ç®—ï¼š
$
K = (W_K z_t) \in \mathbb{R}^{H d_h} \Rightarrow \text{reshape to } (H, d_h) \
V = (W_V z_t) \in \mathbb{R}^{H d_h} \Rightarrow \text{reshape to } (H, d_h)
$</p>
<h3 id="æ­¥éª¤-3æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—">æ­¥éª¤ 3ï¼šæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—<a hidden class="anchor" aria-hidden="true" href="#æ­¥éª¤-3æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—">#</a></h3>
<p>æŸ¥è¯¢ä»ç”±ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆï¼ˆå›  Q ä¸éœ€ç¼“å­˜ï¼‰ï¼š
$
Q = x_t W_Q \in \mathbb{R}^{H \times d_h}
$</p>
<p>ç„¶åè¿›è¡Œæ ‡å‡† scaled dot-product attentionï¼š
$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_h}} \right) V
$</p>
<h3 id="ç¼“å­˜ç­–ç•¥">ç¼“å­˜ç­–ç•¥<a hidden class="anchor" aria-hidden="true" href="#ç¼“å­˜ç­–ç•¥">#</a></h3>
<ul>
<li><strong>ç¼“å­˜å†…å®¹</strong>ï¼šä»…ç¼“å­˜ $ {z_1, z_2, &hellip;, z_t} $ï¼Œè€Œé $ K, V $ã€‚</li>
<li><strong>ç¼“å­˜å¤§å°</strong>ï¼šä» $ O(T \cdot H \cdot d_h) $ é™è‡³ $ O(T \cdot d_z) $ã€‚</li>
<li>è‹¥ $ d_z \ll H \cdot d_h $ï¼ˆä¾‹å¦‚ $ d_z = 128 $, $ H=32, d_h=128 \Rightarrow H d_h = 4096 $ï¼‰ï¼Œåˆ™å‹ç¼©æ¯”è¾¾ <strong>32x</strong>ã€‚</li>
</ul>
<hr>
<h2 id="å››pytorch-å®ç°ç®€åŒ–ç‰ˆ">å››ã€PyTorch å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰<a hidden class="anchor" aria-hidden="true" href="#å››pytorch-å®ç°ç®€åŒ–ç‰ˆ">#</a></h2>
<p>ä»¥ä¸‹æ˜¯ä¸€ä¸ª <strong>å•å±‚ MLA æ³¨æ„åŠ›æ¨¡å—</strong> çš„ PyTorch å®ç°ï¼Œç”¨äºæ¼”ç¤ºåŸç†ï¼š</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_latent</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Multi-head Latent Attention (MLA) as in DeepSeek-V2.
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            d_model: model dimension (e.g., 4096)
</span></span></span><span class="line"><span class="cl"><span class="s2">            n_heads: number of attention heads
</span></span></span><span class="line"><span class="cl"><span class="s2">            d_head: dimension per head (e.g., 128)
</span></span></span><span class="line"><span class="cl"><span class="s2">            d_latent: latent dimension for KV compression (e.g., 128)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_head</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_latent</span> <span class="o">=</span> <span class="n">d_latent</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">d_head</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Query projection (not compressed)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Latent vector projection (shared for all heads)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wz</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Reconstruction matrices for K and V (per head, but stored as big matrices)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wk_recon</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wv_recon</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Output projection</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize reconstruction matrices (optional: Xavier)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wk_recon</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv_recon</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># { &#39;z&#39;: [seq_len, d_latent] }</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        x: [batch, seq_len, d_model]
</span></span></span><span class="line"><span class="cl"><span class="s2">        kv_cache: optional dict to store/reuse latent vectors
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            output: [batch, seq_len, d_model]
</span></span></span><span class="line"><span class="cl"><span class="s2">            new_kv_cache: updated cache with latent vectors
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Project queries</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [B, L, H * dh]</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, H, L, dh]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Project to latent space</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wz</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [B, L, d_latent]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Update KV cache</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="s1">&#39;z&#39;</span> <span class="ow">in</span> <span class="n">kv_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv_cache</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">z</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># prepend cached latents</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv_cache</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># update cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Reconstruct K and V from latent vectors</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># z: [B, T, d_latent] where T = current total length</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Reconstruct all K and V at once</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_flat</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk_recon</span><span class="p">)</span>  <span class="c1"># [B, T, H * dh]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_flat</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_recon</span><span class="p">)</span>  <span class="c1"># [B, T, H * dh]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">k_flat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, H, T, dh]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">v_flat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, H, T, dh]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Scaled dot-product attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>  <span class="c1"># [B, H, L, T]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># [B, H, L, dh]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, L, H*dh]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">kv_cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ç¤ºä¾‹ä½¿ç”¨</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">    <span class="n">d_head</span> <span class="o">=</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl">    <span class="n">d_latent</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># &lt;&lt; n_heads * d_head = 512</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mla</span> <span class="o">=</span> <span class="n">MLAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># First forward (no cache)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out1</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">mla</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="p">{})</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Output shape:&#34;</span><span class="p">,</span> <span class="n">out1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>      <span class="c1"># [2, 8, 512]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Cache z shape:&#34;</span><span class="p">,</span> <span class="n">cache</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># [2, 8, 128]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Next token (simulate autoregressive)</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">mla</span><span class="p">(</span><span class="n">next_token</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;After one more token, cache z shape:&#34;</span><span class="p">,</span> <span class="n">cache</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># [2, 9, 128]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h2 id="äº”ä¼˜åŠ¿ä¸å±€é™">äº”ã€ä¼˜åŠ¿ä¸å±€é™<a hidden class="anchor" aria-hidden="true" href="#äº”ä¼˜åŠ¿ä¸å±€é™">#</a></h2>
<h3 id="-ä¼˜åŠ¿">âœ… ä¼˜åŠ¿<a hidden class="anchor" aria-hidden="true" href="#-ä¼˜åŠ¿">#</a></h3>
<ul>
<li><strong>KV Cache å‹ç¼©ç‡é«˜</strong>ï¼šç¼“å­˜ä» $ O(H d_h) $ é™è‡³ $ O(d_z) $ï¼Œå…¸å‹å‹ç¼©æ¯” 4xâ€“32xã€‚</li>
<li><strong>æ¨ç†æ˜¾å­˜å¤§å¹…é™ä½</strong>ï¼šé€‚åˆé•¿ä¸Šä¸‹æ–‡éƒ¨ç½²ã€‚</li>
<li><strong>è®­ç»ƒå…¼å®¹æ€§å¥½</strong>ï¼šç«¯åˆ°ç«¯å¯è®­ç»ƒï¼Œæ— éœ€ä¿®æ”¹æŸå¤±å‡½æ•°ã€‚</li>
</ul>
<h3 id="-å±€é™">âš ï¸ å±€é™<a hidden class="anchor" aria-hidden="true" href="#-å±€é™">#</a></h3>
<ul>
<li><strong>é‡å»ºè¯¯å·®</strong>ï¼šK/V æ˜¯è¿‘ä¼¼é‡å»ºï¼Œå¯èƒ½å½±å“æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼ˆä½† DeepSeek-V2 è¡¨æ˜å½±å“å¯æ§ï¼‰ã€‚</li>
<li><strong>é¢å¤–è®¡ç®—å¼€é”€</strong>ï¼šæ¯æ¬¡ attention éƒ½éœ€é‡å»º K/Vï¼Œå¢åŠ  FLOPsï¼ˆä½†å†…å­˜å¸¦å®½å¾€å¾€æ˜¯ç“¶é¢ˆï¼Œè®¡ç®—æ¢å†…å­˜å€¼å¾—ï¼‰ã€‚</li>
<li><strong>éœ€è°ƒæ•´è®­ç»ƒç­–ç•¥</strong>ï¼šæ½œåœ¨ç©ºé—´éœ€å……åˆ†å­¦ä¹ ï¼Œå¯èƒ½éœ€æ›´å¤§å­¦ä¹ ç‡æˆ– warmupã€‚</li>
</ul>
<hr>
<h2 id="å…­æ€»ç»“">å…­ã€æ€»ç»“<a hidden class="anchor" aria-hidden="true" href="#å…­æ€»ç»“">#</a></h2>
<p><strong>MLAï¼ˆMulti-head Latent Attentionï¼‰</strong> æ˜¯ä¸€ç§é¢å‘é«˜æ•ˆæ¨ç†çš„æ³¨æ„åŠ›æœºåˆ¶åˆ›æ–°ï¼Œé€šè¿‡å¼•å…¥<strong>ä½ç»´æ½œåœ¨è¡¨ç¤º + åŠ¨æ€é‡å»º</strong>ï¼Œæ˜¾è‘—å‹ç¼© KV Cacheï¼Œå·²åœ¨ DeepSeek-V2 ç­‰å·¥ä¸šçº§å¤§æ¨¡å‹ä¸­éªŒè¯æœ‰æ•ˆæ€§ã€‚å®ƒä»£è¡¨äº†â€œ<strong>ç”¨è®¡ç®—æ¢å†…å­˜</strong>â€ çš„ç°ä»£å¤§æ¨¡å‹ä¼˜åŒ–èŒƒå¼ã€‚</p>
<blockquote>
<p>ğŸ” æ³¨ï¼šè‹¥ä½ åœ¨å…¶ä»–æ–‡çŒ®ä¸­çœ‹åˆ° â€œMLAâ€ æŒ‡ä»£ä¸åŒæ¦‚å¿µï¼ˆå¦‚ Multi-Layer Attentionã€Meta-Learning Attention ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­ã€‚ä½†åœ¨ 2024â€“2026 å¹´å¤§æ¨¡å‹æ•ˆç‡ä¼˜åŒ–è¯­å¢ƒä¸‹ï¼Œ<strong>MLA â‰ˆ Multi-head Latent Attentionï¼ˆDeepSeek-V2ï¼‰</strong>ã€‚</p>
</blockquote>
<hr>
<h3 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™<a hidden class="anchor" aria-hidden="true" href="#å‚è€ƒèµ„æ–™">#</a></h3>
<ul>
<li>DeepSeek-V2 Technical Report (2024). <a href="https://github.com/deepseek-ai/DeepSeek-V2">https://github.com/deepseek-ai/DeepSeek-V2</a></li>
<li>&ldquo;Efficient Transformers: A Survey&rdquo; (2022+)</li>
<li>Googleâ€™s <strong>AFT</strong>ã€Metaâ€™s <strong>MQA/GQA</strong> ç­‰ç›¸å…³å·¥ä½œ</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
