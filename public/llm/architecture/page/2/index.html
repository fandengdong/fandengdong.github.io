<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLM architecture | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="大语言模型的架构解析">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/llm/architecture/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="LLM architecture">
  <meta property="og:description" content="大语言模型的架构解析">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LLM architecture">
<meta name="twitter:description" content="大语言模型的架构解析">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "大语言模型 (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span>欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    LLM architecture
  </h1>
  <div class="post-description">
    大语言模型的架构解析
  </div>
</header>
<div class="post-content"><p>本节尝试将大语言模型的架构进行解析，并希望同时给出代码实现，已帮助更高的理解大语言模型的构建。此外，基于transformer架构，我们希望能够将架构的解读延续到当前的多模态大模型。</p>


</div>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">KV cache - 大模型高效推理的基石
    </h2>
  </header>
  <div class="entry-content">
    <p>KV Cache（Key-Value Cache）是大语言模型（LLM, Large Language Model）推理过程中用于加速自回归生成的一项关键技术。它通过缓存先前 token 的 Key 和 Value 向量，避免在生成新 token 时重复计算已处理上下文的注意力信息，从而显著提升推理效率。
一、为什么需要 KV Cache？ 1. 自回归生成的本质 大语言模型通常以自回归方式生成文本：每次只预测一个 token，然后将该 token 拼接到输入序列末尾，再预测下一个 token。例如：
输入: &#34;今天天气&#34; 第1步输出: &#34;真&#34; 输入变为: &#34;今天天气真&#34; 第2步输出: &#34;好&#34; ... 2. 注意力机制的重复计算问题 Transformer 使用 自注意力机制（Self-Attention），对长度为 $n$ 的序列，每个 token 都要与其他所有 token 计算注意力权重。
假设当前已生成 $t$ 个 token，现在要生成第 $t&#43;1$ 个 token。若每次都重新计算整个长度为 $t&#43;1$ 的序列的 Q、K、V，那么：
第1步：计算1个token → 1次QKV 第2步：计算2个token → 2次QKV（但前1个其实已经算过） … 第$t$步：计算$t$个token → 前$t-1$个重复计算！ 这导致 时间复杂度为 $O(n^2)$，且大量重复计算。
3. KV Cache 的提出 为解决此问题，研究者提出：在生成过程中缓存每个 token 对应的 K（Key）和 V（Value）向量。因为：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-13 00:00:00 +0000 UTC'>January 13, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to KV cache - 大模型高效推理的基石" href="http://localhost:1313/llm/architecture/kv-cache/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Mamba - A Transformer-like Architecture for Long Sequence Modeling
    </h2>
  </header>
  <div class="entry-content">
    <p>一、Mamba 的设计思想 背景：Transformer 的局限性 自注意力机制的时间复杂度为 $O(L^2)$（L 为序列长度），对长序列（如基因组、高分辨率音频、长文本）效率低。 线性注意力等近似方法牺牲了建模能力。 RNN / SSM（状态空间模型） 具有线性复杂度 $O(L)$，但传统 SSM（如 Linear Time-Invariant SSM, LTI-SSM）是时不变的，无法像 Transformer 那样根据输入内容动态调整行为。 Mamba 的核心创新 Selective State Space Model (SSM) —— 将 SSM 与输入相关（input-dependent），使其具备上下文感知能力，同时保持线性复杂度。
关键点：
选择性（Selectivity）：SSM 的参数（如 A, B, C）不再是固定或仅时间相关的，而是由当前输入 token 动态生成。 硬件感知设计：利用现代 GPU 的并行特性，通过“扫描（scan）&#43; 并行前缀”实现高效训练。 结构简单：没有 attention，只有 MLP &#43; SSM block。 二、Mamba 架构概览 Mamba Block 替代了 Transformer 中的 Attention &#43; MLP 子层：
Input → LayerNorm → SSM (Selective SSM) → Residual Add → LayerNorm → MLP → Residual Add → Output 其中最核心的是 Selective SSM 模块。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-13 00:00:00 +0000 UTC'>January 13, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to Mamba - A Transformer-like Architecture for Long Sequence Modeling" href="http://localhost:1313/llm/architecture/mamba/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GDN - Qwen3-Next中引入的一种状态空间模型变体
    </h2>
  </header>
  <div class="entry-content">
    <p>Ref: 知乎
Gated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。
下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。
一、由来与动机 1.1 背景：Transformer 的局限 标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。 对于超长上下文（如 100K&#43; tokens），注意力机制变得不可扩展。 1.2 状态空间模型（SSM）的兴起 SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。 其核心是维护一个隐状态 $h_t$，通过递推更新： $$ h_t = \bar{A} h_{t-1} &#43; \bar{B} x_t $$ $$ y_t = C h_t $$ 其中 $\bar{A}, \bar{B}$ 是离散化后的系统矩阵。 1.3 Delta Rule 与 Hebbian 学习 Delta Rule 是一种增量学习规则，形式为： $$ \Delta W \propto (y_{\text{target}} - y) x^\top $$ 在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。 1.4 Gated DeltaNet 的提出 Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。 它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。 二、Gated DeltaNet 的数学形式 Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \in \mathbb{R}^{d_k \times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-09 00:00:00 +0000 UTC'>January 9, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to GDN - Qwen3-Next中引入的一种状态空间模型变体" href="http://localhost:1313/llm/architecture/gdn/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">mHC - Manifold-Constrained Hyper-Connections 流形约束超链接
    </h2>
  </header>
  <div class="entry-content">
    <p>Ref: mHC: Manifold-Constrained Hyper-Connections Introduction 自从ResNets的提出，深度神经网络架构经历了快速的演进。一个常规的ResNet层结果如上面图(a)所示，其数学表达式描述为：
$$ \mathbf{x}_{l&#43;1} = \mathbf{x}_l &#43; \mathcal{F}(\mathbf{x}_l, W_l) \tag{1} $$
其中$\mathbf{x}l$和$\mathbf{x}{l&#43;1}$表示输入和输出向量，是一个$C$维度的向量。$\mathcal{F}$表示残差函数，$W_l$表示权重矩阵。尽管残差函数F在过去十年中已发展出卷积、注意力机制和前馈网络等多种操作，但残差连接的范式仍保持其原始形式。随着Transformer架构的发展，该范式目前已确立为大型语言模型（LLMs）的基本设计元素。
这一成功主要归功于残差连接的简洁形式。更重要的是，早期研究揭示了残差连接的恒等映射特性在大规模训练中保持稳定性和效率。通过递归扩展残差连接跨越多层，等式（1）得出：
$$ \mathbf{x}_{L} = \mathbf{x}l &#43; \sum{i=l}^{L-1} \mathcal{F}(\mathbf{x}_l, W_l) \tag{2} $$
其中$L$和$l$分别对应较深和较浅的层。恒等映射项指的是组件$x_l$本身，强调了来自较浅层的信号未经任何修改直接映射到较深层的特性。
近期，以超连接（Hyper-Connections，HC）（Zhu 等人，2024）为代表的研究为残差连接引入了新维度，并通过实证验证了其性能潜力。HC的单层架构如图1(b)所示。通过扩展残差流宽度并增强连接复杂度，HC在不改变单个单元浮点运算（FLOPs）计算开销的前提下，显著提升了拓扑复杂度。形式化地，HC中的单层传播定义为：
$$ \mathbf{x}_{l&#43;1} = \mathcal{H}_l^{\text{res}} \mathbf{x}_l &#43; \mathcal{H}_l^{\text{post}}{}^\top \mathcal{F}(\mathcal{H}_l^{\text{pre}} \mathbf{x}_l, W_l) \tag{3} $$
其中，$\mathbf{x}l$ 和 $\mathbf{x}{l&#43;1}$ 分别表示第 $l$ 层的输入和输出。与公式 (1) 中的表述不同，$\mathbf{x}l$ 和 $\mathbf{x}{l&#43;1}$ 的特征维度从 $C$ 扩展到了 $n \times C$，其中 $n$ 是扩展倍率（expansion rate）。项 $\mathcal{H}_l^{\text{res}} \in \mathbb{R}^{n \times n}$ 表示一个可学习的映射，用于在残差流（residual stream）内部混合特征。同样作为可学习映射，$\mathcal{H}_l^{\text{pre}} \in \mathbb{R}^{1 \times n}$ 将来自 $nC$ 维流中的特征聚合为一个 $C$ 维的层输入；反之，$\mathcal{H}_l^{\text{post}} \in \mathbb{R}^{1 \times n}$ 将层的输出重新映射回原始流中。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-01-08 00:00:00 +0000 UTC'>January 8, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to mHC - Manifold-Constrained Hyper-Connections 流形约束超链接" href="http://localhost:1313/llm/architecture/mhc/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/llm/architecture/">
      «&nbsp;Prev&nbsp;
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
