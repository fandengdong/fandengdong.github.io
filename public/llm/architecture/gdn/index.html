<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GDN - Qwen3-Next中引入的一种状态空间模型变体 | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="Ref: 知乎
Gated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。
下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。

一、由来与动机
1.1 背景：Transformer 的局限

标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。
对于超长上下文（如 100K&#43; tokens），注意力机制变得不可扩展。

1.2 状态空间模型（SSM）的兴起

SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。
其核心是维护一个隐状态 $h_t$，通过递推更新：
$$
h_t = \bar{A} h_{t-1} &#43; \bar{B} x_t
$$
$$
y_t = C h_t
$$
其中 $\bar{A}, \bar{B}$ 是离散化后的系统矩阵。

1.3 Delta Rule 与 Hebbian 学习

Delta Rule 是一种增量学习规则，形式为：
$$
\Delta W \propto (y_{\text{target}} - y) x^\top
$$
在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。

1.4 Gated DeltaNet 的提出

Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。
它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。


二、Gated DeltaNet 的数学形式
Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \in \mathbb{R}^{d_k \times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/architecture/gdn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/architecture/gdn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/architecture/gdn/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="GDN - Qwen3-Next中引入的一种状态空间模型变体">
  <meta property="og:description" content="Ref: 知乎
Gated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。
下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。
一、由来与动机 1.1 背景：Transformer 的局限 标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。 对于超长上下文（如 100K&#43; tokens），注意力机制变得不可扩展。 1.2 状态空间模型（SSM）的兴起 SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。 其核心是维护一个隐状态 $h_t$，通过递推更新： $$ h_t = \bar{A} h_{t-1} &#43; \bar{B} x_t $$ $$ y_t = C h_t $$ 其中 $\bar{A}, \bar{B}$ 是离散化后的系统矩阵。 1.3 Delta Rule 与 Hebbian 学习 Delta Rule 是一种增量学习规则，形式为： $$ \Delta W \propto (y_{\text{target}} - y) x^\top $$ 在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。 1.4 Gated DeltaNet 的提出 Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。 它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。 二、Gated DeltaNet 的数学形式 Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \in \mathbb{R}^{d_k \times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-01-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-09T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GDN - Qwen3-Next中引入的一种状态空间模型变体">
<meta name="twitter:description" content="Ref: 知乎
Gated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。
下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。

一、由来与动机
1.1 背景：Transformer 的局限

标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。
对于超长上下文（如 100K&#43; tokens），注意力机制变得不可扩展。

1.2 状态空间模型（SSM）的兴起

SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。
其核心是维护一个隐状态 $h_t$，通过递推更新：
$$
h_t = \bar{A} h_{t-1} &#43; \bar{B} x_t
$$
$$
y_t = C h_t
$$
其中 $\bar{A}, \bar{B}$ 是离散化后的系统矩阵。

1.3 Delta Rule 与 Hebbian 学习

Delta Rule 是一种增量学习规则，形式为：
$$
\Delta W \propto (y_{\text{target}} - y) x^\top
$$
在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。

1.4 Gated DeltaNet 的提出

Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。
它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。


二、Gated DeltaNet 的数学形式
Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \in \mathbb{R}^{d_k \times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "大语言模型 (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM architecture",
      "item": "http://localhost:1313/llm/architecture/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "GDN - Qwen3-Next中引入的一种状态空间模型变体",
      "item": "http://localhost:1313/llm/architecture/gdn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GDN - Qwen3-Next中引入的一种状态空间模型变体",
  "name": "GDN - Qwen3-Next中引入的一种状态空间模型变体",
  "description": "Ref: 知乎\nGated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。\n下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。\n一、由来与动机 1.1 背景：Transformer 的局限 标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。 对于超长上下文（如 100K+ tokens），注意力机制变得不可扩展。 1.2 状态空间模型（SSM）的兴起 SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。 其核心是维护一个隐状态 $h_t$，通过递推更新： $$ h_t = \\bar{A} h_{t-1} + \\bar{B} x_t $$ $$ y_t = C h_t $$ 其中 $\\bar{A}, \\bar{B}$ 是离散化后的系统矩阵。 1.3 Delta Rule 与 Hebbian 学习 Delta Rule 是一种增量学习规则，形式为： $$ \\Delta W \\propto (y_{\\text{target}} - y) x^\\top $$ 在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。 1.4 Gated DeltaNet 的提出 Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。 它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。 二、Gated DeltaNet 的数学形式 Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \\in \\mathbb{R}^{d_k \\times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。\n",
  "keywords": [
    
  ],
  "articleBody": "Ref: 知乎\nGated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种状态空间模型（State Space Model, SSM）变体，用于增强 Transformer 架构在长序列建模中的能力。它融合了 Delta Rule（增量学习规则）、门控机制（Gating） 和 状态空间建模思想，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。\n下面我们将从由来、演变、集成方式、数学形式到 PyTorch 实现 一步步解析 Gated DeltaNet。\n一、由来与动机 1.1 背景：Transformer 的局限 标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。 对于超长上下文（如 100K+ tokens），注意力机制变得不可扩展。 1.2 状态空间模型（SSM）的兴起 SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。 其核心是维护一个隐状态 $h_t$，通过递推更新： $$ h_t = \\bar{A} h_{t-1} + \\bar{B} x_t $$ $$ y_t = C h_t $$ 其中 $\\bar{A}, \\bar{B}$ 是离散化后的系统矩阵。 1.3 Delta Rule 与 Hebbian 学习 Delta Rule 是一种增量学习规则，形式为： $$ \\Delta W \\propto (y_{\\text{target}} - y) x^\\top $$ 在神经记忆模型中，Delta Rule 可被解释为快速联想记忆更新：新输入 $x_t$ 与当前输出误差共同驱动权重更新。 1.4 Gated DeltaNet 的提出 Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种可学习的、门控控制的记忆更新机制。 它不是直接更新权重，而是维护一个键值记忆矩阵 $M_t$，并通过门控机制进行增量更新。 二、Gated DeltaNet 的数学形式 Gated DeltaNet 的核心是维护一个外部记忆矩阵 $M_t \\in \\mathbb{R}^{d_k \\times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。\n设输入序列为 ${x_t}_{t=1}^L$，每个 token 映射为：\n查询向量：$q_t = W_q x_t$ 键向量：$k_t = W_k x_t$ 值向量：$v_t = W_v x_t$ 2.1 记忆更新规则（Delta Rule 风格） $$ M_t = \\lambda_t M_{t-1} + \\eta_t k_t v_t^\\top $$ 其中：\n$\\lambda_t \\in [0,1]$ 是遗忘门（forget gate），控制旧记忆的保留程度； $\\eta_t \\in [0,1]$ 是写入门（write gate），控制新信息的写入强度； $k_t v_t^\\top$ 是外积形式的“联想记忆”项，类似 Hebbian 学习。 2.2 输出计算 $$ o_t = q_t^\\top M_t $$ 即用当前查询 $q_t$ 从记忆矩阵 $M_t$ 中读取信息。\n2.3 门控机制（Gating） $$ \\lambda_t = \\sigma(g_\\lambda^\\top x_t), \\quad \\eta_t = \\sigma(g_\\eta^\\top x_t) $$ 其中 $\\sigma$ 是 sigmoid，$g_\\lambda, g_\\eta$ 是可学习向量（或通过小型 MLP 实现）。\n注意：实际实现中，为避免显式存储 $M_t$（其大小为 $d_k \\times d_v$），通常采用低秩近似或递推展开技巧。但在 Gated DeltaNet 中，由于 $d_k, d_v$ 通常较小（如 64），直接维护 $M_t$ 是可行的。\n三、在 Transformer 中的集成方式 Gated DeltaNet 通常作为 替代或补充标准自注意力模块 的组件，集成在 Transformer 层中：\n方案 A（替代）：完全用 Gated DeltaNet 替代 Multi-Head Attention（MHA），形成纯 SSM 架构。 方案 B（混合）：在 MHA 之后添加 Gated DeltaNet 作为“记忆增强”模块（类似 Linear Transformer 或 RWKV 的思路）。 Qwen3-Next 采用的是混合架构：在某些层使用 Gated DeltaNet，尤其在处理长上下文时。 其位置通常如下：\n[Input] → LayerNorm → GatedDeltaNet → Residual → LayerNorm → FFN → Residual → [Output] 四、PyTorch 实现 下面是一个简化但功能完整的 Gated DeltaNet 模块实现（支持 batch 和序列维度）(注意：仅供参考，某些细节可能不同）：\nimport torch import torch.nn as nn import torch.nn.functional as F class GatedDeltaNet(nn.Module): def __init__(self, d_model: int, d_k: int = 64, d_v: int = 64): super().__init__() self.d_model = d_model self.d_k = d_k self.d_v = d_v # Projection matrices self.W_q = nn.Linear(d_model, d_k, bias=False) self.W_k = nn.Linear(d_model, d_k, bias=False) self.W_v = nn.Linear(d_model, d_v, bias=False) # Gating parameters (simplified: use linear + sigmoid) self.gate_proj = nn.Linear(d_model, 2, bias=True) # outputs [logit_lambda, logit_eta] self._reset_parameters() def _reset_parameters(self): nn.init.xavier_uniform_(self.W_q.weight) nn.init.xavier_uniform_(self.W_k.weight) nn.init.xavier_uniform_(self.W_v.weight) nn.init.zeros_(self.gate_proj.bias) nn.init.zeros_(self.gate_proj.weight) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: \"\"\" x: (batch_size, seq_len, d_model) returns: (batch_size, seq_len, d_v) \"\"\" B, L, D = x.shape q = self.W_q(x) # (B, L, d_k) k = self.W_k(x) # (B, L, d_k) v = self.W_v(x) # (B, L, d_v) # Compute gates: lambda (forget), eta (write) gate_logits = self.gate_proj(x) # (B, L, 2) lambda_t = torch.sigmoid(gate_logits[..., 0]) # (B, L) eta_t = torch.sigmoid(gate_logits[..., 1]) # (B, L) # Initialize memory M: (B, d_k, d_v) M = torch.zeros(B, self.d_k, self.d_v, device=x.device, dtype=x.dtype) outputs = [] for t in range(L): # Update memory: M_t = lambda_t * M_{t-1} + eta_t * (k_t ⊗ v_t) kt = k[:, t].unsqueeze(2) # (B, d_k, 1) vt = v[:, t].unsqueeze(1) # (B, 1, d_v) outer = kt @ vt # (B, d_k, d_v) lambda_t_i = lambda_t[:, t].view(B, 1, 1) # (B, 1, 1) eta_t_i = eta_t[:, t].view(B, 1, 1) # (B, 1, 1) M = lambda_t_i * M + eta_t_i * outer # Read: o_t = q_t^T M qt = q[:, t].unsqueeze(1) # (B, 1, d_k) ot = qt @ M # (B, 1, d_v) outputs.append(ot.squeeze(1)) output = torch.stack(outputs, dim=1) # (B, L, d_v) return output 说明： 该实现是顺序递推的，因此时间复杂度为 $O(L \\cdot d_k \\cdot d_v)$，空间复杂度为 $O(d_k \\cdot d_v)$（不随 L 增长）。 若 d_k = d_v = 64，则每步更新仅需约 4K 参数操作，远低于注意力的 $O(L^2)$。 实际部署中可进一步优化（如使用 torch.compile 或 CUDA kernel）。 五、与 Mamba / SSM 的区别 特性 Mamba Gated DeltaNet 核心机制 连续 SSM + 选择性扫描 外积记忆 + 门控增量更新 状态形式 向量 $h_t \\in \\mathbb{R}^N$ 矩阵 $M_t \\in \\mathbb{R}^{d_k \\times d_v}$ 更新规则 $h_t = A h_{t-1} + B x_t$ $M_t = \\lambda M_{t-1} + \\eta k_t v_t^\\top$ 可解释性 黑盒动态系统 显式键值记忆，类似快速 Hebbian 学习 六、总结 Gated DeltaNet 是 Qwen3-Next 中一种创新的高效长程建模模块，其核心思想是：\n利用外积形式构建联想记忆； 通过双门控机制（遗忘门 + 写入门）实现可控的增量更新； 以线性复杂度替代注意力，同时保留对关键信息的长期记忆能力。 它代表了大模型架构从“纯注意力”向“混合记忆-注意力”演进的重要一步。\n注：截至 2026 年初，Gated DeltaNet 的细节尚未完全公开，以上解析基于 Qwen 团队技术报告、ICLR/NeurIPS 相关工作（如 DeltaNet、Hebbian Transformer、Mamba）以及合理推断。实际 Qwen3-Next 实现可能包含更多工程优化（如分组、低秩、并行化等）。\n",
  "wordCount" : "635",
  "inLanguage": "en",
  "datePublished": "2025-01-09T00:00:00Z",
  "dateModified": "2025-01-09T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/architecture/gdn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span>欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false }
            ],
            trust: true,
            throwOnError: false
        });
        });
    </script>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      GDN - Qwen3-Next中引入的一种状态空间模型变体
    </h1>
    <div class="post-meta"><span title='2025-01-09 00:00:00 +0000 UTC'>January 9, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><p>Ref: <a href="https://zhuanlan.zhihu.com/p/1970797248171475540">知乎</a></p>
<p>Gated DeltaNet 是 Qwen3-Next（通义千问 3 的下一代模型）中引入的一种<strong>状态空间模型（State Space Model, SSM）变体</strong>，用于增强 Transformer 架构在长序列建模中的能力。它融合了 <strong>Delta Rule（增量学习规则）</strong>、<strong>门控机制（Gating）</strong> 和 <strong>状态空间建模思想</strong>，旨在解决传统注意力机制在处理超长上下文时的计算和内存瓶颈。</p>
<p>下面我们将从<strong>由来、演变、集成方式、数学形式</strong>到 <strong>PyTorch 实现</strong> 一步步解析 Gated DeltaNet。</p>
<hr>
<h2 id="一由来与动机">一、由来与动机<a hidden class="anchor" aria-hidden="true" href="#一由来与动机">#</a></h2>
<h3 id="11-背景transformer-的局限">1.1 背景：Transformer 的局限<a hidden class="anchor" aria-hidden="true" href="#11-背景transformer-的局限">#</a></h3>
<ul>
<li>标准 Transformer 使用自注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 是序列长度。</li>
<li>对于超长上下文（如 100K+ tokens），注意力机制变得不可扩展。</li>
</ul>
<h3 id="12-状态空间模型ssm的兴起">1.2 状态空间模型（SSM）的兴起<a hidden class="anchor" aria-hidden="true" href="#12-状态空间模型ssm的兴起">#</a></h3>
<ul>
<li>SSM（如 S4、S5、Mamba）提供了一种线性复杂度 $O(L)$ 的序列建模范式。</li>
<li>其核心是维护一个<strong>隐状态</strong> $h_t$，通过递推更新：
$$
h_t = \bar{A} h_{t-1} + \bar{B} x_t
$$
$$
y_t = C h_t
$$
其中 $\bar{A}, \bar{B}$ 是离散化后的系统矩阵。</li>
</ul>
<h3 id="13-delta-rule-与-hebbian-学习">1.3 Delta Rule 与 Hebbian 学习<a hidden class="anchor" aria-hidden="true" href="#13-delta-rule-与-hebbian-学习">#</a></h3>
<ul>
<li>Delta Rule 是一种增量学习规则，形式为：
$$
\Delta W \propto (y_{\text{target}} - y) x^\top
$$</li>
<li>在神经记忆模型中，Delta Rule 可被解释为<strong>快速联想记忆更新</strong>：新输入 $x_t$ 与当前输出误差共同驱动权重更新。</li>
</ul>
<h3 id="14-gated-deltanet-的提出">1.4 Gated DeltaNet 的提出<a hidden class="anchor" aria-hidden="true" href="#14-gated-deltanet-的提出">#</a></h3>
<ul>
<li>Gated DeltaNet 将 Delta Rule 的思想嵌入到 SSM 框架中，形成一种<strong>可学习的、门控控制的记忆更新机制</strong>。</li>
<li>它不是直接更新权重，而是维护一个<strong>键值记忆矩阵</strong> $M_t$，并通过门控机制进行增量更新。</li>
</ul>
<hr>
<h2 id="二gated-deltanet-的数学形式">二、Gated DeltaNet 的数学形式<a hidden class="anchor" aria-hidden="true" href="#二gated-deltanet-的数学形式">#</a></h2>
<p>Gated DeltaNet 的核心是维护一个<strong>外部记忆矩阵</strong> $M_t \in \mathbb{R}^{d_k \times d_v}$，并在每个时间步通过输入 $x_t$ 更新它。</p>
<p>设输入序列为 ${x_t}_{t=1}^L$，每个 token 映射为：</p>
<ul>
<li>查询向量：$q_t = W_q x_t$</li>
<li>键向量：$k_t = W_k x_t$</li>
<li>值向量：$v_t = W_v x_t$</li>
</ul>
<h3 id="21-记忆更新规则delta-rule-风格">2.1 记忆更新规则（Delta Rule 风格）<a hidden class="anchor" aria-hidden="true" href="#21-记忆更新规则delta-rule-风格">#</a></h3>
<p>$$
M_t = \lambda_t M_{t-1} + \eta_t k_t v_t^\top
$$
其中：</p>
<ul>
<li>$\lambda_t \in [0,1]$ 是<strong>遗忘门</strong>（forget gate），控制旧记忆的保留程度；</li>
<li>$\eta_t \in [0,1]$ 是<strong>写入门</strong>（write gate），控制新信息的写入强度；</li>
<li>$k_t v_t^\top$ 是外积形式的“联想记忆”项，类似 Hebbian 学习。</li>
</ul>
<h3 id="22-输出计算">2.2 输出计算<a hidden class="anchor" aria-hidden="true" href="#22-输出计算">#</a></h3>
<p>$$
o_t = q_t^\top M_t
$$
即用当前查询 $q_t$ 从记忆矩阵 $M_t$ 中读取信息。</p>
<h3 id="23-门控机制gating">2.3 门控机制（Gating）<a hidden class="anchor" aria-hidden="true" href="#23-门控机制gating">#</a></h3>
<p>$$
\lambda_t = \sigma(g_\lambda^\top x_t), \quad \eta_t = \sigma(g_\eta^\top x_t)
$$
其中 $\sigma$ 是 sigmoid，$g_\lambda, g_\eta$ 是可学习向量（或通过小型 MLP 实现）。</p>
<blockquote>
<p>注意：实际实现中，为避免显式存储 $M_t$（其大小为 $d_k \times d_v$），通常采用<strong>低秩近似</strong>或<strong>递推展开</strong>技巧。但在 Gated DeltaNet 中，由于 $d_k, d_v$ 通常较小（如 64），直接维护 $M_t$ 是可行的。</p>
</blockquote>
<hr>
<h2 id="三在-transformer-中的集成方式">三、在 Transformer 中的集成方式<a hidden class="anchor" aria-hidden="true" href="#三在-transformer-中的集成方式">#</a></h2>
<p>Gated DeltaNet 通常作为 <strong>替代或补充标准自注意力模块</strong> 的组件，集成在 Transformer 层中：</p>
<ul>
<li><strong>方案 A（替代）</strong>：完全用 Gated DeltaNet 替代 Multi-Head Attention（MHA），形成纯 SSM 架构。</li>
<li><strong>方案 B（混合）</strong>：在 MHA 之后添加 Gated DeltaNet 作为“记忆增强”模块（类似 Linear Transformer 或 RWKV 的思路）。</li>
<li><strong>Qwen3-Next 采用的是混合架构</strong>：在某些层使用 Gated DeltaNet，尤其在处理长上下文时。</li>
</ul>
<p>其位置通常如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">[Input] → LayerNorm → GatedDeltaNet → Residual → LayerNorm → FFN → Residual → [Output]
</span></span></code></pre></div><hr>
<h2 id="四pytorch-实现">四、PyTorch 实现<a hidden class="anchor" aria-hidden="true" href="#四pytorch-实现">#</a></h2>
<p>下面是一个简化但功能完整的 Gated DeltaNet 模块实现（支持 batch 和序列维度）(注意：仅供参考，某些细节可能不同）：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GatedDeltaNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">d_v</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Projection matrices</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Gating parameters (simplified: use linear + sigmoid)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># outputs [logit_lambda, logit_eta]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        x: (batch_size, seq_len, d_model)
</span></span></span><span class="line"><span class="cl"><span class="s2">        returns: (batch_size, seq_len, d_v)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, L, d_k)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, L, d_k)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, L, d_v)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute gates: lambda (forget), eta (write)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gate_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, L, 2)</span>
</span></span><span class="line"><span class="cl">        <span class="n">lambda_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># (B, L)</span>
</span></span><span class="line"><span class="cl">        <span class="n">eta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>     <span class="c1"># (B, L)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize memory M: (B, d_k, d_v)</span>
</span></span><span class="line"><span class="cl">        <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Update memory: M_t = lambda_t * M_{t-1} + eta_t * (k_t ⊗ v_t)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kt</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>      <span class="c1"># (B, d_k, 1)</span>
</span></span><span class="line"><span class="cl">            <span class="n">vt</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># (B, 1, d_v)</span>
</span></span><span class="line"><span class="cl">            <span class="n">outer</span> <span class="o">=</span> <span class="n">kt</span> <span class="o">@</span> <span class="n">vt</span>                <span class="c1"># (B, d_k, d_v)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">lambda_t_i</span> <span class="o">=</span> <span class="n">lambda_t</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, 1)</span>
</span></span><span class="line"><span class="cl">            <span class="n">eta_t_i</span> <span class="o">=</span> <span class="n">eta_t</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>        <span class="c1"># (B, 1, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">M</span> <span class="o">=</span> <span class="n">lambda_t_i</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">eta_t_i</span> <span class="o">*</span> <span class="n">outer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Read: o_t = q_t^T M</span>
</span></span><span class="line"><span class="cl">            <span class="n">qt</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># (B, 1, d_k)</span>
</span></span><span class="line"><span class="cl">            <span class="n">ot</span> <span class="o">=</span> <span class="n">qt</span> <span class="o">@</span> <span class="n">M</span>                    <span class="c1"># (B, 1, d_v)</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, L, d_v)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div><h3 id="说明">说明：<a hidden class="anchor" aria-hidden="true" href="#说明">#</a></h3>
<ul>
<li>该实现是<strong>顺序递推</strong>的，因此时间复杂度为 $O(L \cdot d_k \cdot d_v)$，空间复杂度为 $O(d_k \cdot d_v)$（不随 L 增长）。</li>
<li>若 <code>d_k = d_v = 64</code>，则每步更新仅需约 4K 参数操作，远低于注意力的 $O(L^2)$。</li>
<li>实际部署中可进一步优化（如使用 <code>torch.compile</code> 或 CUDA kernel）。</li>
</ul>
<hr>
<h2 id="五与-mamba--ssm-的区别">五、与 Mamba / SSM 的区别<a hidden class="anchor" aria-hidden="true" href="#五与-mamba--ssm-的区别">#</a></h2>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>Mamba</th>
          <th>Gated DeltaNet</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>核心机制</td>
          <td>连续 SSM + 选择性扫描</td>
          <td>外积记忆 + 门控增量更新</td>
      </tr>
      <tr>
          <td>状态形式</td>
          <td>向量 $h_t \in \mathbb{R}^N$</td>
          <td>矩阵 $M_t \in \mathbb{R}^{d_k \times d_v}$</td>
      </tr>
      <tr>
          <td>更新规则</td>
          <td>$h_t = A h_{t-1} + B x_t$</td>
          <td>$M_t = \lambda M_{t-1} + \eta k_t v_t^\top$</td>
      </tr>
      <tr>
          <td>可解释性</td>
          <td>黑盒动态系统</td>
          <td>显式键值记忆，类似快速 Hebbian 学习</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="六总结">六、总结<a hidden class="anchor" aria-hidden="true" href="#六总结">#</a></h2>
<p>Gated DeltaNet 是 Qwen3-Next 中一种创新的<strong>高效长程建模模块</strong>，其核心思想是：</p>
<ul>
<li>利用<strong>外积形式</strong>构建联想记忆；</li>
<li>通过<strong>双门控机制</strong>（遗忘门 + 写入门）实现可控的增量更新；</li>
<li>以<strong>线性复杂度</strong>替代注意力，同时保留对关键信息的长期记忆能力。</li>
</ul>
<p>它代表了大模型架构从“纯注意力”向“混合记忆-注意力”演进的重要一步。</p>
<blockquote>
<p>注：截至 2026 年初，Gated DeltaNet 的细节尚未完全公开，以上解析基于 Qwen 团队技术报告、ICLR/NeurIPS 相关工作（如 DeltaNet、Hebbian Transformer、Mamba）以及合理推断。实际 Qwen3-Next 实现可能包含更多工程优化（如分组、低秩、并行化等）。</p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
