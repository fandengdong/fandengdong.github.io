<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ | My work notes</title>
<meta name="keywords" content="">
<meta name="description" content="æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚

ç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯

Mindspeed commit ID: 89f4632d
Megatron åˆ†æ”¯: core_v0.12.1
CANN: 8.2.RC1
PyTorch: 2.5.1



1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ
Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š
import os
import torch
import mindspeed.megatron_adaptor  # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹
from megatron.core import parallel_state

def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1):
    # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰
    parallel_state.destroy_model_parallel()

    # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½®
    rank = int(os.environ.get(&#39;LOCAL_RANK&#39;, 0))
    world_size = int(os.environ.get(&#34;WORLD_SIZE&#34;, 1))
    torch.cuda.set_device(rank)
    torch.distributed.init_process_group(world_size=world_size, rank=rank)

    # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€
    parallel_state.initialize_model_parallel(
        tensor_model_parallel_size=tensor_model_parallel_size,
        pipeline_model_parallel_size=pipeline_model_parallel_size
    )

å…³é”®ç‚¹è¯´æ˜ï¼š
å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚
é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚
éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚

Mindspeed æ¨¡å‹åˆå§‹åŒ–
Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ">
  <meta property="og:description" content="æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚
ç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯
Mindspeed commit ID: 89f4632d Megatron åˆ†æ”¯: core_v0.12.1 CANN: 8.2.RC1 PyTorch: 2.5.1 1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š
import os import torch import mindspeed.megatron_adaptor # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹ from megatron.core import parallel_state def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1): # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰ parallel_state.destroy_model_parallel() # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½® rank = int(os.environ.get(&#39;LOCAL_RANK&#39;, 0)) world_size = int(os.environ.get(&#34;WORLD_SIZE&#34;, 1)) torch.cuda.set_device(rank) torch.distributed.init_process_group(world_size=world_size, rank=rank) # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€ parallel_state.initialize_model_parallel( tensor_model_parallel_size=tensor_model_parallel_size, pipeline_model_parallel_size=pipeline_model_parallel_size ) å…³é”®ç‚¹è¯´æ˜ï¼š å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚ é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚ éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚
Mindspeed æ¨¡å‹åˆå§‹åŒ– Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-11-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-11T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ">
<meta name="twitter:description" content="æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚

ç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯

Mindspeed commit ID: 89f4632d
Megatron åˆ†æ”¯: core_v0.12.1
CANN: 8.2.RC1
PyTorch: 2.5.1



1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ
Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š
import os
import torch
import mindspeed.megatron_adaptor  # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹
from megatron.core import parallel_state

def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1):
    # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰
    parallel_state.destroy_model_parallel()

    # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½®
    rank = int(os.environ.get(&#39;LOCAL_RANK&#39;, 0))
    world_size = int(os.environ.get(&#34;WORLD_SIZE&#34;, 1))
    torch.cuda.set_device(rank)
    torch.distributed.init_process_group(world_size=world_size, rank=rank)

    # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€
    parallel_state.initialize_model_parallel(
        tensor_model_parallel_size=tensor_model_parallel_size,
        pipeline_model_parallel_size=pipeline_model_parallel_size
    )

å…³é”®ç‚¹è¯´æ˜ï¼š
å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚
é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚
éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚

Mindspeed æ¨¡å‹åˆå§‹åŒ–
Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "å¤§è¯­è¨€æ¨¡å‹ (LLM)",
      "item": "http://localhost:1313/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mindspeed",
      "item": "http://localhost:1313/llm/mindspeed/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ",
      "item": "http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ",
  "name": "å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ",
  "description": "æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚\nç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯\nMindspeed commit ID: 89f4632d Megatron åˆ†æ”¯: core_v0.12.1 CANN: 8.2.RC1 PyTorch: 2.5.1 1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š\nimport os import torch import mindspeed.megatron_adaptor # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹ from megatron.core import parallel_state def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1): # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰ parallel_state.destroy_model_parallel() # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½® rank = int(os.environ.get(\u0026#39;LOCAL_RANK\u0026#39;, 0)) world_size = int(os.environ.get(\u0026#34;WORLD_SIZE\u0026#34;, 1)) torch.cuda.set_device(rank) torch.distributed.init_process_group(world_size=world_size, rank=rank) # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€ parallel_state.initialize_model_parallel( tensor_model_parallel_size=tensor_model_parallel_size, pipeline_model_parallel_size=pipeline_model_parallel_size ) å…³é”®ç‚¹è¯´æ˜ï¼š å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚ é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚ éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚\nMindspeed æ¨¡å‹åˆå§‹åŒ– Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š\n",
  "keywords": [
    
  ],
  "articleBody": "æœ¬æ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mindspeed æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚\nç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯\nMindspeed commit ID: 89f4632d Megatron åˆ†æ”¯: core_v0.12.1 CANN: 8.2.RC1 PyTorch: 2.5.1 1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š\nimport os import torch import mindspeed.megatron_adaptor # å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹ from megatron.core import parallel_state def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1): # æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰ parallel_state.destroy_model_parallel() # æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½® rank = int(os.environ.get('LOCAL_RANK', 0)) world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) torch.cuda.set_device(rank) torch.distributed.init_process_group(world_size=world_size, rank=rank) # åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€ parallel_state.initialize_model_parallel( tensor_model_parallel_size=tensor_model_parallel_size, pipeline_model_parallel_size=pipeline_model_parallel_size ) å…³é”®ç‚¹è¯´æ˜ï¼š å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚ é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚ éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚\nMindspeed æ¨¡å‹åˆå§‹åŒ– Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š\nfrom megatron.core.transformer.transformer_config import TransformerConfig from megatron.core.models.gpt.gpt_model import GPTModel from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec _SEQUENCE_LENGTH = 64 def model_provider(): transformer_config = TransformerConfig( num_layers=2, hidden_size=12, num_attention_heads=4, use_cpu_initialization=True, pipeline_dtype=torch.float32, params_dtype=torch.float16, # æ§åˆ¶æ¨¡å‹å‚æ•°å­˜å‚¨ç±»å‹ bf16=True, # æ§åˆ¶å‰å‘/åå‘è®¡ç®—çš„æ•°æ®ç±»å‹ï¼ˆéœ€é…åˆ get_model ä½¿ç”¨ï¼‰ ) print(\"Creating GPT model...\") gpt_model = GPTModel( config=transformer_config, transformer_layer_spec=get_gpt_layer_local_spec(), vocab_size=100, max_sequence_length=_SEQUENCE_LENGTH, ) print(gpt_model) print(\"GPT model created.\") return gpt_model æ¨¡å‹ç»“æ„è§£æ\næ¨¡å‹åˆ›å»ºåˆ†ä¸ºä¸¤æ­¥ï¼š\nTransformerçš„é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬transformer layerå±‚æ•°ï¼Œhidden sizeï¼Œattention headsæ•°ï¼Œæ¨¡å‹å‚æ•°æ•°æ®ç±»å‹ã€‚ æ¨¡å‹ç»“æ„ï¼ŒåŒ…æ‹¬transformer layerå±‚ï¼Œä»¥åŠvocab sizeå’Œmax sequence lengthã€‚ ä¸Šé¢è¿™ä¸¤æ­¥åŸºæœ¬å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªTransformerç½‘ç»œçš„ç»“æ„äº†ã€‚å¯ä»¥æŸ¥çœ‹æ¨¡å‹æ‰“å°å‡ºæ¥çš„ç»“æ„ï¼š\nGPTModel( (embedding): LanguageModelEmbedding( (word_embeddings): VocabParallelEmbedding() (position_embeddings): Embedding(64, 12) (embedding_dropout): Dropout(p=0.1, inplace=False) ) (decoder): TransformerBlock( (layers): ModuleList( (0-1): 2 x TransformerLayer( (input_layernorm): FusedLayerNorm() (self_attention): SelfAttention( (core_attention): DotProductAttention( (scale_mask_softmax): FusedScaleMaskSoftmax() (attention_dropout): Dropout(p=0.1, inplace=False) ) (linear_proj): RowParallelLinear(in_features=12, out_features=12, bias=False, TP=1) (linear_qkv): ColumnParallelLinear(in_features=12, out_features=36, bias=False, TP=1) (q_layernorm): IdentityOp() (k_layernorm): IdentityOp() ) (pre_cross_attn_layernorm): IdentityOp() (cross_attention): IdentityOp() (cross_attn_bda): IdentityFuncOp() (pre_mlp_layernorm): FusedLayerNorm() (mlp): MLP( (linear_fc1): ColumnParallelLinear(in_features=12, out_features=48, bias=False, TP=1) (linear_fc2): RowParallelLinear(in_features=48, out_features=12, bias=False, TP=1) ) ) ) (final_layernorm): FusedLayerNorm() ) (output_layer): ColumnParallelLinear(in_features=12, out_features=100, bias=False, TP=1) ) åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°å‡ºæ¨¡å‹å‚æ•°çš„ä¿¡æ¯(TP=PP=1)ï¼š\nembedding.word_embeddings.weight, [100, 12], torch.float16, cpu embedding.position_embeddings.weight, [64, 12], torch.float32, cpu decoder.layers.0.input_layernorm.weight, [12], torch.float32, cpu decoder.layers.0.input_layernorm.bias, [12], torch.float32, cpu decoder.layers.0.self_attention.linear_proj.weight, [12, 12], torch.float16, cpu decoder.layers.0.self_attention.linear_proj.bias, [12], torch.float16, cpu decoder.layers.0.self_attention.linear_qkv.weight, [36, 12], torch.float16, cpu decoder.layers.0.self_attention.linear_qkv.bias, [36], torch.float16, cpu decoder.layers.0.pre_mlp_layernorm.weight, [12], torch.float32, cpu decoder.layers.0.pre_mlp_layernorm.bias, [12], torch.float32, cpu decoder.layers.0.mlp.linear_fc1.weight, [48, 12], torch.float16, cpu decoder.layers.0.mlp.linear_fc1.bias, [48], torch.float16, cpu decoder.layers.0.mlp.linear_fc2.weight, [12, 48], torch.float16, cpu decoder.layers.0.mlp.linear_fc2.bias, [12], torch.float16, cpu decoder.layers.1.input_layernorm.weight, [12], torch.float32, cpu decoder.layers.1.input_layernorm.bias, [12], torch.float32, cpu decoder.layers.1.self_attention.linear_proj.weight, [12, 12], torch.float16, cpu decoder.layers.1.self_attention.linear_proj.bias, [12], torch.float16, cpu decoder.layers.1.self_attention.linear_qkv.weight, [36, 12], torch.float16, cpu decoder.layers.1.self_attention.linear_qkv.bias, [36], torch.float16, cpu decoder.layers.1.pre_mlp_layernorm.weight, [12], torch.float32, cpu decoder.layers.1.pre_mlp_layernorm.bias, [12], torch.float32, cpu decoder.layers.1.mlp.linear_fc1.weight, [48, 12], torch.float16, cpu decoder.layers.1.mlp.linear_fc1.bias, [48], torch.float16, cpu decoder.layers.1.mlp.linear_fc2.weight, [12, 48], torch.float16, cpu decoder.layers.1.mlp.linear_fc2.bias, [12], torch.float16, cpu decoder.final_layernorm.weight, [12], torch.float32, cpu decoder.final_layernorm.bias, [12], torch.float32, cpu output_layer.weight, [100, 12], torch.float16, cpu Summary: Unique dtypes in model: {torch.float16, torch.float32} Unique devices in model: {device(type='cpu')} Total parameters: 6960 æ³¨æ„è§‚å¯Ÿæˆ‘ä»¬è®¾ç½®çš„å‚æ•°ä¸ä¸Šé¢å‚æ•°ç»´åº¦çš„å¯¹åº”å…³ç³»ï¼š\nvocab_size: 100 hidden_size: 12 sequence_length: 64 num_attention_heads: 4 è¿˜å¯ä»¥å‘ç°ï¼Œè™½ç„¶å‚æ•°ç±»å‹è®¾ç½®çš„æ˜¯float16ï¼Œä½†æ˜¯æ¨¡å‹å‚æ•°å¹¶ä¸éƒ½æ˜¯float16ï¼Œæœ‰éƒ¨åˆ†ç½‘ç»œå‚æ•°ä»ç„¶ä¸ºfloat32ï¼Œç‰¹åˆ«æ˜¯layernormçš„ç½‘ç»œå±‚ã€‚\næˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°å‡ºå…¶å®ƒå¹¶è¡Œé…ç½®ä¸‹æ¨¡å‹å‚æ•°çš„ä¿¡æ¯(TP=2, PP=1)ï¼š\nembedding.word_embeddings.weight, [50, 12], torch.float16, cpu embedding.position_embeddings.weight, [64, 12], torch.float32, cpu decoder.layers.0.input_layernorm.weight, [12], torch.float32, cpu decoder.layers.0.input_layernorm.bias, [12], torch.float32, cpu decoder.layers.0.self_attention.linear_proj.weight, [12, 6], torch.float16, cpu decoder.layers.0.self_attention.linear_proj.bias, [12], torch.float16, cpu decoder.layers.0.self_attention.linear_qkv.weight, [18, 12], torch.float16, cpu decoder.layers.0.self_attention.linear_qkv.bias, [18], torch.float16, cpu decoder.layers.0.pre_mlp_layernorm.weight, [12], torch.float32, cpu decoder.layers.0.pre_mlp_layernorm.bias, [12], torch.float32, cpu decoder.layers.0.mlp.linear_fc1.weight, [24, 12], torch.float16, cpu decoder.layers.0.mlp.linear_fc1.bias, [24], torch.float16, cpu decoder.layers.0.mlp.linear_fc2.weight, [12, 24], torch.float16, cpu decoder.layers.0.mlp.linear_fc2.bias, [12], torch.float16, cpu decoder.layers.1.input_layernorm.weight, [12], torch.float32, cpu decoder.layers.1.input_layernorm.bias, [12], torch.float32, cpu decoder.layers.1.self_attention.linear_proj.weight, [12, 6], torch.float16, cpu decoder.layers.1.self_attention.linear_proj.bias, [12], torch.float16, cpu decoder.layers.1.self_attention.linear_qkv.weight, [18, 12], torch.float16, cpu decoder.layers.1.self_attention.linear_qkv.bias, [18], torch.float16, cpu decoder.layers.1.pre_mlp_layernorm.weight, [12], torch.float32, cpu decoder.layers.1.pre_mlp_layernorm.bias, [12], torch.float32, cpu decoder.layers.1.mlp.linear_fc1.weight, [24, 12], torch.float16, cpu decoder.layers.1.mlp.linear_fc1.bias, [24], torch.float16, cpu decoder.layers.1.mlp.linear_fc2.weight, [12, 24], torch.float16, cpu decoder.layers.1.mlp.linear_fc2.bias, [12], torch.float16, cpu decoder.final_layernorm.weight, [12], torch.float32, cpu decoder.final_layernorm.bias, [12], torch.float32, cpu output_layer.weight, [50, 12], torch.float16, cpu Summary: Unique dtypes in model: {torch.float32, torch.float16} Unique devices in model: {device(type='cpu')} Total parameters: 3948 å¯ä»¥çœ‹åˆ°ï¼Œæœ‰çš„å‚æ•°çš„ç»´åº¦å‡åŠäº†ï¼è¿™æ˜¯å› ä¸ºTPå¹¶è¡Œï¼Œå¯¹æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°æŒ‰ç…§TPæ•°è¿›è¡Œäº†åˆ‡åˆ†ã€‚ä»”ç»†çœ‹ï¼Œåˆ‡åˆ†çš„ç½‘ç»œå±‚ä¸»è¦æ˜¯ï¼š\nembedding.word_embeddings.weight self_attention.linear_qkv.weightï¼Œself_attention.linear_qkv.bias mlp.linear_fc1.weight, mlp.linear_fc1.bias output_layer.weight å¦‚æœæˆ‘ä»¬é‡‡ç”¨PPåˆ‡åˆ†ï¼Œåˆ™èƒ½çœ‹åˆ°rank0è¿›ç¨‹çš„modelåªåŒ…å«äº†å‰åŠéƒ¨åˆ†çš„ç½‘ç»œå±‚ï¼Œrank1è¿›ç¨‹çš„modelåŒ…å«ååŠéƒ¨åˆ†ã€‚\nå¦å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾ç½®bf16=Trueï¼Œå»æ£€æŸ¥æ¨¡å‹çš„å‚æ•°ï¼Œæˆ–è€…debugæ¨¡å‹ä¸­é—´å±‚çš„è¾“å…¥è¾“å‡ºï¼Œå…¶ç±»å‹ä¾ç„¶æ˜¯float32ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰è°ƒç”¨megatron.training.get_modelå‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥è°ƒç”¨äº†megatron.model.GPTModelï¼Œè¿™é‡ŒGPTModelæ²¡æœ‰å¯¹æ¨¡å‹çš„æ•°å€¼ç±»å‹åšä»»ä½•çš„å¤„ç†ï¼Œä»…ä»…æ˜¯é€šè¿‡para_dtypeåˆå§‹åŒ–äº†æ¨¡å‹å‚æ•°ã€‚è€Œget_modelå‡½æ•°é‡Œé¢ï¼Œåˆ™ä¼šæ ¹æ®bf16å‚æ•°ï¼Œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œç±»å‹è½¬æ¢ï¼š\nfrom megatron.core.enums import ModelType def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap_with_ddp=True): ... # Fp16 conversion. if args.fp16 or args.bf16: config = get_model_config(model[0]) model = [Float16Module(config, model_module) for model_module in model] ... è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹Float16Moduleçš„å®ç°ï¼Œè¯¥æ¨¡å—åœ¨è®¡ç®—å‰å‘çš„æ—¶å€™ï¼Œåœ¨forwardå‡½æ•°ä¸­ä¸´æ—¶å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œç®—å®Œåï¼Œåˆè½¬æ¢ä¸ºfp32ç²¾åº¦ã€‚\nclass Float16Module(MegatronModule): \"\"\"Float 16 Module. Attributes: config (TransformerConfig): Transformer config fp16 (bool) : Specifies if the model runs in fp16 mode bf16 (bool) : Specifies if the model runs in bf16 mode Args: config (TransformerConfig): The transformer config used to initalize the model \"\"\" def __init__(self, config: TransformerConfig, module: torch.nn.Module): super(Float16Module, self).__init__(config) self.config = config self.fp16 = config.fp16 self.bf16 = config.bf16 if self.fp16: self.add_module('module', module.half()) def float16_convertor(val): return val.half() elif self.bf16: self.add_module('module', module.bfloat16()) def float16_convertor(val): return val.bfloat16() else: raise Exception('Either config.fp16 or config.bf16 should be True.') self.float16_convertor = float16_convertor def forward(self, *inputs, **kwargs): if parallel_state.is_pipeline_first_stage(): inputs = fp32_to_float16(inputs, self.float16_convertor) outputs = self.module(*inputs, **kwargs) if parallel_state.is_pipeline_last_stage(): outputs = float16_to_fp32(outputs) return outputs ... åœ¨å®è·µä¸­ï¼Œä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œæƒ³é¿å…Float16Moduleå¯¹è‡ªå®šä¹‰çš„ç½‘ç»œå±‚çš„å‚æ•°æ•°æ®ç±»å‹è¿›è¡Œè½¬æ¢ï¼ˆä¸»è¦æ˜¯æƒ³ä¿æŒfp32ç²¾åº¦ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶é‡å†™ç½‘ç»œå±‚çš„half(self)ï¼Œbfloat16(self)æ–¹æ³•å’Œ_apply(self, fn)æ–¹æ³•ï¼š\nclass CustomLayer(nn.Module): ... def _apply(self, fn): return self def half(self): return self def bfloat16(self): return self ... è¿™æ ·å¯ä»¥é¿å…ç½‘ç»œå¯¹è‡ªå®šä¹‰ç½‘ç»œå±‚çš„å‚æ•°è¿›è¡Œç±»å‹è½¬æ¢ã€‚å¦å¤–ï¼Œåœ¨å‰å‘çš„æ—¶å€™ï¼Œè¿˜å°½é‡æ˜¾å¼çš„å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºtorch.float32ã€‚\nMindSpeed è®­ç»ƒä¸»å¾ªç¯å…¥å£ åœ¨åŸºäºMegatronçš„æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ç»å¸¸çœ‹åˆ°è¿™æ ·çš„è®­ç»ƒå¾ªç¯ï¼š\nfrom megatron.core.pipeline_parallel.schedules import get_forward_backward_func optim = Adam(gpt_model.parameters()) forward_backward_func = get_forward_backward_func() for i in range(5): print(f\"Starting iteration {i+1}/5...\") optim.zero_grad() print(\"Gradients zeroed.\") losses_reduced = forward_backward_func( forward_step_func=forward_step_func, data_iterator=train_iterator, model=gpt_model, num_microbatches=1, seq_length=_SEQUENCE_LENGTH, micro_batch_size=8, decoder_seq_length=_SEQUENCE_LENGTH, forward_only=False) print(f\"Forward-backward pass completed with losses: {losses_reduced}\") optim.step() forward_backward_funcæ˜¯megatronå†…ç½®çš„åŒ…è£…å¥½çš„å‡½æ•°ï¼Œå…¶å†…ç½®äº†å‰å‘å’Œåå‘çš„è®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯ä¼ å…¥è‡ªå®šä¹‰çš„forward_step_funcå’Œdata_iteratorå’Œmodelã€‚è¾“å‡ºåˆ™æ˜¯reducedçš„lossï¼Œå…¶shapeä¸º[bs, seq_len]ï¼Œå³æ¯ä¸€ä¸ªtokençš„lossã€‚\nè‡ªå®šä¹‰çš„å‰å‘å‡½æ•°forward_backward_funcï¼Œä¸»è¦åŒ…æ‹¬äº†å¯¹è¾“å…¥æ•°æ®çš„ç®€å•é¢„å¤„ç†å’Œlosså‡½æ•°çš„å®šä¹‰ï¼š\ndef forward_step_func(data_iterator, model): \"\"\" è‡ªå®šä¹‰å‰å‘å‡½æ•° Notes: 1. model(tokens, position_ids, attention_mask, labels=labels)è¿”å›çš„æ˜¯lossï¼Œè€Œä¸æ˜¯logits 2. è¦è¿”å›logitsï¼Œåˆ™ä»…éœ€è¦æ‹¿æ‰labelså‚æ•°å³å¯ \"\"\" def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor): losses = output_tensor.float() loss_mask = loss_mask.view(-1).float() loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum() # If you have data parallel reduce loss across data parallel groups. # If pipeline parallel, loss computation is done only in last stage. return loss, {'lm loss': loss} data = next(data_iterator) tokens = data['tokens'].to(device) attention_mask = data['attention_mask'].to(device) position_ids = data['position_ids'].to(device) labels = data['labels'].to(device) loss_mask = data['loss_mask'].to(device) # output loss output_tensor = model(tokens, position_ids, attention_mask, labels=labels) return output_tensor, partial(loss_func, loss_mask) æ³¨æ„ï¼Œå‰å‘å‡½æ•°è¿”å›çš„æ˜¯æ¨¡å‹è¾“å‡ºçš„æ¯ä¸€ä¸ªtokençš„lossï¼Œå’Œlosså‡½æ•°çš„åå‡½æ•°ã€‚å…³äºlossçš„è®¡ç®—ï¼Œå¤§æ¨¡å‹ç»™å‡ºäº†ä¸€ä¸ªè§£ç­”ï¼š\næ•´ä¸ªlossè®¡ç®—åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š **ç¬¬ä¸€é˜¶æ®µï¼ˆåœ¨æ¨¡å‹å†…éƒ¨ï¼‰**ï¼š - æ¨¡å‹è‡ªå·±è®¡ç®—åŸºç¡€çš„tokençº§åˆ«loss - è¿”å›å½¢çŠ¶ä¸º [batch_size, sequence_length] çš„tensor **ç¬¬äºŒé˜¶æ®µï¼ˆåœ¨è‡ªå®šä¹‰loss_funcä¸­ï¼‰**ï¼š - å¯¹æ¨¡å‹è¿”å›çš„lossè¿›è¡Œè¿›ä¸€æ­¥å¤„ç† - åº”ç”¨loss_maskè¿›è¡Œè¿‡æ»¤ - è®¡ç®—æœ€ç»ˆçš„å¹³å‡loss è°ƒè¯•Mindspeedè®­ç»ƒ Mindspeedè®­ç»ƒä»£ç é€šå¸¸æ˜¯å¤šæœºå¤šå¡çš„ï¼Œå› æ­¤è°ƒè¯•èµ·æ¥å¯èƒ½ä¼šæ¯”è¾ƒéº»çƒ¦ã€‚è¿™é‡Œæä¾›ä¸€ä¸ªæˆç†ŸéªŒè¯çš„æ–¹æ³•æ¥debugã€‚\nåœ¨è®­ç»ƒä»£ç ä¸­ï¼Œæ·»åŠ setup_debugpy()å‡½æ•° import os def setup_debugpy(): \"\"\"åœ¨æ¯ä¸ª rank ä¸­è®¾ç½® debugpy\"\"\" rank = int(os.environ.get(\"RANK\", 0)) # ä½¿ç”¨ 5678 + rank ä½œä¸ºç«¯å£ï¼ˆç¡®ä¿ä¸å†²çªï¼‰ debugpy_port = 22333 + rank print(f\"[Rank {rank}] Waiting for debugger attach on port {debugpy_port}...\") debugpy.listen((\"0.0.0.0\", debugpy_port)) # ç»‘å®šæ‰€æœ‰æ¥å£ï¼ˆå…¼å®¹å®¹å™¨/è¿œç¨‹ï¼‰ debugpy.wait_for_client() # é˜»å¡ç›´åˆ° VS Code è¿æ¥ print(f\"[Rank {rank}] Debugger attached!\") é…ç½®debugæ–‡ä»¶,.vscode/launch.jsoné‡Œé¢å†™å…¥é…ç½®ï¼š { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Attach to Worker 0\", \"type\": \"debugpy\", \"request\": \"attach\", \"connect\": { \"host\": \"localhost\", \"port\": 22333 }, \"pathMappings\": [ { \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \".\" // å¦‚æœæœ¬åœ°/è¿œç¨‹è·¯å¾„ä¸€è‡´ } ] }, { \"name\": \"Attach to Worker 1\", \"type\": \"debugpy\", \"request\": \"attach\", \"connect\": { \"host\": \"localhost\", \"port\": 22334 } } ] ... } åœ¨åˆé€‚çš„åœ°æ–¹å‡ºå…¥setup_debugpy()ï¼Œæ”¾åœ¨å¯åŠ¨äº†å¤šè¿›ç¨‹ä¹‹åçš„ä»£ç ä½ç½®ï¼Œå‚è€ƒè¿™é‡Œ\næ­£å¸¸å¯åŠ¨Mindspeedçš„è®­ç»ƒ\ntorchrun --nproc-per-node 4 mindspeed_train.py vscodeè°ƒè¯•å™¨é“¾æ¥ä»£ç  å½“ç»ˆç«¯æ‰“å°å‡º[Rank 0] Waiting for debugger attach on port 22333...ï¼Œåˆ™å¯ä»¥ç‚¹å‡»vscodeçš„debuggerå›¾æ ‡ï¼Œé€‰æ‹©Attach to Worker 0ï¼Œç„¶åç‚¹å‡»å¯åŠ¨æŒ‰é’®ã€‚\nå…³äºmindspeedçš„è®­ç»ƒç²¾åº¦ mindspeedæä¾›äº†ä¸¤ä¸ªflagï¼Œç”¨äºè®¾ç½®è®­ç»ƒç²¾åº¦ï¼š\n--fp16: ä½¿ç”¨fp16è®­ç»ƒç²¾åº¦ã€‚fp16è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´æ¯”è¾ƒå°ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸Šæº¢æˆ–è€…ä¸‹æº¢ï¼Œé‡‡ç”¨fp16è¿›è¡Œè®­ç»ƒï¼Œmindspeedä¼šè‡ªåŠ¨è°ƒç”¨loss scaleræ¥å¤„ç†æ¢¯åº¦å‘ä¸‹æº¢å‡ºçš„é—®é¢˜ï¼ˆæ³¨æ„loss scaleræ˜¯è‡ªåŠ¨åœ¨optimizerä¸­å¤„ç†çš„ï¼‰ã€‚ --bf16: ä½¿ç”¨bf16è®­ç»ƒç²¾åº¦ã€‚bf16é‡‡ç”¨äº†æ›´å¤šçš„æŒ‡æ•°ä½ï¼Œå› æ­¤è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´ä¼šå¤§å¾ˆå¤šï¼Œå› æ­¤ä¸éœ€è¦loss scalerï¼Œä½†æ˜¯bf16çš„æ•°å€¼ç²¾åº¦è¦æ¯”fp16å°ã€‚ å¦‚æœ--fp16å’Œ--bf16éƒ½æ˜¯falseï¼Œåˆ™é»˜è®¤ä¼šä½¿ç”¨fp32è®­ç»ƒç²¾åº¦ã€‚ æµ®ç‚¹æ•°æ ¼å¼æ¯”è¾ƒ FP16 æ ¼å¼ FP16 æ˜¯ä¸€ç§ 16 ä½æµ®ç‚¹æ•°æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š\n1 ä½ç”¨äºç¬¦å·ï¼ˆSï¼‰ 5 ä½ç”¨äºæŒ‡æ•°ï¼ˆEï¼‰ 10 ä½ç”¨äºå°¾æ•°ï¼ˆMï¼Œä¹Ÿç§°ä¸ºåˆ†æ•°æˆ– mantissaï¼‰ å­—æ®µ æ¯”ç‰¹æ•° æè¿° ç¬¦å· (S) 1 è¡¨ç¤ºæ•°å­—çš„æ­£è´Ÿ æŒ‡æ•° (E) 5 ä½¿ç”¨åç½®ï¼ˆbiasï¼‰ä¸º 15ï¼Œè¡¨ç¤ºèŒƒå›´çº¦ä¸º [-14, 15] å°¾æ•° (M) 10 éšå«å‰å¯¼ 1ï¼Œå®é™…æœ‰æ•ˆç²¾åº¦ä¸º 11 ä½ BF16 æ ¼å¼ BF16 ä¹Ÿæ˜¯ä¸€ç§ 16 ä½æµ®ç‚¹æ•°æ ¼å¼ï¼Œä½†æ¯”ç‰¹åˆ†é…ä¸åŒï¼š\n1 ä½ç”¨äºç¬¦å·ï¼ˆSï¼‰ 8 ä½ç”¨äºæŒ‡æ•°ï¼ˆEï¼‰ 7 ä½ç”¨äºå°¾æ•°ï¼ˆMï¼‰ å­—æ®µ æ¯”ç‰¹æ•° æè¿° ç¬¦å· (S) 1 è¡¨ç¤ºæ•°å­—çš„æ­£è´Ÿ æŒ‡æ•° (E) 8 ä½¿ç”¨åç½®ï¼ˆbiasï¼‰ä¸º 127ï¼Œè¡¨ç¤ºèŒƒå›´çº¦ä¸º [-126, 127] å°¾æ•° (M) 7 æœ‰æ•ˆç²¾åº¦è¾ƒä½ï¼Œæ— éšå«ä½ä¼˜åŒ–ï¼ˆä¸ FP32 å¯¹é½ï¼‰ æ•°å€¼è¡¨ç¤ºèŒƒå›´å’Œç²¾åº¦å¯¹æ¯” ç”±äº BF16 å°†æ›´å¤šæ¯”ç‰¹åˆ†é…ç»™æŒ‡æ•°éƒ¨åˆ†ï¼Œå…¶æ•°å€¼è¡¨ç¤ºèŒƒå›´è¿œå¤§äº FP16ï¼Œæ¥è¿‘ FP32ï¼›ä½†å°¾æ•°ä½æ›´å°‘ï¼Œå› æ­¤ç²¾åº¦ä½äº FP16ã€‚\næ ¼å¼ æ€»ä½æ•° æŒ‡æ•°ä½æ•° å°¾æ•°ä½æ•° æ•°å€¼èŒƒå›´ï¼ˆè¿‘ä¼¼ï¼‰ ç²¾åº¦ FP16 16 5 10 (6.1 \\times 10^{-5}) åˆ° (6.5 \\times 10^{4}) è¾ƒé«˜ BF16 16 8 7 æ¥è¿‘ FP32ï¼ˆçº¦ (10^{-38}) åˆ° (10^{38})ï¼‰ ä¸­ç­‰ï¼ˆä½äº FP16ï¼‰ ğŸ’¡ æ€»ç»“ï¼š\nFP16ï¼šç²¾åº¦é«˜ï¼Œä½†åŠ¨æ€èŒƒå›´å°ï¼Œè®­ç»ƒæ—¶éœ€é…åˆ loss scaler é˜²æ­¢ä¸‹æº¢ã€‚ BF16ï¼šåŠ¨æ€èŒƒå›´å¤§ï¼ˆç±»ä¼¼ FP32ï¼‰ï¼Œæ— éœ€ loss scalerï¼Œä½†æ•°å€¼ç²¾åº¦è¾ƒä½ï¼Œé€‚åˆå¯¹èŒƒå›´æ•æ„Ÿã€å¯¹ç²¾åº¦å®¹å¿åº¦è¾ƒé«˜çš„æ·±åº¦å­¦ä¹ è®­ç»ƒåœºæ™¯ã€‚ ",
  "wordCount" : "1096",
  "inLanguage": "en",
  "datePublished": "2025-11-11T00:00:00Z",
  "dateModified": "2025-11-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "fandengdong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/llm/mindspeed/get_started_with_mindspeed/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My work notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="å¤§è¯­è¨€æ¨¡å‹ (LLM)">
                    <span>å¤§è¯­è¨€æ¨¡å‹ (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="å·¥å…·ç®± (toolbox)">
                    <span>å·¥å…·ç®± (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="å¼ºåŒ–å­¦ä¹  (RL)">
                    <span>å¼ºåŒ–å­¦ä¹  (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´">
                    <span>æ¬¢è¿æ¥åˆ°æˆ‘çš„å·¥ä½œç©ºé—´</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      å¼€å§‹ä½¿ç”¨ Mindspeed è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒ
    </h1>
    <div class="post-meta"><span title='2025-11-11 00:00:00 +0000 UTC'>November 11, 2025</span>&nbsp;Â·&nbsp;<span>fandengdong</span>

</div>
  </header> 
  <div class="post-content"><p>æœ¬æ–‡é€šè¿‡ä¸€ä¸ª<a href="https://github.com/fandengdong/fdd.github.io/blob/main/content/LLM/mindspeed/codes/simple_mcore_train_loop.py">å®Œæ•´ç¤ºä¾‹ä»£ç </a>æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ <strong>Mindspeed</strong> æ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒã€‚</p>
<blockquote>
<p><strong>ç¯å¢ƒç‰ˆæœ¬ä¿¡æ¯</strong></p>
<ul>
<li>Mindspeed commit ID: <code>89f4632d</code></li>
<li>Megatron åˆ†æ”¯: <code>core_v0.12.1</code></li>
<li>CANN: <code>8.2.RC1</code></li>
<li>PyTorch: <code>2.5.1</code></li>
</ul>
</blockquote>
<hr>
<h2 id="1-åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ">1. åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ<a hidden class="anchor" aria-hidden="true" href="#1-åˆå§‹åŒ–åˆ†å¸ƒå¼å¹¶è¡Œç¯å¢ƒ">#</a></h2>
<p>Mindspeed åŸºäº Megatron æ„å»ºï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¿…é¡»æ­£ç¡®åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">mindspeed.megatron_adaptor</span>  <span class="c1"># å…³é”®ï¼šç¡®ä¿ Mindspeed ä¸ Megatron API å…¼å®¹</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core</span> <span class="kn">import</span> <span class="n">parallel_state</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">initialize_distributed</span><span class="p">(</span><span class="n">tensor_model_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pipeline_model_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># æ¸…ç†å·²æœ‰çŠ¶æ€ï¼ˆé˜²æ­¢é‡å¤åˆå§‹åŒ–ï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel_state</span><span class="o">.</span><span class="n">destroy_model_parallel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># æ ‡å‡† PyTorch åˆ†å¸ƒå¼è®¾ç½®</span>
</span></span><span class="line"><span class="cl">    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;LOCAL_RANK&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;WORLD_SIZE&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># åˆå§‹åŒ– Megatron å¹¶è¡ŒçŠ¶æ€</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel_state</span><span class="o">.</span><span class="n">initialize_model_parallel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_model_parallel_size</span><span class="o">=</span><span class="n">tensor_model_parallel_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline_model_parallel_size</span><span class="o">=</span><span class="n">pipeline_model_parallel_size</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><blockquote>
<p><strong>å…³é”®ç‚¹è¯´æ˜</strong>ï¼š
å¿…é¡»æ˜¾å¼å¯¼å…¥ mindspeed.megatron_adaptorï¼Œä»¥å¯ç”¨å…¼å®¹å±‚ã€‚
é™¤äº†æ ‡å‡†çš„ torch.distributed.init_process_groupï¼Œè¿˜éœ€è°ƒç”¨ parallel_state.initialize_model_parallel æ¥æ¿€æ´» TP/PP æ”¯æŒã€‚
éœ€æ ¹æ®å®é™…è®­ç»ƒé…ç½®ä¼ å…¥ tensor_model_parallel_size å’Œ pipeline_model_parallel_sizeã€‚</p>
</blockquote>
<h2 id="mindspeed-æ¨¡å‹åˆå§‹åŒ–">Mindspeed æ¨¡å‹åˆå§‹åŒ–<a hidden class="anchor" aria-hidden="true" href="#mindspeed-æ¨¡å‹åˆå§‹åŒ–">#</a></h2>
<p>Mindspeed ä½¿ç”¨ Megatron Core çš„æ¨¡å—åŒ–è®¾è®¡æ„å»ºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€å° GPT æ¨¡å‹çš„æ„å»ºç¤ºä¾‹ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core.transformer.transformer_config</span> <span class="kn">import</span> <span class="n">TransformerConfig</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core.models.gpt.gpt_model</span> <span class="kn">import</span> <span class="n">GPTModel</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core.models.gpt.gpt_layer_specs</span> <span class="kn">import</span> <span class="n">get_gpt_layer_local_spec</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">model_provider</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformer_config</span> <span class="o">=</span> <span class="n">TransformerConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cpu_initialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">params_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>  <span class="c1"># æ§åˆ¶æ¨¡å‹å‚æ•°å­˜å‚¨ç±»å‹</span>
</span></span><span class="line"><span class="cl">        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                   <span class="c1"># æ§åˆ¶å‰å‘/åå‘è®¡ç®—çš„æ•°æ®ç±»å‹ï¼ˆéœ€é…åˆ get_model ä½¿ç”¨ï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Creating GPT model...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">gpt_model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span><span class="o">=</span><span class="n">transformer_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer_layer_spec</span><span class="o">=</span><span class="n">get_gpt_layer_local_spec</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">_SEQUENCE_LENGTH</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">gpt_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;GPT model created.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">gpt_model</span>
</span></span></code></pre></div><p><strong>æ¨¡å‹ç»“æ„è§£æ</strong></p>
<p>æ¨¡å‹åˆ›å»ºåˆ†ä¸ºä¸¤æ­¥ï¼š</p>
<ol>
<li>Transformerçš„é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬transformer layerå±‚æ•°ï¼Œhidden sizeï¼Œattention headsæ•°ï¼Œæ¨¡å‹å‚æ•°æ•°æ®ç±»å‹ã€‚</li>
<li>æ¨¡å‹ç»“æ„ï¼ŒåŒ…æ‹¬transformer layerå±‚ï¼Œä»¥åŠvocab sizeå’Œmax sequence lengthã€‚</li>
</ol>
<p>ä¸Šé¢è¿™ä¸¤æ­¥åŸºæœ¬å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªTransformerç½‘ç»œçš„ç»“æ„äº†ã€‚å¯ä»¥æŸ¥çœ‹æ¨¡å‹æ‰“å°å‡ºæ¥çš„ç»“æ„ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">GPTModel<span class="o">(</span>
</span></span><span class="line"><span class="cl">  <span class="o">(</span>embedding<span class="o">)</span>: LanguageModelEmbedding<span class="o">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">(</span>word_embeddings<span class="o">)</span>: VocabParallelEmbedding<span class="o">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">(</span>position_embeddings<span class="o">)</span>: Embedding<span class="o">(</span>64, 12<span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">(</span>embedding_dropout<span class="o">)</span>: Dropout<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.1, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">(</span>decoder<span class="o">)</span>: TransformerBlock<span class="o">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">(</span>layers<span class="o">)</span>: ModuleList<span class="o">(</span>
</span></span><span class="line"><span class="cl">      <span class="o">(</span>0-1<span class="o">)</span>: <span class="m">2</span> x TransformerLayer<span class="o">(</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>input_layernorm<span class="o">)</span>: FusedLayerNorm<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>self_attention<span class="o">)</span>: SelfAttention<span class="o">(</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>core_attention<span class="o">)</span>: DotProductAttention<span class="o">(</span>
</span></span><span class="line"><span class="cl">            <span class="o">(</span>scale_mask_softmax<span class="o">)</span>: FusedScaleMaskSoftmax<span class="o">()</span>
</span></span><span class="line"><span class="cl">            <span class="o">(</span>attention_dropout<span class="o">)</span>: Dropout<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.1, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
</span></span><span class="line"><span class="cl">          <span class="o">)</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>linear_proj<span class="o">)</span>: RowParallelLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12, <span class="nv">out_features</span><span class="o">=</span>12, <span class="nv">bias</span><span class="o">=</span>False, <span class="nv">TP</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>linear_qkv<span class="o">)</span>: ColumnParallelLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12, <span class="nv">out_features</span><span class="o">=</span>36, <span class="nv">bias</span><span class="o">=</span>False, <span class="nv">TP</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>q_layernorm<span class="o">)</span>: IdentityOp<span class="o">()</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>k_layernorm<span class="o">)</span>: IdentityOp<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>pre_cross_attn_layernorm<span class="o">)</span>: IdentityOp<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>cross_attention<span class="o">)</span>: IdentityOp<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>cross_attn_bda<span class="o">)</span>: IdentityFuncOp<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>pre_mlp_layernorm<span class="o">)</span>: FusedLayerNorm<span class="o">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">(</span>mlp<span class="o">)</span>: MLP<span class="o">(</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>linear_fc1<span class="o">)</span>: ColumnParallelLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12, <span class="nv">out_features</span><span class="o">=</span>48, <span class="nv">bias</span><span class="o">=</span>False, <span class="nv">TP</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">          <span class="o">(</span>linear_fc2<span class="o">)</span>: RowParallelLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>48, <span class="nv">out_features</span><span class="o">=</span>12, <span class="nv">bias</span><span class="o">=</span>False, <span class="nv">TP</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">)</span>
</span></span><span class="line"><span class="cl">      <span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">(</span>final_layernorm<span class="o">)</span>: FusedLayerNorm<span class="o">()</span>
</span></span><span class="line"><span class="cl">  <span class="o">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">(</span>output_layer<span class="o">)</span>: ColumnParallelLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>False, <span class="nv">TP</span><span class="o">=</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="o">)</span>
</span></span></code></pre></div><p>åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°å‡ºæ¨¡å‹å‚æ•°çš„ä¿¡æ¯(TP=PP=1)ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">embedding.word_embeddings.weight, <span class="o">[</span>100, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">embedding.position_embeddings.weight, <span class="o">[</span>64, 12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.input_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.input_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_proj.weight, <span class="o">[</span>12, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_proj.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_qkv.weight, <span class="o">[</span>36, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_qkv.bias, <span class="o">[</span>36<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.pre_mlp_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.pre_mlp_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc1.weight, <span class="o">[</span>48, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc1.bias, <span class="o">[</span>48<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc2.weight, <span class="o">[</span>12, 48<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc2.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.input_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.input_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_proj.weight, <span class="o">[</span>12, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_proj.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_qkv.weight, <span class="o">[</span>36, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_qkv.bias, <span class="o">[</span>36<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.pre_mlp_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.pre_mlp_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc1.weight, <span class="o">[</span>48, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc1.bias, <span class="o">[</span>48<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc2.weight, <span class="o">[</span>12, 48<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc2.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.final_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.final_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">output_layer.weight, <span class="o">[</span>100, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Summary:
</span></span><span class="line"><span class="cl">Unique dtypes in model: <span class="o">{</span>torch.float16, torch.float32<span class="o">}</span>
</span></span><span class="line"><span class="cl">Unique devices in model: <span class="o">{</span>device<span class="o">(</span><span class="nv">type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="o">)}</span>
</span></span><span class="line"><span class="cl">Total parameters: <span class="m">6960</span>
</span></span></code></pre></div><p>æ³¨æ„è§‚å¯Ÿæˆ‘ä»¬è®¾ç½®çš„å‚æ•°ä¸ä¸Šé¢å‚æ•°ç»´åº¦çš„å¯¹åº”å…³ç³»ï¼š</p>
<ol>
<li>vocab_size: 100</li>
<li>hidden_size: 12</li>
<li>sequence_length: 64</li>
<li>num_attention_heads: 4</li>
</ol>
<p>è¿˜å¯ä»¥å‘ç°ï¼Œè™½ç„¶å‚æ•°ç±»å‹è®¾ç½®çš„æ˜¯float16ï¼Œä½†æ˜¯æ¨¡å‹å‚æ•°å¹¶ä¸éƒ½æ˜¯float16ï¼Œæœ‰éƒ¨åˆ†ç½‘ç»œå‚æ•°ä»ç„¶ä¸ºfloat32ï¼Œç‰¹åˆ«æ˜¯layernormçš„ç½‘ç»œå±‚ã€‚</p>
<p>æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°å‡ºå…¶å®ƒå¹¶è¡Œé…ç½®ä¸‹æ¨¡å‹å‚æ•°çš„ä¿¡æ¯(TP=2, PP=1)ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">embedding.word_embeddings.weight, <span class="o">[</span>50, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">embedding.position_embeddings.weight, <span class="o">[</span>64, 12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.input_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.input_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_proj.weight, <span class="o">[</span>12, 6<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_proj.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_qkv.weight, <span class="o">[</span>18, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.self_attention.linear_qkv.bias, <span class="o">[</span>18<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.pre_mlp_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.pre_mlp_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc1.weight, <span class="o">[</span>24, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc1.bias, <span class="o">[</span>24<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc2.weight, <span class="o">[</span>12, 24<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.0.mlp.linear_fc2.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.input_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.input_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_proj.weight, <span class="o">[</span>12, 6<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_proj.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_qkv.weight, <span class="o">[</span>18, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.self_attention.linear_qkv.bias, <span class="o">[</span>18<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.pre_mlp_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.pre_mlp_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc1.weight, <span class="o">[</span>24, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc1.bias, <span class="o">[</span>24<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc2.weight, <span class="o">[</span>12, 24<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.layers.1.mlp.linear_fc2.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">decoder.final_layernorm.weight, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">decoder.final_layernorm.bias, <span class="o">[</span>12<span class="o">]</span>, torch.float32, cpu
</span></span><span class="line"><span class="cl">output_layer.weight, <span class="o">[</span>50, 12<span class="o">]</span>, torch.float16, cpu
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Summary:
</span></span><span class="line"><span class="cl">Unique dtypes in model: <span class="o">{</span>torch.float32, torch.float16<span class="o">}</span>
</span></span><span class="line"><span class="cl">Unique devices in model: <span class="o">{</span>device<span class="o">(</span><span class="nv">type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="o">)}</span>
</span></span><span class="line"><span class="cl">Total parameters: <span class="m">3948</span>
</span></span></code></pre></div><p>å¯ä»¥çœ‹åˆ°ï¼Œæœ‰çš„å‚æ•°çš„ç»´åº¦å‡åŠäº†ï¼è¿™æ˜¯å› ä¸ºTPå¹¶è¡Œï¼Œå¯¹æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°æŒ‰ç…§TPæ•°è¿›è¡Œäº†åˆ‡åˆ†ã€‚ä»”ç»†çœ‹ï¼Œåˆ‡åˆ†çš„ç½‘ç»œå±‚ä¸»è¦æ˜¯ï¼š</p>
<ol>
<li>embedding.word_embeddings.weight</li>
<li>self_attention.linear_qkv.weightï¼Œself_attention.linear_qkv.bias</li>
<li>mlp.linear_fc1.weight, mlp.linear_fc1.bias</li>
<li>output_layer.weight</li>
</ol>
<p>å¦‚æœæˆ‘ä»¬é‡‡ç”¨PPåˆ‡åˆ†ï¼Œåˆ™èƒ½çœ‹åˆ°rank0è¿›ç¨‹çš„modelåªåŒ…å«äº†å‰åŠéƒ¨åˆ†çš„ç½‘ç»œå±‚ï¼Œrank1è¿›ç¨‹çš„modelåŒ…å«ååŠéƒ¨åˆ†ã€‚</p>
<p>å¦å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾ç½®<code>bf16=True</code>ï¼Œå»æ£€æŸ¥æ¨¡å‹çš„å‚æ•°ï¼Œæˆ–è€…debugæ¨¡å‹ä¸­é—´å±‚çš„è¾“å…¥è¾“å‡ºï¼Œå…¶ç±»å‹ä¾ç„¶æ˜¯float32ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰è°ƒç”¨megatron.training.get_modelå‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥è°ƒç”¨äº†megatron.model.GPTModelï¼Œè¿™é‡ŒGPTModelæ²¡æœ‰å¯¹æ¨¡å‹çš„æ•°å€¼ç±»å‹åšä»»ä½•çš„å¤„ç†ï¼Œä»…ä»…æ˜¯é€šè¿‡para_dtypeåˆå§‹åŒ–äº†æ¨¡å‹å‚æ•°ã€‚è€Œget_modelå‡½æ•°é‡Œé¢ï¼Œåˆ™ä¼šæ ¹æ®bf16å‚æ•°ï¼Œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œç±»å‹è½¬æ¢ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core.enums</span> <span class="kn">import</span> <span class="n">ModelType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">model_provider_func</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="n">ModelType</span><span class="o">.</span><span class="n">encoder_or_decoder</span><span class="p">,</span> <span class="n">wrap_with_ddp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Fp16 conversion.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">bf16</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span> <span class="o">=</span> <span class="n">get_model_config</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="p">[</span><span class="n">Float16Module</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model_module</span><span class="p">)</span> <span class="k">for</span> <span class="n">model_module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span></code></pre></div><p>è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹Float16Moduleçš„å®ç°ï¼Œè¯¥æ¨¡å—åœ¨è®¡ç®—å‰å‘çš„æ—¶å€™ï¼Œåœ¨forwardå‡½æ•°ä¸­ä¸´æ—¶å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œç®—å®Œåï¼Œåˆè½¬æ¢ä¸ºfp32ç²¾åº¦ã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Float16Module</span><span class="p">(</span><span class="n">MegatronModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Float 16 Module.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Attributes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        config (TransformerConfig): Transformer config
</span></span></span><span class="line"><span class="cl"><span class="s2">        fp16 (bool) : Specifies if the model runs in fp16 mode
</span></span></span><span class="line"><span class="cl"><span class="s2">        bf16 (bool) : Specifies if the model runs in bf16 mode
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        config (TransformerConfig): The transformer config used to initalize the model
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">TransformerConfig</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">Float16Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fp16</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fp16</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">bf16</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">bf16</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp16</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;module&#39;</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">half</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">float16_convertor</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">val</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf16</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;module&#39;</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">float16_convertor</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">val</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Either config.fp16 or config.bf16 should be True.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">float16_convertor</span> <span class="o">=</span> <span class="n">float16_convertor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">parallel_state</span><span class="o">.</span><span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">inputs</span> <span class="o">=</span> <span class="n">fp32_to_float16</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">float16_convertor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">parallel_state</span><span class="o">.</span><span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="n">float16_to_fp32</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">outputs</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span></code></pre></div><p>åœ¨å®è·µä¸­ï¼Œä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œæƒ³é¿å…Float16Moduleå¯¹è‡ªå®šä¹‰çš„ç½‘ç»œå±‚çš„å‚æ•°æ•°æ®ç±»å‹è¿›è¡Œè½¬æ¢ï¼ˆä¸»è¦æ˜¯æƒ³ä¿æŒfp32ç²¾åº¦ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶é‡å†™ç½‘ç»œå±‚çš„half(self)ï¼Œbfloat16(self)æ–¹æ³•å’Œ_apply(self, fn)æ–¹æ³•ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CustomLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span> 
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span></code></pre></div><p>è¿™æ ·å¯ä»¥é¿å…ç½‘ç»œå¯¹è‡ªå®šä¹‰ç½‘ç»œå±‚çš„å‚æ•°è¿›è¡Œç±»å‹è½¬æ¢ã€‚å¦å¤–ï¼Œåœ¨å‰å‘çš„æ—¶å€™ï¼Œè¿˜å°½é‡æ˜¾å¼çš„å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºtorch.float32ã€‚</p>
<h2 id="mindspeed-è®­ç»ƒä¸»å¾ªç¯å…¥å£">MindSpeed è®­ç»ƒä¸»å¾ªç¯å…¥å£<a hidden class="anchor" aria-hidden="true" href="#mindspeed-è®­ç»ƒä¸»å¾ªç¯å…¥å£">#</a></h2>
<p>åœ¨åŸºäºMegatronçš„æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ç»å¸¸çœ‹åˆ°è¿™æ ·çš„è®­ç»ƒå¾ªç¯ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">megatron.core.pipeline_parallel.schedules</span> <span class="kn">import</span> <span class="n">get_forward_backward_func</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optim</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">gpt_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">forward_backward_func</span> <span class="o">=</span> <span class="n">get_forward_backward_func</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Starting iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/5...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Gradients zeroed.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">losses_reduced</span> <span class="o">=</span> <span class="n">forward_backward_func</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">forward_step_func</span><span class="o">=</span><span class="n">forward_step_func</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_iterator</span><span class="o">=</span><span class="n">train_iterator</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">=</span><span class="n">gpt_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_microbatches</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_length</span><span class="o">=</span><span class="n">_SEQUENCE_LENGTH</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">micro_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">decoder_seq_length</span><span class="o">=</span><span class="n">_SEQUENCE_LENGTH</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">forward_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Forward-backward pass completed with losses: </span><span class="si">{</span><span class="n">losses_reduced</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></div><p><code>forward_backward_func</code>æ˜¯<code>megatron</code>å†…ç½®çš„åŒ…è£…å¥½çš„å‡½æ•°ï¼Œå…¶å†…ç½®äº†å‰å‘å’Œåå‘çš„è®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯ä¼ å…¥è‡ªå®šä¹‰çš„<code>forward_step_func</code>å’Œ<code>data_iterator</code>å’Œ<code>model</code>ã€‚è¾“å‡ºåˆ™æ˜¯reducedçš„lossï¼Œå…¶shapeä¸º[bs, seq_len]ï¼Œå³æ¯ä¸€ä¸ªtokençš„lossã€‚</p>
<p>è‡ªå®šä¹‰çš„å‰å‘å‡½æ•°<code>forward_backward_func</code>ï¼Œä¸»è¦åŒ…æ‹¬äº†å¯¹è¾“å…¥æ•°æ®çš„ç®€å•é¢„å¤„ç†å’Œlosså‡½æ•°çš„å®šä¹‰ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward_step_func</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    è‡ªå®šä¹‰å‰å‘å‡½æ•°
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Notes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. model(tokens, position_ids, attention_mask, labels=labels)è¿”å›çš„æ˜¯lossï¼Œè€Œä¸æ˜¯logits
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. è¦è¿”å›logitsï¼Œåˆ™ä»…éœ€è¦æ‹¿æ‰labelså‚æ•°å³å¯
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">loss_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">losses</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss_mask</span> <span class="o">=</span> <span class="n">loss_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_mask</span><span class="p">)</span> <span class="o">/</span> <span class="n">loss_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># If you have data parallel reduce loss across data parallel groups.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># If pipeline parallel, loss computation is done only in last stage.</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;lm loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokens</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;position_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;loss_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># output loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="n">partial</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span> <span class="n">loss_mask</span><span class="p">)</span>
</span></span></code></pre></div><p>æ³¨æ„ï¼Œå‰å‘å‡½æ•°è¿”å›çš„æ˜¯æ¨¡å‹è¾“å‡ºçš„æ¯ä¸€ä¸ªtokençš„lossï¼Œå’Œlosså‡½æ•°çš„åå‡½æ•°ã€‚å…³äºlossçš„è®¡ç®—ï¼Œå¤§æ¨¡å‹ç»™å‡ºäº†ä¸€ä¸ªè§£ç­”ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">æ•´ä¸ªlossè®¡ç®—åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="gs">**ç¬¬ä¸€é˜¶æ®µï¼ˆåœ¨æ¨¡å‹å†…éƒ¨ï¼‰**</span>ï¼š
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">-</span> æ¨¡å‹è‡ªå·±è®¡ç®—åŸºç¡€çš„tokençº§åˆ«loss
</span></span><span class="line"><span class="cl"><span class="k">-</span> è¿”å›å½¢çŠ¶ä¸º [batch_size, sequence_length] çš„tensor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="gs">**ç¬¬äºŒé˜¶æ®µï¼ˆåœ¨è‡ªå®šä¹‰loss_funcä¸­ï¼‰**</span>ï¼š
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">-</span> å¯¹æ¨¡å‹è¿”å›çš„lossè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
</span></span><span class="line"><span class="cl"><span class="k">-</span> åº”ç”¨loss_maskè¿›è¡Œè¿‡æ»¤
</span></span><span class="line"><span class="cl"><span class="k">-</span> è®¡ç®—æœ€ç»ˆçš„å¹³å‡loss
</span></span></code></pre></div><h2 id="è°ƒè¯•mindspeedè®­ç»ƒ">è°ƒè¯•Mindspeedè®­ç»ƒ<a hidden class="anchor" aria-hidden="true" href="#è°ƒè¯•mindspeedè®­ç»ƒ">#</a></h2>
<p>Mindspeedè®­ç»ƒä»£ç é€šå¸¸æ˜¯å¤šæœºå¤šå¡çš„ï¼Œå› æ­¤è°ƒè¯•èµ·æ¥å¯èƒ½ä¼šæ¯”è¾ƒéº»çƒ¦ã€‚è¿™é‡Œæä¾›ä¸€ä¸ªæˆç†ŸéªŒè¯çš„æ–¹æ³•æ¥debugã€‚</p>
<ol>
<li>åœ¨è®­ç»ƒä»£ç ä¸­ï¼Œæ·»åŠ setup_debugpy()å‡½æ•°</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">setup_debugpy</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;åœ¨æ¯ä¸ª rank ä¸­è®¾ç½® debugpy&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;RANK&#34;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ä½¿ç”¨ 5678 + rank ä½œä¸ºç«¯å£ï¼ˆç¡®ä¿ä¸å†²çªï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="n">debugpy_port</span> <span class="o">=</span> <span class="mi">22333</span> <span class="o">+</span> <span class="n">rank</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Waiting for debugger attach on port </span><span class="si">{</span><span class="n">debugpy_port</span><span class="si">}</span><span class="s2">...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">debugpy</span><span class="o">.</span><span class="n">listen</span><span class="p">((</span><span class="s2">&#34;0.0.0.0&#34;</span><span class="p">,</span> <span class="n">debugpy_port</span><span class="p">))</span>  <span class="c1"># ç»‘å®šæ‰€æœ‰æ¥å£ï¼ˆå…¼å®¹å®¹å™¨/è¿œç¨‹ï¼‰</span>
</span></span><span class="line"><span class="cl">    <span class="n">debugpy</span><span class="o">.</span><span class="n">wait_for_client</span><span class="p">()</span>  <span class="c1"># é˜»å¡ç›´åˆ° VS Code è¿æ¥</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Debugger attached!&#34;</span><span class="p">)</span>
</span></span></code></pre></div><ol start="2">
<li>é…ç½®debugæ–‡ä»¶,.vscode/launch.jsoné‡Œé¢å†™å…¥é…ç½®ï¼š</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;version&#34;</span><span class="p">:</span> <span class="s2">&#34;0.2.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;configurations&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;Attach to Worker 0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;debugpy&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;request&#34;</span><span class="p">:</span> <span class="s2">&#34;attach&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;connect&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;host&#34;</span><span class="p">:</span> <span class="s2">&#34;localhost&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;port&#34;</span><span class="p">:</span> <span class="mi">22333</span>
</span></span><span class="line"><span class="cl">      <span class="p">},</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;pathMappings&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;localRoot&#34;</span><span class="p">:</span> <span class="s2">&#34;${workspaceFolder}&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;remoteRoot&#34;</span><span class="p">:</span> <span class="s2">&#34;.&#34;</span>  <span class="c1">// å¦‚æœæœ¬åœ°/è¿œç¨‹è·¯å¾„ä¸€è‡´
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;Attach to Worker 1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;debugpy&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;request&#34;</span><span class="p">:</span> <span class="s2">&#34;attach&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;connect&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;host&#34;</span><span class="p">:</span> <span class="s2">&#34;localhost&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;port&#34;</span><span class="p">:</span> <span class="mi">22334</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="err">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ol start="3">
<li>
<p>åœ¨åˆé€‚çš„åœ°æ–¹å‡ºå…¥setup_debugpy()ï¼Œæ”¾åœ¨å¯åŠ¨äº†å¤šè¿›ç¨‹ä¹‹åçš„ä»£ç ä½ç½®ï¼Œå‚è€ƒ<a href="https://github.com/fandengdong/fandengdong.github.io/blob/main/content/LLM/mindspeed/codes/simple_mcore_train_loop.py#L129">è¿™é‡Œ</a></p>
</li>
<li>
<p>æ­£å¸¸å¯åŠ¨Mindspeedçš„è®­ç»ƒ</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">torchrun --nproc-per-node <span class="m">4</span> mindspeed_train.py 
</span></span></code></pre></div><ol start="5">
<li>vscodeè°ƒè¯•å™¨é“¾æ¥ä»£ç </li>
</ol>
<p>å½“ç»ˆç«¯æ‰“å°å‡º<code>[Rank 0] Waiting for debugger attach on port 22333...</code>ï¼Œåˆ™å¯ä»¥ç‚¹å‡»vscodeçš„debuggerå›¾æ ‡ï¼Œé€‰æ‹©Attach to Worker 0ï¼Œç„¶åç‚¹å‡»å¯åŠ¨æŒ‰é’®ã€‚</p>
<h2 id="å…³äºmindspeedçš„è®­ç»ƒç²¾åº¦">å…³äºmindspeedçš„è®­ç»ƒç²¾åº¦<a hidden class="anchor" aria-hidden="true" href="#å…³äºmindspeedçš„è®­ç»ƒç²¾åº¦">#</a></h2>
<p>mindspeedæä¾›äº†ä¸¤ä¸ªflagï¼Œç”¨äºè®¾ç½®è®­ç»ƒç²¾åº¦ï¼š</p>
<ol>
<li><code>--fp16</code>: ä½¿ç”¨fp16è®­ç»ƒç²¾åº¦ã€‚fp16è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´æ¯”è¾ƒå°ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸Šæº¢æˆ–è€…ä¸‹æº¢ï¼Œé‡‡ç”¨fp16è¿›è¡Œè®­ç»ƒï¼Œmindspeedä¼šè‡ªåŠ¨è°ƒç”¨loss scaleræ¥å¤„ç†æ¢¯åº¦å‘ä¸‹æº¢å‡ºçš„é—®é¢˜ï¼ˆæ³¨æ„loss scaleræ˜¯è‡ªåŠ¨åœ¨optimizerä¸­å¤„ç†çš„ï¼‰ã€‚</li>
<li><code>--bf16</code>: ä½¿ç”¨bf16è®­ç»ƒç²¾åº¦ã€‚bf16é‡‡ç”¨äº†æ›´å¤šçš„æŒ‡æ•°ä½ï¼Œå› æ­¤è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´ä¼šå¤§å¾ˆå¤šï¼Œå› æ­¤ä¸éœ€è¦loss scalerï¼Œä½†æ˜¯bf16çš„æ•°å€¼ç²¾åº¦è¦æ¯”fp16å°ã€‚</li>
<li>å¦‚æœ<code>--fp16</code>å’Œ<code>--bf16</code>éƒ½æ˜¯falseï¼Œåˆ™é»˜è®¤ä¼šä½¿ç”¨fp32è®­ç»ƒç²¾åº¦ã€‚</li>
</ol>
<h3 id="æµ®ç‚¹æ•°æ ¼å¼æ¯”è¾ƒ">æµ®ç‚¹æ•°æ ¼å¼æ¯”è¾ƒ<a hidden class="anchor" aria-hidden="true" href="#æµ®ç‚¹æ•°æ ¼å¼æ¯”è¾ƒ">#</a></h3>
<h4 id="fp16-æ ¼å¼">FP16 æ ¼å¼<a hidden class="anchor" aria-hidden="true" href="#fp16-æ ¼å¼">#</a></h4>
<p>FP16 æ˜¯ä¸€ç§ 16 ä½æµ®ç‚¹æ•°æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š</p>
<ul>
<li><strong>1 ä½</strong>ç”¨äºç¬¦å·ï¼ˆSï¼‰</li>
<li><strong>5 ä½</strong>ç”¨äºæŒ‡æ•°ï¼ˆEï¼‰</li>
<li><strong>10 ä½</strong>ç”¨äºå°¾æ•°ï¼ˆMï¼Œä¹Ÿç§°ä¸ºåˆ†æ•°æˆ– mantissaï¼‰</li>
</ul>
<table>
  <thead>
      <tr>
          <th>å­—æ®µ</th>
          <th>æ¯”ç‰¹æ•°</th>
          <th>æè¿°</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ç¬¦å· (S)</td>
          <td>1</td>
          <td>è¡¨ç¤ºæ•°å­—çš„æ­£è´Ÿ</td>
      </tr>
      <tr>
          <td>æŒ‡æ•° (E)</td>
          <td>5</td>
          <td>ä½¿ç”¨åç½®ï¼ˆbiasï¼‰ä¸º 15ï¼Œè¡¨ç¤ºèŒƒå›´çº¦ä¸º [-14, 15]</td>
      </tr>
      <tr>
          <td>å°¾æ•° (M)</td>
          <td>10</td>
          <td>éšå«å‰å¯¼ 1ï¼Œå®é™…æœ‰æ•ˆç²¾åº¦ä¸º 11 ä½</td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="bf16-æ ¼å¼">BF16 æ ¼å¼<a hidden class="anchor" aria-hidden="true" href="#bf16-æ ¼å¼">#</a></h4>
<p>BF16 ä¹Ÿæ˜¯ä¸€ç§ 16 ä½æµ®ç‚¹æ•°æ ¼å¼ï¼Œä½†æ¯”ç‰¹åˆ†é…ä¸åŒï¼š</p>
<ul>
<li><strong>1 ä½</strong>ç”¨äºç¬¦å·ï¼ˆSï¼‰</li>
<li><strong>8 ä½</strong>ç”¨äºæŒ‡æ•°ï¼ˆEï¼‰</li>
<li><strong>7 ä½</strong>ç”¨äºå°¾æ•°ï¼ˆMï¼‰</li>
</ul>
<table>
  <thead>
      <tr>
          <th>å­—æ®µ</th>
          <th>æ¯”ç‰¹æ•°</th>
          <th>æè¿°</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ç¬¦å· (S)</td>
          <td>1</td>
          <td>è¡¨ç¤ºæ•°å­—çš„æ­£è´Ÿ</td>
      </tr>
      <tr>
          <td>æŒ‡æ•° (E)</td>
          <td>8</td>
          <td>ä½¿ç”¨åç½®ï¼ˆbiasï¼‰ä¸º 127ï¼Œè¡¨ç¤ºèŒƒå›´çº¦ä¸º [-126, 127]</td>
      </tr>
      <tr>
          <td>å°¾æ•° (M)</td>
          <td>7</td>
          <td>æœ‰æ•ˆç²¾åº¦è¾ƒä½ï¼Œæ— éšå«ä½ä¼˜åŒ–ï¼ˆä¸ FP32 å¯¹é½ï¼‰</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="æ•°å€¼è¡¨ç¤ºèŒƒå›´å’Œç²¾åº¦å¯¹æ¯”">æ•°å€¼è¡¨ç¤ºèŒƒå›´å’Œç²¾åº¦å¯¹æ¯”<a hidden class="anchor" aria-hidden="true" href="#æ•°å€¼è¡¨ç¤ºèŒƒå›´å’Œç²¾åº¦å¯¹æ¯”">#</a></h3>
<p>ç”±äº BF16 å°†æ›´å¤šæ¯”ç‰¹åˆ†é…ç»™æŒ‡æ•°éƒ¨åˆ†ï¼Œå…¶<strong>æ•°å€¼è¡¨ç¤ºèŒƒå›´è¿œå¤§äº FP16</strong>ï¼Œæ¥è¿‘ FP32ï¼›ä½†å°¾æ•°ä½æ›´å°‘ï¼Œå› æ­¤<strong>ç²¾åº¦ä½äº FP16</strong>ã€‚</p>
<table>
  <thead>
      <tr>
          <th>æ ¼å¼</th>
          <th>æ€»ä½æ•°</th>
          <th>æŒ‡æ•°ä½æ•°</th>
          <th>å°¾æ•°ä½æ•°</th>
          <th>æ•°å€¼èŒƒå›´ï¼ˆè¿‘ä¼¼ï¼‰</th>
          <th>ç²¾åº¦</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FP16</td>
          <td>16</td>
          <td>5</td>
          <td>10</td>
          <td>(6.1 \times 10^{-5}) åˆ° (6.5 \times 10^{4})</td>
          <td>è¾ƒé«˜</td>
      </tr>
      <tr>
          <td>BF16</td>
          <td>16</td>
          <td>8</td>
          <td>7</td>
          <td>æ¥è¿‘ FP32ï¼ˆçº¦ (10^{-38}) åˆ° (10^{38})ï¼‰</td>
          <td>ä¸­ç­‰ï¼ˆä½äº FP16ï¼‰</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>ğŸ’¡ <strong>æ€»ç»“</strong>ï¼š</p>
<ul>
<li><strong>FP16</strong>ï¼šç²¾åº¦é«˜ï¼Œä½†åŠ¨æ€èŒƒå›´å°ï¼Œè®­ç»ƒæ—¶éœ€é…åˆ <strong>loss scaler</strong> é˜²æ­¢ä¸‹æº¢ã€‚</li>
<li><strong>BF16</strong>ï¼šåŠ¨æ€èŒƒå›´å¤§ï¼ˆç±»ä¼¼ FP32ï¼‰ï¼Œæ— éœ€ loss scalerï¼Œä½†æ•°å€¼ç²¾åº¦è¾ƒä½ï¼Œé€‚åˆå¯¹èŒƒå›´æ•æ„Ÿã€å¯¹ç²¾åº¦å®¹å¿åº¦è¾ƒé«˜çš„æ·±åº¦å­¦ä¹ è®­ç»ƒåœºæ™¯ã€‚</li>
</ul>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My work notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
