<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head>
	<meta name="generator" content="Hugo 0.152.2"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>My work notes</title>

<meta name="description" content="记录论文阅读与代码实践">
<meta name="author" content="fandengdong">
<link rel="canonical" href="http://localhost:1313/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="My work notes">
  <meta property="og:title" content="欢迎来到我的工作空间">
  <meta property="og:description" content=" 📚 我的工作笔记 这里记录我在人工智能、大语言模型（LLM）和开发工具方面的学习与实践。 欢迎一起探索技术的边界！ 🧠 RL 💬 LLM 🧰 工具箱 ">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="欢迎来到我的工作空间">
<meta name="twitter:description" content="记录论文阅读与代码实践">

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "My work notes",
  "url": "http://localhost:1313/",
  "description": "记录论文阅读与代码实践",
  "logo": "http://localhost:1313/favicon.ico",
  "sameAs": [
      
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My work notes (Alt + H)">My work notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/llm/" title="大语言模型 (LLM)">
                    <span>大语言模型 (LLM)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/toolbox/" title="工具箱 (toolbox)">
                    <span>工具箱 (toolbox)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/rl/" title="强化学习 (RL)">
                    <span>强化学习 (RL)</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/" title="欢迎来到我的工作空间">
                    <span class="active">欢迎来到我的工作空间</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<div class="post-content"><div style="text-align: center; max-width: 700px; margin: 2rem auto; padding: 0 1rem;">
  <h2>📚 我的工作笔记</h2>
  <p style="font-size: 1.1em; color: #555; line-height: 1.6;">
    这里记录我在人工智能、大语言模型（LLM）和开发工具方面的学习与实践。
    欢迎一起探索技术的边界！
  </p>
<div style="display: flex; justify-content: center; gap: 2rem; margin-top: 2rem; flex-wrap: wrap;">
  <a href="/rl/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    🧠 RL
  </a>

  <a href="/llm/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    💬 LLM
  </a>

  <a href="/toolbox/" class="btn btn-primary" style="display: inline-block; padding: 0.8rem 1.5rem; background: #f0f8ff; border-radius: 12px; text-decoration: none; color: #2c3e50; font-weight: bold; box-shadow: 0 2px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;">
    🧰 工具箱
  </a>

</div>
  </div>
</div>


</div>

<article class="first-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">MLA：Multi-head Latent Attention
    </h2>
  </header>
  <div class="entry-content">
    <p>一、MLA 的由来 背景：KV Cache 成为推理瓶颈 在 Transformer 解码器中，自回归生成时需缓存所有历史 token 的 Key 和 Value（即 KV Cache）。随着上下文长度增长（如 32K、128K tokens），KV Cache 占用大量显存和带宽，成为推理效率的主要瓶颈。
目标：压缩 KV Cache 为减少 KV Cache 大小，研究者提出多种压缩方法：
MQA（Multi-Query Attention）：所有头共享一组 K/V。 GQA（Grouped-Query Attention）：将多个头分组共享 K/V。 MLA（Multi-head Latent Attention）：不直接存储原始 K/V，而是学习一个低维“潜在表示”（latent code），通过小型网络动态重建近似的 K/V。 提出者与出处 DeepSeek-V2（2024） 首次系统性提出并命名 MLA（Multi-head Latent Attention）。 核心思想：用 低秩潜在向量 &#43; 小型投影网络 代替传统 KV Cache，大幅降低内存占用（论文称减少 77% KV Cache）。 二、基本原理 MLA 的核心思想是：
不显式存储每个 token 的 K 和 V，而是存储一个紧凑的“潜在向量” $ z_t \in \mathbb{R}^{d_z} $（$ d_z \ll d_k, d_v $），在需要时通过轻量级可学习映射 $ f_K, f_V $ 动态重建 K 和 V。
...</p>
  </div>
  <footer class="entry-footer"><span title='2026-01-07 00:00:00 +0000 UTC'>January 7, 2026</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to MLA：Multi-head Latent Attention" href="http://localhost:1313/llm/architecture/mla/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">SwiGLU激活函数解析
    </h2>
  </header>
  <div class="entry-content">
    <p>SwiGLU（Swish-Gated Linear Unit）是一种在现代大语言模型（如PaLM、LLaMA等）中广泛使用的激活函数变体，用于替代传统的前馈网络（Feed-Forward Network, FFN）中的 ReLU 或 GELU。它结合了门控机制（gating）与 Swish 激活函数，具有更强的表达能力和训练稳定性。
一、SwiGLU 的由来 SwiGLU 最早由 Shazeer (2020) 在论文《GLU Variants Improve Transformer》中系统性地提出并评估。该工作探索了多种基于 Gated Linear Unit (GLU) 的激活函数变体，并发现 SwiGLU 在语言建模任务中表现最优。
背景：GLU（Gated Linear Unit） GLU 最初由 Dauphin 等人（2017）在《Language Modeling with Gated Convolutional Networks》中提出，其基本形式为：
[ \text{GLU}(x) = (W_1 x &#43; b_1) \otimes \sigma(W_2 x &#43; b_2) ]
其中：
( \otimes ) 表示逐元素乘法（Hadamard product）， ( \sigma ) 是 Sigmoid 激活函数。 GLU 引入了“门控”思想：一部分线性变换被另一部分通过激活函数“门控”，从而实现更灵活的信息控制。
SwiGLU 的改进 SwiGLU 将 GLU 中的 Sigmoid 替换为 Swish（也称 SiLU）激活函数：
...</p>
  </div>
  <footer class="entry-footer"><span title='2026-01-07 00:00:00 +0000 UTC'>January 7, 2026</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to SwiGLU激活函数解析" href="http://localhost:1313/llm/architecture/swiglu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">ALiBi - 解决传统位置编码外推问题
    </h2>
  </header>
  <div class="entry-content">
    <p>一、ALiBi 的由来：为什么需要它？ 1. 传统位置编码的外推问题 在标准 Transformer 中，位置信息通过 绝对位置编码（如正弦、可学习）或相对位置编码（如 RoPE） 注入。但这些方法在 训练长度 &lt; 推理长度 时表现不佳：
绝对位置编码：无法处理训练时未见过的位置索引 RoPE：虽可通过插值（如 YaRN）扩展，但外推能力仍有限，且需额外调参 💡 问题核心：位置编码与序列长度强耦合
2. ALiBi 的提出 论文：《Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation》（ICLR 2022） 作者：Ofir Press et al.（来自 AI21 Labs） 核心思想：完全移除位置编码，改用 与距离成线性关系的偏置（bias） 直接加到 attention score 上 ✅ 优势：
模型可在短序列上训练，在超长序列上直接推理（无需微调） 架构更简洁（无位置嵌入层） 在长上下文任务上表现优异 被 BLOOM（176B） 等大模型采用 二、基本原理 ALiBi 的关键洞察是：
人类语言中，近期 token 通常比远期 token 更相关。
因此，注意力应天然倾向于 局部性（locality），且衰减速度可 head-specific 控制。
为此，ALiBi 在计算 attention score 时，对每个 head 引入一个 线性偏置项：
...</p>
  </div>
  <footer class="entry-footer"><span title='2026-01-04 00:00:00 +0000 UTC'>January 4, 2026</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to ALiBi - 解决传统位置编码外推问题" href="http://localhost:1313/llm/architecture/alibi-%E8%A7%A3%E5%86%B3%E4%BC%A0%E7%BB%9F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A4%96%E6%8E%A8%E9%97%AE%E9%A2%98/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RMSNorm解析 - Root Mean Square Layer Normalization
    </h2>
  </header>
  <div class="entry-content">
    <p>RMSNorm（Root Mean Square Layer Normalization）是一种用于归一化层输入的归一化方法，它基于输入向量的均方根（RMS）作为归一化的权重。RMSNorm 的主要作用是提高训练效率，并防止梯度爆炸。
一、RMSNorm 的由来 1. Layer Normalization（LayerNorm）的局限性 在 Transformer 架构中，LayerNorm 被广泛用于稳定训练。其标准形式为：
[ \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 &#43; \epsilon}} &#43; \beta ]
其中：
(\mu = \frac{1}{d}\sum_{i=1}^d x_i)（均值） (\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2)（方差） 虽然有效，但 减去均值（centering）操作在语言建模任务中可能并非必要，甚至可能干扰某些表示。
2. RMSNorm 的提出 论文：《Root Mean Square Layer Normalization》（ICLR 2020） 作者：Biao Zhang, Rico Sennrich（来自爱丁堡大学 &amp; Amazon） 核心思想：去掉 centering（即不减均值），仅做 scaling（缩放），用更简单的归一化方式达到类似甚至更好的效果。 ✅ 优势：
计算更简单（少一次减均值） 减少计算开销（约 7%~10% faster） 在 NLP 任务上性能相当或略优 更适合大规模自回归语言模型 如今，LLaMA、Qwen、Mistral、DeepSeek 等主流大模型全部采用 RMSNorm，而非 LayerNorm。
...</p>
  </div>
  <footer class="entry-footer"><span title='2026-01-04 00:00:00 +0000 UTC'>January 4, 2026</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to RMSNorm解析 - Root Mean Square Layer Normalization" href="http://localhost:1313/llm/architecture/rmsnorm%E8%A7%A3%E6%9E%90/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GQA - Grouped-query Attention 分组查询注意力
    </h2>
  </header>
  <div class="entry-content">
    <p>GQA（Grouped-Query Attention）将多个查询头（Query heads）分组，每组共享同一个键/值头（Key/Value head），在显著降低KV缓存和计算开销的同时，保持接近多头注意力（MHA）的模型性能。
它在MHA（多头）和MQA（单头KV）之间取得高效与性能的平衡，已成为LLaMA、Qwen、DeepSeek等主流大模型的标准组件。
一、GQA 的由来：为什么需要 GQA？ 在标准的 Multi-Head Attention（MHA） 中：
Query（Q）、Key（K）、Value（V） 都被分成 h 个头，每个头独立计算注意力。 推理时，K 和 V 需要缓存（KV Cache），总缓存大小为：
[ \text{Cache Size} = 2 \times h \times n \times d_k ] 其中 (n) 是上下文长度，(d_k) 是每个 head 的维度。 当模型变大（如 LLaMA-65B，h=64），KV Cache 占用大量显存，成为推理瓶颈。
替代方案对比： 方法 Q 头数 K/V 头数 KV Cache 表达能力 MHA（多头注意力） h h 大 强 MQA（多查询注意力） h 1 极小 弱（性能下降明显） GQA（分组查询注意力） h g（g ≪ h） 中等 接近 MHA 💡 GQA 由 Google 在 2023 年论文《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》提出，旨在 平衡效率与性能。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-31 00:00:00 +0000 UTC'>December 31, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to GQA - Grouped-query Attention 分组查询注意力" href="http://localhost:1313/llm/architecture/gqa/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">YaRN -  旋转位置编码拓展
    </h2>
  </header>
  <div class="entry-content">
    <p>旋转位置编码（RoPE, Rotary Position Embedding）是一种在Transformer模型中引入相对位置信息 的方法，由苏剑林等Credo在2021年提出。相比传统的绝对位置编码（如BERT中的可学习位置嵌入或正弦位置编码），RoPE通过将位置信息以旋变换的方式融入注意力机制，使得模型可以自然地感知相对位置信息。
但是在实践中，我们往往发现有些应用需要处理更长的序列，而模型在训练时候的输入序列长度往往有限，这时RoPE就无法满足需求。我们需要一个能够将位置编码进行外推的技术。YARN（Yet Another RoPE-based Attention Network）是一种用于扩展大语言模型上下文长度的技术，它是在RoPE（Rotary Position Embedding）基础上进行改进的，旨在让模型在训练时使用较短上下文，但在推理时能有效处理远超训练长度的输入序列。
背景：为什么需要 YARN？ 1. 位置编码的外推问题 大多数大语言模型（如 LLaMA、ChatGLM 等）使用 RoPE（旋转位置编码） 来注入位置信息。RoPE 的优点是具有良好的相对位置建模能力，但其缺点是：
训练时最大上下文长度有限（比如 2048 或 4096）。 直接推理更长序列时性能急剧下降（因为 RoPE 中的频率参数未覆盖更长距离）。 2. 已有方法的局限 NTK-aware scaling（Neural Tangent Kernel 缩放）：通过缩放 RoPE 的 base 频率来“拉伸”位置编码，但会牺牲短距离精度。 YaRN 提出了一种更精细的插值 &#43; 缩放策略，在保持短程精度的同时提升长程泛化能力。 YARN 的核心思想 YARN 在 NTK-aware scaling 基础上引入了以下关键改进：
1. 局部窗口内保持原始 RoPE（不缩放） 对于位置 ≤ 原始训练长度（如 32K），使用原始 RoPE，保留高精度。 2. 对超出部分使用缩放后的 RoPE 对于位置 &gt; 训练长度，使用经过缩放的 base 频率（类似 NTK 方法）。
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-25 00:00:00 +0000 UTC'>December 25, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to YaRN -  旋转位置编码拓展" href="http://localhost:1313/llm/architecture/yarn%E6%8E%A8%E7%90%86%E6%89%A9%E5%B1%95%E6%8A%80%E6%9C%AF/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">MOE - 混合专家架构
    </h2>
  </header>
  <div class="entry-content">
    <p>混合专家（Mixture of Experts, MoE）是一种**条件计算（conditional computation）**架构，旨在在不显著增加计算成本的前提下扩展模型容量。它已被广泛应用于现代大语言模型（如 Google 的 GLaM、Mixtral、Qwen-MoE、DeepSeek-MoE 等）。
MoE 的核心思想 🎯 目标： 扩大模型参数量（提升表达能力） 保持每次前向计算的 FLOPs 基本不变（高效推理） 🔑 关键机制： 模型由多个“专家”（Experts）组成，每个专家是一个子网络（如 FFN）。 引入一个门控网络（Gating Network），根据输入动态决定激活哪几个专家。 通常只激活 Top-K 个专家（如 K=1 或 K=2），其余专家不参与计算。 ✅ 这样：总参数量 = 所有专家参数之和（很大），但每 token 计算量 ≈ K 个专家（很小）。
MoE给Transformer架构带来的改变 下面我们从 Transformer 架构出发，深入讲解 MoE（Mixture of Experts）的数学形式、门控机制来源、专家模块在 Transformer 中的具体位置与作用
MoE 在 Transformer 中的位置 在标准 Transformer 中，每个 Transformer 层包含两个核心子模块：
多头注意力（Multi-Head Attention, MHA） 前馈网络（Feed-Forward Network, FFN） ✅ MoE 通常只替换 FFN 部分，而保留 MHA 不变。
也就是说，一个 MoE Transformer 层的结构为：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-24 00:00:00 +0000 UTC'>December 24, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to MOE - 混合专家架构" href="http://localhost:1313/llm/architecture/moe/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RoPE - 旋转位置编码解读
    </h2>
  </header>
  <div class="entry-content">
    <p>旋转位置编码（RoPE, Rotary Position Embedding）是一种在Transformer模型中引入相对位置信息的方法，由苏剑林等人在2021年提出。相比传统的绝对位置编码（如BERT中的可学习位置嵌入或正弦位置编码），RoPE通过将位置信息以旋转变换的方式融入注意力机制，使得模型天然具备对相对位置的感知能力。
回顾原始位置编码 在Transformer模型中，位置编码（Position Encoding）用于将输入序列中的每个位置映射为向量，以表示其相对位置信息。原始位置编码的实现方式有很多种，如可学习位置嵌入（Learnable Position Embedding）或正弦位置编码（Sinusoidal Position Encoding）。
假设我们的词汇表大小为[vocab_size, embedding_dim]，输入句子的token长度为seq_len，每个token会被转换成一个vector向量，维度为[embedding_dim]，则整个输入句子的shape为[ seq_len, embedding_dim]。
但是，为了让模型理解序列的顺序，还需要加入位置编码(Positional Encoding)。这就是著名的公式：
$$ PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
$$ PE_{(pos,2i&#43;1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
pos是位置索引，即token在序列中的位置，从0开始； i是维度索引，从0开始到$(d_{model}-1)/2$结束，表示词嵌入的维度； $d_{model}$是词嵌入（word embedding）的维度embedding_dim 观察输入句子的shape: [seq_len, embedding_dim]，位置编码的时候，pos对应的是token在seq_len维度的位置，而维度索引i对应词嵌入embedding_dim的维度。
pytorch代码实现:
class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1).float() div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) self.register_buffer(&#39;pe&#39;, pe.unsqueeze(0)) def forward(self, x): return x &#43; self.pe[:, :x.size(1)] RoPE 的核心思想 相对位置 vs 绝对位置 在传统Transformer中，位置编码是加到词嵌入上的（如 x &#43; pos_emb），这种方式只保留了绝对位置信息。 而 RoPE 则通过将查询（Q）和键（K）向量进行旋转变换，使得注意力分数自然包含两个 token 之间的相对距离。 旋转操作 对于维度为 d 的向量，RoPE 将其分成若干二维子空间（通常是相邻两个维度一组），每组应用一个二维旋转矩阵：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to RoPE - 旋转位置编码解读" href="http://localhost:1313/llm/architecture/rope/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">一切的开始 - Transformer架构
    </h2>
  </header>
  <div class="entry-content">
    <p>Transformer的诞生背景 在深度学习的世界里，序列建模一直是个老大难问题。传统的循环神经网络(RNN)虽然能够处理序列数据，但在处理长序列时存在梯度消失和计算效率低下的问题。而卷积神经网络(CNN)虽然并行化程度高，但在捕捉长距离依赖关系方面力不从心。
就在这个关键时刻，Google的大神们在2017年扔出了一个&#34;核弹级&#34;的解决方案——《Attention Is All You Need》。这篇论文彻底颠覆了人们对序列建模的认知，提出了完全基于注意力机制的Transformer架构。
翻译任务：Transformer的试验场 Transformer最初是为了机器翻译任务而设计的。想象一下，你要把一句中文翻译成英文：
中文输入: “我喜欢学习人工智能”
英文输出: “I like studying artificial intelligence”
在这个过程中，模型需要理解每个词的含义，并找到它们之间的对应关系。传统的Seq2Seq模型依赖RNN来编码输入序列，但效果有限。而Transformer通过自注意力机制，能够同时关注输入序列中的所有位置，大大提升了翻译质量。
Transformer的基本输入输出 输入的秘密 Transformer的输入其实很简单，就是一个个token（可以是单词、子词或者字符）。假设我们的词汇表大小为[vocab_size, embedding_dim]，输入句子的token长度为seq_len，每个token会被转换成一个vector向量，维度为[embedding_dim]，则整个输入句子的shape为[ seq_len, embedding_dim]。
但是，为了让模型理解序列的顺序，还需要加入位置编码(Positional Encoding)。这就是著名的公式：
$$ PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
$$ PE_{(pos,2i&#43;1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
pos是位置索引，即token在序列中的位置，从0开始； i是维度索引，从0开始到$d_{model}-1$结束，表示词嵌入的维度； $d_{model}$是词嵌入（word embedding）的维度embedding_dim 观察输入句子的shape: [seq_len, embedding_dim]，位置编码的时候，pos对应的是token在seq_len维度的位置，而维度索引i对应词嵌入embedding_dim的维度。
输出的魅力 Transformer的输出同样是一系列token，但它还有一个特殊之处——它是一个概率分布。对于每个时间步，模型会输出词汇表中每个词的概率，然后通过贪婪搜索或束搜索来选择最合适的词。这些搜索算法构成了推理时扩展算法的基础。
补充贪婪搜索和束搜索的算法：
贪婪搜索是一种简单直接的解码策略。在每个时间步，它总是选择具有最高概率的token作为下一个输出，不考虑后续可能的影响。优点是计算简单，速度快，内存占用少；缺点则是容易陷入局部最优，生成结果缺乏多样性。 束搜索是对贪婪搜索的改进，在每个时间步保留beam_width个最有希望的候选序列，而不是只保留一个。相比贪婪搜索能找到更优的序列，在合理的时间内探索多个可能性；缺点则是计算复杂度较高，需要调整beam_width参数，仍可能错过全局最优解。 核心模块：Encoder和Decoder Transformer的核心由两大部分组成：Encoder和Decoder。
Encoder：信息的加工厂 Encoder负责将输入序列转换成一系列隐藏表示。它由N个相同的层堆叠而成，每一层都包含两个子层：
多头自注意力机制(Multi-Head Self-Attention) 位置全连接前馈网络(Position-wise Feed-Forward Networks) 每层都有残差连接和层归一化，这使得深层网络也能稳定训练。
Decoder：创作的艺术家 Decoder则负责根据Encoder的输出和之前生成的token来预测下一个token。它的结构比Encoder稍复杂一些，包含三个子层：
掩码多头自注意力机制(Masked Multi-Head Self-Attention) 多头注意力机制(Multi-Head Attention) 位置全连接前馈网络(Position-wise Feed-Forward Networks) 其中掩码机制确保在生成当前位置的输出时，只能看到之前的位置，不能&#34;偷看&#34;未来的信息。
基本组件详解 现在让我们深入了解一下Transformer内部的各个组件：
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 一切的开始 - Transformer架构" href="http://localhost:1313/llm/architecture/transformer/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">综述 - Transformer架构的演进
    </h2>
  </header>
  <div class="entry-content">
    <p>Transformer架构演进：从自注意力到长序列与多模态的革命性突破 Transformer自2017年提出以来，已成为深度学习领域的基石架构，其演进历程反映了AI研究者对计算效率、长序列处理能力和跨模态应用的不懈追求。从最初的O(n²)计算复杂度到如今的O(n)线性复杂度，从单模态文本处理到原生多模态架构，Transformer的演进不仅解决了自身局限，更推动了大语言模型、视觉模型和多模态系统的革命性发展。这些演进主要体现在注意力机制创新、位置编码改进、模型架构变体与扩展、训练推理效率优化四大方向，共同构建了现代AI系统的计算基础。
一、注意力机制的演进：从全局到稀疏再到状态空间模型 Transformer原始架构的核心是自注意力机制，它通过计算查询(Q)、键(K)和值(V)之间的相似度来捕捉序列内部的长距离依赖关系 。标准自注意力的计算公式为：
$$ \text{Attention}(Q,K,V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V $$
其中，$Q∈ℝ^{n×d_k}$，$K∈ℝ^{n×d_k}$，$V∈ℝ^{n×d_v}$，d_k是键向量的维度。这一机制虽然强大，但计算复杂度为O(n²)，随着序列长度n的增加呈平方增长，严重限制了处理长文本的能力。
为解决这一瓶颈，研究者从多个角度对注意力机制进行了创新：
1. 稀疏注意力机制
稀疏注意力通过限制每个token只能关注序列中的局部区域，将计算复杂度从O(n²)降至O(n)。主要代表包括：
Longformer：采用滑动窗口注意力机制，每个token仅关注其周围固定窗口内的token，同时保留少数全局token以维持上下文连贯性 。其窗口注意力公式为： $$ \text{WindowAttention}(Q,K,V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V_{\text{window}} $$
其中，V_window是窗口内值向量的子集，窗口大小通常为512或1024个token 。
BigBird：结合三种注意力模式——随机注意力（通过Erdős-Rényi图采样）、滑动窗口注意力和全局注意力 。随机注意力使模型能够捕捉长距离依赖，同时保持O(n)的线性复杂度。其随机注意力矩阵可以表示为： $$ \text{RandomAttention}(Q,K,V) = \text{softmax}\left( \frac{QW_{\text{random}}K^\top}{\sqrt{d_k}} \right)V_{\text{random}} $$
其中，$W_{random}$是随机掩码矩阵，$V_{random}$是随机采样值向量的子集 。
局部增强Mamba（LEVM）：在Mamba架构中，通过动态选择性处理，对重要信息保留完整状态，对不重要信息进行压缩，实现O(n)计算复杂度和O(1)显存占用 。其核心状态方程为： $$ h_t = \overline{A}h_{t-1} &#43; \overline{B}x_t $$
其中，$\overline{A}$和$\overline{B}$是动态生成的参数矩阵，允许模型根据输入选择性地保留信息 。
2. 线性注意力机制
线性注意力通过数学近似技术，将注意力计算的复杂度降至O(n)，同时保持模型的表达能力。主要代表包括：
Performer：采用核函数近似方法，将标准注意力公式中的softmax(QKᵀ)替换为核函数K(Q,K)，使得计算可以分解为O(n)的线性操作 。其核心公式为： $$ \text{LinearAttention}(Q,K,V) = \text{核函数}(Q,K)V $$
其中，核函数可以是高斯核、多项式核或其他类型的核函数 。
Mamba：将注意力机制替换为结构化状态空间模型（SSM），通过递归计算实现长序列处理 。Mamba在S4的基础上改进，引入选择性处理机制，允许模型根据输入动态生成参数$\overline{A}$, $\overline{B}$, Δ，从而实现对信息的选择性保留和压缩 。其状态方程为： $$ h_t = \overline{A}h_{t-1} &#43; \overline{B}x_t,\quad y_t = Ch_t $$
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-12-19 00:00:00 +0000 UTC'>December 19, 2025</span>&nbsp;·&nbsp;<span>fandengdong</span></footer>
  <a class="entry-link" aria-label="post link to 综述 - Transformer架构的演进" href="http://localhost:1313/llm/architecture/transformer%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="http://localhost:1313/page/2/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">My work notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
