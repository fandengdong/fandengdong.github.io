---
title: "MLAï¼šMulti-head Latent Attention"
date: 2026-01-07
math: true
---


## ä¸€ã€MLA çš„ç”±æ¥

### èƒŒæ™¯ï¼šKV Cache æˆä¸ºæ¨ç†ç“¶é¢ˆ
åœ¨ Transformer è§£ç å™¨ä¸­ï¼Œè‡ªå›å½’ç”Ÿæˆæ—¶éœ€ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Valueï¼ˆå³ KV Cacheï¼‰ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¦‚ 32Kã€128K tokensï¼‰ï¼ŒKV Cache å ç”¨å¤§é‡æ˜¾å­˜å’Œå¸¦å®½ï¼Œæˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚

### ç›®æ ‡ï¼šå‹ç¼© KV Cache
ä¸ºå‡å°‘ KV Cache å¤§å°ï¼Œç ”ç©¶è€…æå‡ºå¤šç§å‹ç¼©æ–¹æ³•ï¼š
- **MQAï¼ˆMulti-Query Attentionï¼‰**ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K/Vã€‚
- **GQAï¼ˆGrouped-Query Attentionï¼‰**ï¼šå°†å¤šä¸ªå¤´åˆ†ç»„å…±äº« K/Vã€‚
- **MLAï¼ˆMulti-head Latent Attentionï¼‰**ï¼š**ä¸ç›´æ¥å­˜å‚¨åŸå§‹ K/Vï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç»´â€œæ½œåœ¨è¡¨ç¤ºâ€ï¼ˆlatent codeï¼‰ï¼Œé€šè¿‡å°å‹ç½‘ç»œåŠ¨æ€é‡å»ºè¿‘ä¼¼çš„ K/V**ã€‚

### æå‡ºè€…ä¸å‡ºå¤„
- **DeepSeek-V2ï¼ˆ2024ï¼‰** é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºå¹¶å‘½å **MLAï¼ˆMulti-head Latent Attentionï¼‰**ã€‚
- æ ¸å¿ƒæ€æƒ³ï¼šç”¨ **ä½ç§©æ½œåœ¨å‘é‡ + å°å‹æŠ•å½±ç½‘ç»œ** ä»£æ›¿ä¼ ç»Ÿ KV Cacheï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆè®ºæ–‡ç§°å‡å°‘ 77% KV Cacheï¼‰ã€‚

<img src="MLA.jpg" alt="MLA" style="width:80%;height:auto;">

---

## äºŒã€åŸºæœ¬åŸç†

MLA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> **ä¸æ˜¾å¼å­˜å‚¨æ¯ä¸ª token çš„ K å’Œ Vï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ªç´§å‡‘çš„â€œæ½œåœ¨å‘é‡â€ $ z_t \in \mathbb{R}^{d_z} $ï¼ˆ$ d_z \ll d_k, d_v $ï¼‰ï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡è½»é‡çº§å¯å­¦ä¹ æ˜ å°„ $ f_K, f_V $ åŠ¨æ€é‡å»º K å’Œ Vã€‚**

å…·ä½“æµç¨‹ï¼š
1. å¯¹æ¯ä¸ªè¾“å…¥ token $ x_t $ï¼Œå…ˆè®¡ç®—ä¸€ä¸ª **å…±äº«çš„æ½œåœ¨è¡¨ç¤º $ z_t $**ã€‚
2. åœ¨ attention è®¡ç®—æ—¶ï¼Œå¯¹æ¯ä¸ª head $ i $ï¼Œç”¨ä¸¤ä¸ªå°å‹ MLPï¼ˆæˆ–çº¿æ€§å±‚ï¼‰å°† $ z_t $ æ˜ å°„ä¸ºè¯¥ head çš„ $ k_{t,i} $ å’Œ $ v_{t,i} $ã€‚
3. ç¼“å­˜çš„æ˜¯ $ z_t $ è€Œé $ k_t, v_t $ï¼Œæ˜¾è‘—èŠ‚çœå†…å­˜ã€‚

ç”±äº $ z_t $ ç»´åº¦è¿œå°äºåŸå§‹ K/Vï¼ˆä¾‹å¦‚ $ d_z = 128 $ï¼Œè€Œ $ d_k = d_v = 128 \times 8 = 1024 $ï¼‰ï¼Œä¸”é‡å»ºç½‘ç»œå‚æ•°é‡å°ï¼Œæ•´ä½“æ•ˆç‡æ›´é«˜ã€‚

---

## ä¸‰ã€è¯¦ç»†æ•°å­¦ç»†èŠ‚

è®¾ï¼š
- è¾“å…¥ token è¡¨ç¤ºï¼š$ x_t \in \mathbb{R}^{d_{\text{model}}} $
- æ½œåœ¨ç»´åº¦ï¼š$ d_z $
- æ³¨æ„åŠ›å¤´æ•°ï¼š$ H $
- æ¯ä¸ª head çš„ key/value ç»´åº¦ï¼š$ d_k = d_v = d_h $

### æ­¥éª¤ 1ï¼šç”Ÿæˆæ½œåœ¨å‘é‡
é€šè¿‡ä¸€ä¸ªå…±äº«çº¿æ€§å±‚ç”Ÿæˆæ½œåœ¨å‘é‡ï¼š
$$
z_t = W_z x_t + b_z \quad \in \mathbb{R}^{d_z}
$$
å…¶ä¸­ $ W_z \in \mathbb{R}^{d_z \times d_{\text{model}}} $

> å®é™…å®ç°ä¸­ï¼Œ$ z_t $ å¯èƒ½ç»è¿‡ LayerNorm æˆ–å…¶ä»–å½’ä¸€åŒ–ã€‚

### æ­¥éª¤ 2ï¼šæŒ‰å¤´é‡å»º K å’Œ V
å¯¹æ¯ä¸ª head $ h \in \{1, ..., H\} $ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„é‡å»ºçŸ©é˜µï¼š
$
k_{t,h} = W_{K,h} z_t \quad \in \mathbb{R}^{d_h} \\
v_{t,h} = W_{V,h} z_t \quad \in \mathbb{R}^{d_h}
$
å…¶ä¸­ï¼š
- $ W_{K,h} \in \mathbb{R}^{d_h \times d_z} $
- $ W_{V,h} \in \mathbb{R}^{d_h \times d_z} $

ä¸ºå‡å°‘å‚æ•°é‡ï¼Œé€šå¸¸å°†æ‰€æœ‰å¤´çš„æƒé‡å †å ï¼š
$
W_K \in \mathbb{R}^{H \cdot d_h \times d_z}, \quad
W_V \in \mathbb{R}^{H \cdot d_h \times d_z}
$

åˆ™å¯æ‰¹é‡è®¡ç®—ï¼š
$
K = (W_K z_t) \in \mathbb{R}^{H d_h} \Rightarrow \text{reshape to } (H, d_h) \\
V = (W_V z_t) \in \mathbb{R}^{H d_h} \Rightarrow \text{reshape to } (H, d_h)
$

### æ­¥éª¤ 3ï¼šæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
æŸ¥è¯¢ä»ç”±ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆï¼ˆå›  Q ä¸éœ€ç¼“å­˜ï¼‰ï¼š
$
Q = x_t W_Q \in \mathbb{R}^{H \times d_h}
$

ç„¶åè¿›è¡Œæ ‡å‡† scaled dot-product attentionï¼š
$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_h}} \right) V
$

### ç¼“å­˜ç­–ç•¥
- **ç¼“å­˜å†…å®¹**ï¼šä»…ç¼“å­˜ $ \{z_1, z_2, ..., z_t\} $ï¼Œè€Œé $ K, V $ã€‚
- **ç¼“å­˜å¤§å°**ï¼šä» $ O(T \cdot H \cdot d_h) $ é™è‡³ $ O(T \cdot d_z) $ã€‚
- è‹¥ $ d_z \ll H \cdot d_h $ï¼ˆä¾‹å¦‚ $ d_z = 128 $, $ H=32, d_h=128 \Rightarrow H d_h = 4096 $ï¼‰ï¼Œåˆ™å‹ç¼©æ¯”è¾¾ **32x**ã€‚

---

## å››ã€PyTorch å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä»¥ä¸‹æ˜¯ä¸€ä¸ª **å•å±‚ MLA æ³¨æ„åŠ›æ¨¡å—** çš„ PyTorch å®ç°ï¼Œç”¨äºæ¼”ç¤ºåŸç†ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MLAttention(nn.Module):
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_head: int,
        d_latent: int,
        dropout: float = 0.1
    ):
        """
        Multi-head Latent Attention (MLA) as in DeepSeek-V2.
        Args:
            d_model: model dimension (e.g., 4096)
            n_heads: number of attention heads
            d_head: dimension per head (e.g., 128)
            d_latent: latent dimension for KV compression (e.g., 128)
        """
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_latent = d_latent
        self.scale = d_head ** -0.5

        # Query projection (not compressed)
        self.wq = nn.Linear(d_model, n_heads * d_head, bias=False)

        # Latent vector projection (shared for all heads)
        self.wz = nn.Linear(d_model, d_latent, bias=False)

        # Reconstruction matrices for K and V (per head, but stored as big matrices)
        self.wk_recon = nn.Parameter(torch.randn(n_heads * d_head, d_latent))
        self.wv_recon = nn.Parameter(torch.randn(n_heads * d_head, d_latent))

        # Output projection
        self.wo = nn.Linear(n_heads * d_head, d_model, bias=False)

        self.dropout = nn.Dropout(dropout)

        # Initialize reconstruction matrices (optional: Xavier)
        nn.init.xavier_uniform_(self.wk_recon)
        nn.init.xavier_uniform_(self.wv_recon)

    def forward(
        self,
        x: torch.Tensor,
        kv_cache: dict = None  # { 'z': [seq_len, d_latent] }
    ):
        """
        x: [batch, seq_len, d_model]
        kv_cache: optional dict to store/reuse latent vectors
        Returns:
            output: [batch, seq_len, d_model]
            new_kv_cache: updated cache with latent vectors
        """
        B, L, D = x.shape

        # Project queries
        q = self.wq(x)  # [B, L, H * dh]
        q = q.view(B, L, self.n_heads, self.d_head).transpose(1, 2)  # [B, H, L, dh]

        # Project to latent space
        z = self.wz(x)  # [B, L, d_latent]

        # Update KV cache
        if kv_cache is not None:
            if 'z' in kv_cache:
                z = torch.cat([kv_cache['z'], z], dim=1)  # prepend cached latents
            kv_cache['z'] = z.detach()  # update cache

        # Reconstruct K and V from latent vectors
        # z: [B, T, d_latent] where T = current total length
        T = z.size(1)
        # Reconstruct all K and V at once
        k_flat = F.linear(z, self.wk_recon)  # [B, T, H * dh]
        v_flat = F.linear(z, self.wv_recon)  # [B, T, H * dh]

        k = k_flat.view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # [B, H, T, dh]
        v = v_flat.view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # [B, H, T, dh]

        # Scaled dot-product attention
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, L, T]
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        output = torch.matmul(attn_weights, v)  # [B, H, L, dh]
        output = output.transpose(1, 2).contiguous().view(B, L, -1)  # [B, L, H*dh]

        output = self.wo(output)
        return output, kv_cache

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    batch_size = 2
    seq_len = 8
    d_model = 512
    n_heads = 8
    d_head = 64
    d_latent = 128  # << n_heads * d_head = 512

    x = torch.randn(batch_size, seq_len, d_model)
    mla = MLAttention(d_model, n_heads, d_head, d_latent)

    # First forward (no cache)
    out1, cache = mla(x, kv_cache={})
    print("Output shape:", out1.shape)      # [2, 8, 512]
    print("Cache z shape:", cache['z'].shape)  # [2, 8, 128]

    # Next token (simulate autoregressive)
    next_token = torch.randn(batch_size, 1, d_model)
    out2, cache = mla(next_token, kv_cache=cache)
    print("After one more token, cache z shape:", cache['z'].shape)  # [2, 9, 128]
```

---

## äº”ã€ä¼˜åŠ¿ä¸å±€é™

### âœ… ä¼˜åŠ¿
- **KV Cache å‹ç¼©ç‡é«˜**ï¼šç¼“å­˜ä» $ O(H d_h) $ é™è‡³ $ O(d_z) $ï¼Œå…¸å‹å‹ç¼©æ¯” 4xâ€“32xã€‚
- **æ¨ç†æ˜¾å­˜å¤§å¹…é™ä½**ï¼šé€‚åˆé•¿ä¸Šä¸‹æ–‡éƒ¨ç½²ã€‚
- **è®­ç»ƒå…¼å®¹æ€§å¥½**ï¼šç«¯åˆ°ç«¯å¯è®­ç»ƒï¼Œæ— éœ€ä¿®æ”¹æŸå¤±å‡½æ•°ã€‚

### âš ï¸ å±€é™
- **é‡å»ºè¯¯å·®**ï¼šK/V æ˜¯è¿‘ä¼¼é‡å»ºï¼Œå¯èƒ½å½±å“æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼ˆä½† DeepSeek-V2 è¡¨æ˜å½±å“å¯æ§ï¼‰ã€‚
- **é¢å¤–è®¡ç®—å¼€é”€**ï¼šæ¯æ¬¡ attention éƒ½éœ€é‡å»º K/Vï¼Œå¢åŠ  FLOPsï¼ˆä½†å†…å­˜å¸¦å®½å¾€å¾€æ˜¯ç“¶é¢ˆï¼Œè®¡ç®—æ¢å†…å­˜å€¼å¾—ï¼‰ã€‚
- **éœ€è°ƒæ•´è®­ç»ƒç­–ç•¥**ï¼šæ½œåœ¨ç©ºé—´éœ€å……åˆ†å­¦ä¹ ï¼Œå¯èƒ½éœ€æ›´å¤§å­¦ä¹ ç‡æˆ– warmupã€‚

---

## å…­ã€æ€»ç»“

**MLAï¼ˆMulti-head Latent Attentionï¼‰** æ˜¯ä¸€ç§é¢å‘é«˜æ•ˆæ¨ç†çš„æ³¨æ„åŠ›æœºåˆ¶åˆ›æ–°ï¼Œé€šè¿‡å¼•å…¥**ä½ç»´æ½œåœ¨è¡¨ç¤º + åŠ¨æ€é‡å»º**ï¼Œæ˜¾è‘—å‹ç¼© KV Cacheï¼Œå·²åœ¨ DeepSeek-V2 ç­‰å·¥ä¸šçº§å¤§æ¨¡å‹ä¸­éªŒè¯æœ‰æ•ˆæ€§ã€‚å®ƒä»£è¡¨äº†â€œ**ç”¨è®¡ç®—æ¢å†…å­˜**â€ çš„ç°ä»£å¤§æ¨¡å‹ä¼˜åŒ–èŒƒå¼ã€‚

> ğŸ” æ³¨ï¼šè‹¥ä½ åœ¨å…¶ä»–æ–‡çŒ®ä¸­çœ‹åˆ° â€œMLAâ€ æŒ‡ä»£ä¸åŒæ¦‚å¿µï¼ˆå¦‚ Multi-Layer Attentionã€Meta-Learning Attention ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­ã€‚ä½†åœ¨ 2024â€“2026 å¹´å¤§æ¨¡å‹æ•ˆç‡ä¼˜åŒ–è¯­å¢ƒä¸‹ï¼Œ**MLA â‰ˆ Multi-head Latent Attentionï¼ˆDeepSeek-V2ï¼‰**ã€‚

---

### å‚è€ƒèµ„æ–™

- DeepSeek-V2 Technical Report (2024). https://github.com/deepseek-ai/DeepSeek-V2
- "Efficient Transformers: A Survey" (2022+)
- Googleâ€™s **AFT**ã€Metaâ€™s **MQA/GQA** ç­‰ç›¸å…³å·¥ä½œ
