---
title: "KV cache - å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†çš„åŸºçŸ³"
date: 2025-01-13
math: true
---


KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰æ¨ç†è¿‡ç¨‹ä¸­ç”¨äº**åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆ**çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒé€šè¿‡ç¼“å­˜å…ˆå‰ token çš„ Key å’Œ Value å‘é‡ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—å·²å¤„ç†ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚


## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ

### 1. è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨
å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥**è‡ªå›å½’æ–¹å¼**ç”Ÿæˆæ–‡æœ¬ï¼šæ¯æ¬¡åªé¢„æµ‹ä¸€ä¸ª tokenï¼Œç„¶åå°†è¯¥ token æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼š

```
è¾“å…¥: "ä»Šå¤©å¤©æ°”"
ç¬¬1æ­¥è¾“å‡º: "çœŸ"
è¾“å…¥å˜ä¸º: "ä»Šå¤©å¤©æ°”çœŸ"
ç¬¬2æ­¥è¾“å‡º: "å¥½"
...
```

### 2. æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜
Transformer ä½¿ç”¨ **è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼Œå¯¹é•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½è¦ä¸å…¶ä»–æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚

å‡è®¾å½“å‰å·²ç”Ÿæˆ $t$ ä¸ª tokenï¼Œç°åœ¨è¦ç”Ÿæˆç¬¬ $t+1$ ä¸ª tokenã€‚è‹¥æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªé•¿åº¦ä¸º $t+1$ çš„åºåˆ—çš„ Qã€Kã€Vï¼Œé‚£ä¹ˆï¼š
- ç¬¬1æ­¥ï¼šè®¡ç®—1ä¸ªtoken â†’ 1æ¬¡QKV
- ç¬¬2æ­¥ï¼šè®¡ç®—2ä¸ªtoken â†’ 2æ¬¡QKVï¼ˆä½†å‰1ä¸ªå…¶å®å·²ç»ç®—è¿‡ï¼‰
- ...
- ç¬¬$t$æ­¥ï¼šè®¡ç®—$t$ä¸ªtoken â†’ å‰$t-1$ä¸ªé‡å¤è®¡ç®—ï¼

è¿™å¯¼è‡´ **æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$**ï¼Œä¸”å¤§é‡é‡å¤è®¡ç®—ã€‚

### 3. KV Cache çš„æå‡º
ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºï¼š**åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“å­˜æ¯ä¸ª token å¯¹åº”çš„ Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰å‘é‡**ã€‚å› ä¸ºï¼š
- åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œ**å†å² token ä¸ä¼šæ”¹å˜**ï¼›
- Attention å…¬å¼ä¸­ï¼Œå½“å‰ token çš„ Q åªéœ€ä¸æ‰€æœ‰å†å² Kã€V è®¡ç®—å³å¯ï¼›
- Q æ˜¯å½“å‰ token çš„è¡¨ç¤ºï¼Œå¿…é¡»å®æ—¶è®¡ç®—ï¼›ä½† Kã€V å¯ä»¥æå‰ç¼“å­˜ã€‚

äºæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥åªéœ€ï¼š
- è®¡ç®—å½“å‰ token çš„ Qï¼›
- å°†å†å²ç¼“å­˜çš„ Kã€V ä¸å½“å‰ Kã€V æ‹¼æ¥ï¼›
- æ‰§è¡Œä¸€æ¬¡ attentionã€‚

è¿™æ ·ï¼Œæ¯æ­¥è®¡ç®—é‡æ’å®šï¼ˆ$O(1)$ per stepï¼‰ï¼Œæ€»å¤æ‚åº¦ä» $O(n^2)$ é™ä¸º $O(n)$ã€‚

> âœ… **KV Cache çš„æ ¸å¿ƒæ€æƒ³ï¼šç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œé¿å…é‡å¤è®¡ç®— Kã€Vã€‚**

---

## äºŒã€æŠ€æœ¯åŸç†è¯¦è§£

### 1. æ ‡å‡† Self-Attention å…¬å¼å›é¡¾

å¯¹äºè¾“å…¥åºåˆ— $X \in \mathbb{R}^{n \times d}$ï¼š

$$
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
$$
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

### 2. è‡ªå›å½’ç”Ÿæˆä¸­çš„ KV Cache åº”ç”¨

è®¾å½“å‰å·²å¤„ç† token æ•°ä¸º $t$ï¼Œç¼“å­˜äº†ï¼š
- $K_{\text{cache}} \in \mathbb{R}^{t \times d_k}$
- $V_{\text{cache}} \in \mathbb{R}^{t \times d_v}$

å½“è¾“å…¥æ–° token $x_{t+1}$ï¼ˆæˆ–åˆå§‹ prompt çš„ä¸‹ä¸€ä¸ª tokenï¼‰ï¼š
1. è®¡ç®—å…¶ Qã€Kã€Vï¼š
   $$
   q_{t+1} = x_{t+1} W_Q,\quad k_{t+1} = x_{t+1} W_K,\quad v_{t+1} = x_{t+1} W_V
   $$
2. æ›´æ–°ç¼“å­˜ï¼š
   $$
   K_{\text{new}} = [K_{\text{cache}}; k_{t+1}],\quad V_{\text{new}} = [V_{\text{cache}}; v_{t+1}]
   $$
3. è®¡ç®— attentionï¼š
   $$
   \text{attn} = \text{softmax}\left(\frac{q_{t+1} K_{\text{new}}^T}{\sqrt{d_k}}\right) V_{\text{new}}
   $$

> æ³¨æ„ï¼šQ åªéœ€å½“å‰ token çš„ï¼ˆå› ä¸ºæ˜¯ decoder-only æ¶æ„ï¼Œå¦‚ GPTï¼‰ï¼Œè€Œ Kã€V éœ€è¦å…¨éƒ¨å†å²ã€‚

> æ³¨æ„ï¼šåœ¨è‡ªå›å½’çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç½‘ç»œä¸­æµé€šçš„tensorçš„shapeä¸ºï¼š[batch_size, 1, hidden_size]ï¼Œè€Œä¸æ˜¯è®­ç»ƒå½“ä¸­çš„[batch_size, seq_len, hidden_size]ã€‚


### 3. å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ KV Cache

æ¯ä¸ª attention head éƒ½æœ‰è‡ªå·±çš„ $W_K, W_V$ï¼Œå› æ­¤ KV Cache é€šå¸¸æ˜¯ shape: $(batch\_size, num\_heads, seq\_len, head\_dim)$ã€‚

åœ¨å®ç°ä¸­ï¼Œå¸¸æŒ‰ head ç»´åº¦ç»„ç»‡ç¼“å­˜ã€‚

### 4. å·¥ç¨‹ä¼˜åŒ–
- **å†…å­˜ç®¡ç†**ï¼šç¼“å­˜éšåºåˆ—å¢é•¿è€Œå¢é•¿ï¼Œéœ€æ³¨æ„æ˜¾å­˜é™åˆ¶ã€‚
- **PagedAttentionï¼ˆvLLMï¼‰**ï¼šå°† KV Cache åˆ†é¡µå­˜å‚¨ï¼Œæé«˜å†…å­˜åˆ©ç”¨ç‡ã€‚
- **é‡åŒ– KV Cache**ï¼šç”¨ int8/float16 å­˜å‚¨ Kã€Vï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚
- **æ»‘åŠ¨çª—å£æ³¨æ„åŠ›**ï¼šåªç¼“å­˜æœ€è¿‘ $N$ ä¸ª token çš„ KVï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡ã€‚

---

## ä¸‰ã€Python ä»£ç  Demoï¼ˆç®€åŒ–ç‰ˆï¼‰

ä¸‹é¢æ˜¯ä¸€ä¸ª **ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶** çš„çº¯ NumPy å®ç°ï¼Œæ¼”ç¤º KV Cache å¦‚ä½•å·¥ä½œã€‚

```python
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°
np.random.seed(42)

class SimpleKVCacheDemo:
    def __init__(self, d_model=64, d_k=32, d_v=32):
        self.d_model = d_model
        self.d_k = d_k
        self.d_v = d_v
        # éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
        self.W_Q = np.random.randn(d_model, d_k)
        self.W_K = np.random.randn(d_model, d_k)
        self.W_V = np.random.randn(d_model, d_v)
        self.W_O = np.random.randn(d_v, d_model)  # è¾“å‡ºæŠ•å½±ï¼ˆå¯é€‰ï¼‰

        # åˆå§‹åŒ– KV ç¼“å­˜
        self.K_cache = None  # shape: (seq_len, d_k)
        self.V_cache = None  # shape: (seq_len, d_v)

    def clear_cache(self):
        self.K_cache = None
        self.V_cache = None

    def forward_step(self, x):
        """
        x: np.array of shape (d_model,) â€” å½“å‰è¾“å…¥ token çš„ embedding
        è¿”å›è¾“å‡ºè¡¨ç¤ºï¼Œå¹¶æ›´æ–° KV cache

        æ­£å¸¸çš„å‰å‘è¾“å…¥æ˜¯[bs, seq_len, d_model],è¿™é‡Œè€ƒè™‘bs=1çš„æƒ…å†µï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¾“å…¥ä¸ºåˆšåˆšç”Ÿæˆçš„æœ€æ–°çš„tokenï¼Œæ‰€ä»¥è¯´å½“å‰è¾“å…¥çš„tokenç»´åº¦ä¸ºï¼š[1, d_model]
        """
        x = x.reshape(1, -1)  # (1, d_model)

        # è®¡ç®—å½“å‰ token çš„ Q, K, V
        Q = x @ self.W_Q  # (1, d_k)
        K = x @ self.W_K  # (1, d_k)
        V = x @ self.W_V  # (1, d_v)

        if self.K_cache is None:
            # ç¬¬ä¸€ä¸ª token
            self.K_cache = K
            self.V_cache = V
            attn_weights = np.array([[1.0]])  # softmax([0]) = [1]
        else:
            # æ‹¼æ¥ç¼“å­˜
            K_full = np.vstack([self.K_cache, K])  # (seq_len+1, d_k)
            V_full = np.vstack([self.V_cache, V])  # (seq_len+1, d_v)

            # è®¡ç®— attention scores: Q @ K_full^T
            scores = Q @ K_full.T / np.sqrt(self.d_k)  # (1, seq_len+1)
            attn_weights = np.exp(scores - np.max(scores))  # numerical stability
            attn_weights /= np.sum(attn_weights, axis=-1, keepdims=True)

            # æ›´æ–°ç¼“å­˜
            self.K_cache = K_full
            self.V_cache = V_full

        # åŠ æƒæ±‚å’Œ
        output = attn_weights @ V_full  # (1, d_v)
        output = output @ self.W_O       # (1, d_model)
        # åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªattentionçš„ç»´åº¦å°±æ˜¯(1, d_model)ï¼Œä¸åŒäºè®­ç»ƒä¸­ï¼Œç»´åº¦ä¸º(seq_len, d_model)
        return output.flatten()

# ------------------ Demo ------------------

# æ¨¡æ‹Ÿ token embeddingsï¼ˆæ¯”å¦‚æ¥è‡ª embedding layerï¼‰
embeddings = [
    np.random.randn(64),
    np.random.randn(64),
    np.random.randn(64),
    np.random.randn(64)
]

model = SimpleKVCacheDemo()

print("=== Without KV Cache (naive recompute) ===")
# è¿™é‡Œæˆ‘ä»¬ä¸å®ç°æ— ç¼“å­˜ç‰ˆæœ¬ï¼Œä½†é€»è¾‘ä¸Šæ¯æ­¥éƒ½è¦é‡ç®—å…¨éƒ¨

print("\n=== With KV Cache ===")
model.clear_cache()
for i, emb in enumerate(embeddings):
    out = model.forward_step(emb)
    print(f"Step {i+1}: output norm = {np.linalg.norm(out):.4f}, "
          f"cache length = {model.K_cache.shape[0]}")

# éªŒè¯ï¼šå¦‚æœé‡æ–°è¾“å…¥ç›¸åŒåºåˆ—ï¼Œç¼“å­˜ä¼šç´¯ç§¯
print("\nAdding one more token...")
out = model.forward_step(np.random.randn(64))
print(f"Step 5: cache length = {model.K_cache.shape[0]}")
```

### è¾“å‡ºç¤ºä¾‹ï¼š
```
=== With KV Cache ===
Step 1: output norm = 7.8921, cache length = 1
Step 2: output norm = 8.1023, cache length = 2
Step 3: output norm = 7.9542, cache length = 3
Step 4: output norm = 8.0124, cache length = 4

Adding one more token...
Step 5: cache length = 5
```

> ğŸ’¡ æ­¤ demo è™½ç®€åŒ–ï¼ˆå•å¤´ã€æ—  batchã€æ—  LayerNorm ç­‰ï¼‰ï¼Œä½†å®Œæ•´å±•ç¤ºäº† KV Cache çš„æ ¸å¿ƒæœºåˆ¶ï¼š**ç¼“å­˜ Kã€Vï¼Œé¿å…é‡å¤è®¡ç®—**ã€‚

---

## å››ã€å®é™…åº”ç”¨ä¸­çš„ KV Cacheï¼ˆè¡¥å……ï¼‰

åœ¨çœŸå® LLM æ¨ç†å¼•æ“ä¸­ï¼ˆå¦‚ HuggingFace Transformersã€vLLMã€TensorRT-LLMï¼‰ï¼š
- KV Cache æ˜¯é»˜è®¤å¯ç”¨çš„ï¼ˆ`past_key_values` å‚æ•°ï¼‰ï¼›
- æ”¯æŒ batch æ¨ç†ï¼ˆä¸åŒåºåˆ—é•¿åº¦éœ€ padding æˆ–ä½¿ç”¨ PagedAttentionï¼‰ï¼›
- å¯é€šè¿‡ `use_cache=True` æ§åˆ¶ï¼›
- æ˜¾å­˜å ç”¨ â‰ˆ $2 \times \text{num\_layers} \times \text{num\_heads} \times \text{seq\_len} \times \text{head\_dim} \times \text{bytes\_per\_param}$

ä¾‹å¦‚ HuggingFace ä¸­ä½¿ç”¨ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2", use_cache=True)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

inputs = tokenizer("Hello, how are", return_tensors="pt")
outputs = model(**inputs)  # ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—å…¨éƒ¨ KV
# outputs.past_key_values åŒ…å«å„å±‚çš„ (K, V) ç¼“å­˜

# ä¸‹ä¸€æ­¥ç”Ÿæˆï¼š
next_input = tokenizer(" you", return_tensors="pt").input_ids[:, -1:]
outputs2 = model(
    input_ids=next_input,
    past_key_values=outputs.past_key_values  # ä¼ å…¥ç¼“å­˜ï¼
)
```

---

## æ€»ç»“

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **åŠ¨æœº** | é¿å…è‡ªå›å½’ç”Ÿæˆä¸­é‡å¤è®¡ç®— Kã€V |
| **æ ¸å¿ƒ** | ç¼“å­˜å†å² token çš„ Key å’Œ Value |
| **ä¼˜åŠ¿** | æ¨ç†é€Ÿåº¦æå‡ï¼Œæ¯æ­¥ $O(1)$ è®¡ç®— |
| **ä»£ä»·** | é¢å¤–æ˜¾å­˜ï¼ˆä¸åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼‰ |
| **æ‰©å±•æŠ€æœ¯** | PagedAttentionã€KV é‡åŒ–ã€æ»‘åŠ¨çª—å£ |

KV Cache æ˜¯ç°ä»£ LLM é«˜æ•ˆæ¨ç†çš„åŸºçŸ³ä¹‹ä¸€ï¼Œç†è§£å®ƒå¯¹ä¼˜åŒ–éƒ¨ç½²ã€è®¾è®¡æ¨ç†å¼•æ“è‡³å…³é‡è¦ã€‚
