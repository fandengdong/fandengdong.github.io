---
title: "MOE - æ··åˆä¸“å®¶æ¶æ„"
date: 2025-12-24
math: true
---

æ··åˆä¸“å®¶ï¼ˆMixture of Experts, MoEï¼‰æ˜¯ä¸€ç§**æ¡ä»¶è®¡ç®—ï¼ˆconditional computationï¼‰**æ¶æ„ï¼Œæ—¨åœ¨åœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹æ‰©å±•æ¨¡å‹å®¹é‡ã€‚å®ƒå·²è¢«å¹¿æ³›åº”ç”¨äºç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Google çš„ GLaMã€Mixtralã€Qwen-MoEã€DeepSeek-MoE ç­‰ï¼‰ã€‚

## MoE çš„æ ¸å¿ƒæ€æƒ³

- ğŸ¯ ç›®æ ‡ï¼š
    - æ‰©å¤§æ¨¡å‹å‚æ•°é‡ï¼ˆæå‡è¡¨è¾¾èƒ½åŠ›ï¼‰
    - ä¿æŒæ¯æ¬¡å‰å‘è®¡ç®—çš„ FLOPs åŸºæœ¬ä¸å˜ï¼ˆé«˜æ•ˆæ¨ç†ï¼‰
- ğŸ”‘ å…³é”®æœºåˆ¶ï¼š
    - æ¨¡å‹ç”±å¤šä¸ªâ€œä¸“å®¶â€ï¼ˆExpertsï¼‰ç»„æˆï¼Œæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªå­ç½‘ç»œï¼ˆå¦‚ FFNï¼‰ã€‚
    - å¼•å…¥ä¸€ä¸ªé—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰ï¼Œæ ¹æ®è¾“å…¥åŠ¨æ€å†³å®šæ¿€æ´»å“ªå‡ ä¸ªä¸“å®¶ã€‚
    - é€šå¸¸åªæ¿€æ´» Top-K ä¸ªä¸“å®¶ï¼ˆå¦‚ K=1 æˆ– K=2ï¼‰ï¼Œå…¶ä½™ä¸“å®¶ä¸å‚ä¸è®¡ç®—ã€‚
> âœ… è¿™æ ·ï¼šæ€»å‚æ•°é‡ = æ‰€æœ‰ä¸“å®¶å‚æ•°ä¹‹å’Œï¼ˆå¾ˆå¤§ï¼‰ï¼Œä½†æ¯ token è®¡ç®—é‡ â‰ˆ K ä¸ªä¸“å®¶ï¼ˆå¾ˆå°ï¼‰ã€‚

## MoEç»™Transformeræ¶æ„å¸¦æ¥çš„æ”¹å˜

ä¸‹é¢æˆ‘ä»¬ä» Transformer æ¶æ„å‡ºå‘ï¼Œæ·±å…¥è®²è§£ MoEï¼ˆMixture of Expertsï¼‰çš„æ•°å­¦å½¢å¼ã€é—¨æ§æœºåˆ¶æ¥æºã€ä¸“å®¶æ¨¡å—åœ¨ Transformer ä¸­çš„å…·ä½“ä½ç½®ä¸ä½œç”¨

### MoE åœ¨ Transformer ä¸­çš„ä½ç½®

åœ¨æ ‡å‡† Transformer ä¸­ï¼Œæ¯ä¸ª Transformer å±‚åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒå­æ¨¡å—ï¼š

- å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attention, MHAï¼‰
- å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰
> âœ… MoE é€šå¸¸åªæ›¿æ¢ FFN éƒ¨åˆ†ï¼Œè€Œä¿ç•™ MHA ä¸å˜ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ä¸ª MoE Transformer å±‚çš„ç»“æ„ä¸ºï¼š

<img src="MOE_transformer.png" alt="MOE" style="width:60%;height:auto;">

**å› æ­¤ï¼ŒMoE æ˜¯ FFN çš„â€œè¶…é›†â€ï¼šä¼ ç»Ÿ FFN = 1 ä¸ªä¸“å®¶ï¼›MoE = å¤šä¸ªä¸“å®¶ + åŠ¨æ€è·¯ç”±ã€‚**

### ä¼ ç»Ÿ FFN çš„æ•°å­¦å½¢å¼ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰

æ ‡å‡† FFNï¼ˆä¸¤å±‚ MLPï¼‰ï¼š
$$
\text{FFN}(x) = W_2 \sigma(W_1 x + b_1) + b_2
$$

å…¶ä¸­ï¼š

- $x \in \mathbb{R}^d$ï¼šè¾“å…¥ token è¡¨ç¤º  
- $W_1 \in \mathbb{R}^{h \times d}, W_2 \in \mathbb{R}^{d \times h}$  
- $\sigma$ï¼šæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUã€SwiGLUï¼‰

æ‰€æœ‰ token éƒ½ç»è¿‡åŒä¸€ä¸ª FFNã€‚

### MoE çš„æ•°å­¦å½¢å¼

#### ä¸“å®¶é›†åˆå®šä¹‰

è®¾æœ‰ E ä¸ªä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ FFNï¼š
$$
\text{Expert}_i(x) = W_{i,2} \sigma(W_{i,1} x + b_{i,1}) + b_{i,2}
$$

> æ³¨æ„ï¼šé€šå¸¸çœç•¥åç½®é¡¹ä»¥ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚

æ‰€æœ‰ä¸“å®¶å‚æ•°ä¸å…±äº«ï¼Œæ€»å‚æ•°é‡ â‰ˆ EÃ—ï¼ˆå•ä¸ª FFN å‚æ•°é‡ï¼‰

#### é—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰

##### é—¨æ§æ‰“åˆ†ï¼ˆlogitsï¼‰ï¼š

$$
g_i = W_g x_i \in \mathbb{R}^E
$$

å…¶ä¸­ $W_g \in \mathbb{R}^{E \times d}$ æ˜¯å¯å­¦ä¹ çš„è·¯ç”±æƒé‡çŸ©é˜µã€‚

> ğŸ’¡ **é—¨æ§ä»å“ªé‡Œæ¥ï¼Ÿ**  
> å®ƒæ˜¯ä¸€ä¸ªé¢å¤–çš„å°å‹çº¿æ€§å±‚ï¼Œè¾“å…¥æ˜¯ token è¡¨ç¤º $x_i$ï¼Œè¾“å‡ºæ˜¯ $E$ ä¸ªä¸“å®¶çš„æ‰“åˆ†ã€‚  
> å®ƒæ˜¯ MoE æ¨¡å—çš„ä¸€éƒ¨åˆ†ï¼Œä¸ä¸“å®¶ä¸€èµ·è®­ç»ƒã€‚

å‡å®šé—¨æ§ç½‘ç»œçš„è¾“å…¥ä¸ºX: [bs, seq_len, hidden_dim]ï¼Œåˆ™é—¨æ§ç½‘ç»œçš„æƒé‡å‚æ•°ä¸ºW: [hidden_dim, E],å…¶ä¸­Eä¸º MoE çš„ä¸“å®¶æ•°é‡ï¼š

$$
\text{logits} = W_g Xï¼Œ å…¶ä¸­W_g \in \mathbb{R}^{E \times \text{hidden\_dim}}
$$

ç„¶åæ¥ä¸€ä¸ªsoftmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼š

$$
p = \text{softmax}(W_g X) \in \mathbb{R}^E
$$


##### Top-K é€‰æ‹©ï¼š

é€‰å–æ¦‚ç‡æœ€å¤§çš„ $K$ ä¸ªä¸“å®¶ï¼ˆé€šå¸¸ $K=1$ æˆ– $2$ï¼‰ï¼š

$$
\mathcal{K}_i = \text{TopK}(p_i, K)
$$

å¯¹åº”çš„æƒé‡ä¸º $p_i^{(k)}$ï¼ˆå½’ä¸€åŒ–æˆ–ä¿æŒåŸå€¼ï¼‰ã€‚

> âš ï¸ å®é™…å®ç°ä¸­ï¼Œä¸ºäº†å¯å¾®åˆ†ï¼Œå³ä½¿åªç”¨ Top-Kï¼Œæ¢¯åº¦ä»é€šè¿‡ Softmax å›ä¼ ï¼ˆä½†åªå¯¹é€‰ä¸­çš„ä¸“å®¶è®¡ç®—å‰å‘ï¼‰ã€‚

---

#### MoE è¾“å‡ºï¼ˆåŠ æƒç»„åˆï¼‰

æœ€ç»ˆè¾“å‡ºä¸ºé€‰ä¸­ä¸“å®¶çš„åŠ æƒå’Œï¼š

$$
y_i = \sum_{e \in \mathcal{K}_i} p_i^{(e)} \cdot \text{Expert}_e(x_i)
$$

> âœ… è¿™å°±æ˜¯ MoE çš„æ ¸å¿ƒï¼š**æ¯ä¸ª token èµ°ä¸åŒçš„â€œå­ç½‘ç»œâ€è·¯å¾„**ã€‚


### å®é™…æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

#### 1. ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡ï¼ˆExpert Collapseï¼‰

- æŸäº›ä¸“å®¶è¢«é¢‘ç¹ä½¿ç”¨ï¼Œå…¶ä»–â€œé—²ç½®â€
- å¯¼è‡´æ¨¡å‹å®¹é‡æµªè´¹ï¼Œç”šè‡³è®­ç»ƒä¸ç¨³å®š

âœ… **è§£å†³æ–¹æ¡ˆï¼šè¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆAuxiliary Lossï¼‰**

##### å®šä¹‰ï¼š

- $f_e = \dfrac{\text{åˆ†é…ç»™ä¸“å®¶ } e \text{ çš„ token æ•°}}{\text{æ€» token æ•°}}$ â†’ å®é™…ä½¿ç”¨é¢‘ç‡
- $P_e = \text{æ‰€æœ‰ token å¯¹ä¸“å®¶ } e \text{ çš„å¹³å‡ softmax æ¦‚ç‡}$

##### è¾…åŠ©æŸå¤±ï¼š

$$
\mathcal{L}_{\text{aux}} = E \cdot \sum_{e=1}^E f_e \cdot P_e
$$

- æœ€å°åŒ–è¯¥æŸå¤± â‡’ é¼“åŠ±é«˜æ¦‚ç‡ä¸“å®¶è¢«å®é™…ä½¿ç”¨ï¼Œä½é¢‘ä¸“å®¶æé«˜æ¦‚ç‡
- æ€»æŸå¤±ï¼š$\mathcal{L} = \mathcal{L}_{\text{task}} + \alpha \mathcal{L}_{\text{aux}}$ï¼ˆ$\alpha \approx 0.01$ï¼‰

### MOEåŸºæœ¬åŸç†çš„pytorchå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleExpert(nn.Module):
    """å•ä¸ªä¸“å®¶ï¼šä¸¤å±‚çº¿æ€§å˜æ¢ + ReLU"""
    def __init__(self, hidden_dim: int, ffn_dim: int):
        super().__init__()
        self.w1 = nn.Linear(hidden_dim, ffn_dim, bias=False)
        self.w2 = nn.Linear(ffn_dim, hidden_dim, bias=False)
        self.act = nn.ReLU()

    def forward(self, x):
        # x: (N, hidden_dim)
        return self.w2(self.act(self.w1(x)))  # (N, hidden_dim)

class SimpleMoE(nn.Module):
    def __init__(
        self,
        hidden_dim: int = 64,
        ffn_dim: int = 128,
        num_experts: int = 4,
        top_k: int = 1,
        aux_loss_coef: float = 0.01,
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_experts = num_experts
        self.top_k = top_k
        self.aux_loss_coef = aux_loss_coef

        # åˆ›å»ºå¤šä¸ªä¸“å®¶
        self.experts = nn.ModuleList([
            SimpleExpert(hidden_dim, ffn_dim) for _ in range(num_experts)
        ])
        # é—¨æ§ç½‘ç»œï¼šå°† token æ˜ å°„åˆ°ä¸“å®¶ logits
        self.gate = nn.Linear(hidden_dim, num_experts, bias=False)

    def forward(self, x: torch.Tensor):
        """
        x: (batch_size, seq_len, hidden_dim)
        Returns:
            output: (batch_size, seq_len, hidden_dim)
            aux_loss: scalar tensor
        """
        batch_size, seq_len, hidden_dim = x.shape
        N = batch_size * seq_len  # æ€» token æ•°

        # [bs, seq_len, hidden_dim] -> [N, hidden_dim]
        x_flat = x.view(N, hidden_dim)  # (N, hidden_dim)

        # === Step 1: é—¨æ§æ‰“åˆ† ===
        gate_logits = self.gate(x_flat)  # (N, E), E = num_experts
        gate_probs = F.softmax(gate_logits, dim=-1)  # (N, E)

        # === Step 2: Top-K é€‰æ‹©ï¼ˆè¿™é‡Œç”¨ Top-1ï¼‰===
        topk_probs, topk_indices = torch.topk(gate_probs, self.top_k, dim=-1)  # (N, K), (N, K)
        # æ„å»º one-hot é£æ ¼è·¯ç”±æ©ç 
        mask = torch.zeros_like(gate_probs).scatter_(-1, topk_indices, 1.0)  # (N, E)

        # === Step 3: è®¡ç®—è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤± ===
        # f_e: æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡ (E,)
        f = mask.float().mean(dim=0)  # (E,)
        # P_e: æ¯ä¸ªä¸“å®¶çš„å¹³å‡ softmax æ¦‚ç‡ (E,)
        P = gate_probs.mean(dim=0)   # (E,)
        aux_loss = torch.sum(f * P) * self.num_experts  # scalar

        # === Step 4: è·¯ç”±å¹¶è®¡ç®—è¾“å‡º ===
        output_flat = torch.zeros_like(x_flat)  # (N, hidden_dim)

        # å¯¹æ¯ä¸ªä¸“å®¶ï¼Œæ‰¾å‡ºåˆ†é…ç»™å®ƒçš„ token
        for e in range(self.num_experts):
            # idx: åˆ†é…ç»™ä¸“å®¶ e çš„ token ç´¢å¼• (M,)
            idx = torch.where(mask[:, e] > 0)[0]
            if idx.numel() == 0:
                continue
            expert_input = x_flat[idx]  # (M, hidden_dim)
            expert_output = self.experts[e](expert_input)  # (M, hidden_dim)
            # ä½¿ç”¨åŸå§‹ softmax æ¦‚ç‡ä½œä¸ºæƒé‡ï¼ˆä¸æ˜¯å½’ä¸€åŒ–åçš„ï¼‰
            weights = gate_probs[idx, e].unsqueeze(1)  # (M, 1)
            output_flat[idx] += weights * expert_output  # (M, hidden_dim)

        output = output_flat.view(batch_size, seq_len, hidden_dim)  # (bs, seq_len, hidden_dim)
        return output, self.aux_loss_coef * aux_loss


# ==============================
# æ¼”ç¤ºç”¨æ³•
# ==============================
if __name__ == "__main__":
    # è®¾ç½®å‚æ•°
    bs, seq_len, hidden_dim = 2, 5, 64
    x = torch.randn(bs, seq_len, hidden_dim)

    # åˆå§‹åŒ– MoE å±‚
    moe = SimpleMoE(
        hidden_dim=hidden_dim,
        ffn_dim=128,
        num_experts=4,
        top_k=1,
        aux_loss_coef=0.01
    )

    # å‰å‘ä¼ æ’­
    output, aux_loss = moe(x)

    print("Input shape:", x.shape)          # torch.Size([2, 5, 64])
    print("Output shape:", output.shape)    # torch.Size([2, 5, 64])
    print("Aux loss:", aux_loss.item())     # scalar, e.g., 0.04

    # æ¨¡æ‹Ÿä»»åŠ¡æŸå¤±ï¼ˆä¾‹å¦‚ MSEï¼‰
    target = torch.randn_like(output)
    task_loss = F.mse_loss(output, target)
    total_loss = task_loss + aux_loss

    print("Task loss:", task_loss.item())
    print("Total loss:", total_loss.item())

    # åå‘ä¼ æ’­ï¼ˆéªŒè¯æ¢¯åº¦æ˜¯å¦æ­£å¸¸ï¼‰
    total_loss.backward()
    print("Backward passed! Gradients computed.")
```
