---
title: "ALiBi - è§£å†³ä¼ ç»Ÿä½ç½®ç¼–ç å¤–æ¨é—®é¢˜"
date: 2026-01-04
math: true
---

## ä¸€ã€ALiBi çš„ç”±æ¥ï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

### 1. ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¤–æ¨é—®é¢˜

åœ¨æ ‡å‡† Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯é€šè¿‡ **ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦ã€å¯å­¦ä¹ ï¼‰æˆ–ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ RoPEï¼‰** æ³¨å…¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨ **è®­ç»ƒé•¿åº¦ < æ¨ç†é•¿åº¦** æ—¶è¡¨ç°ä¸ä½³ï¼š

- **ç»å¯¹ä½ç½®ç¼–ç **ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®ç´¢å¼•
- **RoPE**ï¼šè™½å¯é€šè¿‡æ’å€¼ï¼ˆå¦‚ YaRNï¼‰æ‰©å±•ï¼Œä½†å¤–æ¨èƒ½åŠ›ä»æœ‰é™ï¼Œä¸”éœ€é¢å¤–è°ƒå‚

> ğŸ’¡ é—®é¢˜æ ¸å¿ƒï¼š**ä½ç½®ç¼–ç ä¸åºåˆ—é•¿åº¦å¼ºè€¦åˆ**

### 2. ALiBi çš„æå‡º

- **è®ºæ–‡**ï¼šã€Š[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)ã€‹ï¼ˆICLR 2022ï¼‰
- **ä½œè€…**ï¼šOfir Press et al.ï¼ˆæ¥è‡ª AI21 Labsï¼‰
- **æ ¸å¿ƒæ€æƒ³**ï¼š**å®Œå…¨ç§»é™¤ä½ç½®ç¼–ç **ï¼Œæ”¹ç”¨ **ä¸è·ç¦»æˆçº¿æ€§å…³ç³»çš„åç½®ï¼ˆbiasï¼‰** ç›´æ¥åŠ åˆ° attention score ä¸Š

> âœ… ä¼˜åŠ¿ï¼š
> - æ¨¡å‹å¯åœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œåœ¨è¶…é•¿åºåˆ—ä¸Šç›´æ¥æ¨ç†ï¼ˆæ— éœ€å¾®è°ƒï¼‰
> - æ¶æ„æ›´ç®€æ´ï¼ˆæ— ä½ç½®åµŒå…¥å±‚ï¼‰
> - åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
> - è¢« **BLOOMï¼ˆ176Bï¼‰** ç­‰å¤§æ¨¡å‹é‡‡ç”¨

---

## äºŒã€åŸºæœ¬åŸç†

ALiBi çš„å…³é”®æ´å¯Ÿæ˜¯ï¼š

> **äººç±»è¯­è¨€ä¸­ï¼Œè¿‘æœŸ token é€šå¸¸æ¯”è¿œæœŸ token æ›´ç›¸å…³**ã€‚  
> å› æ­¤ï¼Œæ³¨æ„åŠ›åº”å¤©ç„¶å€¾å‘äº **å±€éƒ¨æ€§ï¼ˆlocalityï¼‰**ï¼Œä¸”è¡°å‡é€Ÿåº¦å¯ head-specific æ§åˆ¶ã€‚

ä¸ºæ­¤ï¼ŒALiBi åœ¨è®¡ç®— attention score æ—¶ï¼Œå¯¹æ¯ä¸ª head å¼•å…¥ä¸€ä¸ª **çº¿æ€§åç½®é¡¹**ï¼š

\[
\text{score}_{ij} = \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot |i - j|
\]

å…¶ä¸­ï¼š
- \(i, j\) æ˜¯ token ä½ç½®ï¼ˆ\(i\) ä¸º query ä½ç½®ï¼Œ\(j\) ä¸º key ä½ç½®ï¼‰
- \(m_h > 0\) æ˜¯ç¬¬ \(h\) ä¸ª head çš„ **è¡°å‡æ–œç‡ï¼ˆslopeï¼‰**
- **æ³¨æ„**ï¼šå³ä½¿ \(i < j\)ï¼ˆæœªæ¥ tokenï¼‰ï¼Œåç½®ä»ä¸ºè´Ÿï¼ˆä½†åœ¨ decoder-only ä¸­ä¼šè¢« causal mask å±è”½ï¼‰

> ğŸ”‘ å…³é”®ç‚¹ï¼š**ä¸ä¾èµ–ä»»ä½•ä½ç½®åµŒå…¥**ï¼Œä»…é è·ç¦» \(|i-j|\) å’Œå¯å­¦ä¹ ï¼ˆæˆ–é¢„è®¾ï¼‰çš„æ–œç‡æ§åˆ¶æ³¨æ„åŠ›èŒƒå›´ã€‚

---

## ä¸‰ã€æ•°å­¦ç»†èŠ‚

### 1. æ³¨æ„åŠ›è®¡ç®—ï¼ˆä»¥ decoder-only ä¸ºä¾‹ï¼‰

æ ‡å‡† causal attentionï¼š
\[
A_{ij} = 
\begin{cases}
\text{softmax}\left( \frac{Q_i K_j^\top}{\sqrt{d}} \right), & j \leq i \\
0, & j > i
\end{cases}
\]

ALiBi ä¿®æ”¹ä¸ºï¼š
\[
A_{ij} = 
\begin{cases}
\text{softmax}\left( \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot (i - j) \right), & j \leq i \\
0, & j > i
\end{cases}
\]

> ğŸ“Œ å› ä¸º \(j \leq i\)ï¼Œæ‰€ä»¥ \(|i - j| = i - j\)ï¼Œåç½®ä¸º \(-m_h (i - j)\)

### 2. æ–œç‡ \(m_h\) çš„è®¾ç½®

è®ºæ–‡å‘ç°ï¼š**ä¸åŒ head åº”å…³æ³¨ä¸åŒå°ºåº¦çš„ä¸Šä¸‹æ–‡**ï¼ˆæœ‰çš„çœ‹è¿‘ï¼Œæœ‰çš„çœ‹è¿œï¼‰ã€‚

å› æ­¤ï¼Œå°† heads åˆ†ç»„ï¼ŒæŒ‰æŒ‡æ•°è¡°å‡åˆ†é…æ–œç‡ï¼š

\[
m_h = 2^{-\frac{8h}{H}}, \quad h = 1, 2, ..., H
\]

ä¾‹å¦‚ï¼Œå½“ \(H = 8\)ï¼š
- head 0: \(m = 2^{-1} = 0.5\)
- head 1: \(m = 2^{-2} = 0.25\)
- ...
- head 7: \(m = 2^{-8} \approx 0.0039\)

> âœ… è¿™æ ·ï¼Œéƒ¨åˆ† head å…³æ³¨å±€éƒ¨ï¼ˆå¤§æ–œç‡ï¼‰ï¼Œéƒ¨åˆ† head å…³æ³¨å…¨å±€ï¼ˆå°æ–œç‡ï¼‰

---

## å››ã€PyTorch å®ç°ï¼ˆå¯è¿è¡Œæ¼”ç¤ºï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

def build_alibi_bias(num_heads: int, seq_len: int, dtype=torch.float32):
    """
    æ„å»º ALiBi åç½®çŸ©é˜µã€‚
    
    Args:
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        seq_len: åºåˆ—é•¿åº¦
        dtype: æ•°æ®ç±»å‹
    
    Returns:
        bias: (num_heads, seq_len, seq_len)
    """
    # é¢„å®šä¹‰æ–œç‡ m_h = 2^(-8h/H)
    slopes = torch.pow(2.0, -torch.arange(1, num_heads + 1, dtype=torch.float32) * 8.0 / num_heads)
    slopes = slopes.view(num_heads, 1, 1)  # (H, 1, 1)

    # æ„å»ºè·ç¦»çŸ©é˜µ: d[i, j] = i - j (ä»…å¯¹ j <= i æœ‰æ•ˆ)
    position_ids = torch.arange(seq_len, dtype=torch.float32)
    relative_position = position_ids[None, :] - position_ids[:, None]  # (L, L)
    relative_position = relative_position.abs().unsqueeze(0)  # (1, L, L)

    # ALiBi åç½® = -m_h * |i - j|
    alibi_bias = -slopes * relative_position  # (H, L, L)

    # å¯¹äº decoder-onlyï¼Œåº”ç”¨ causal maskï¼ˆä¸Šä¸‰è§’è®¾ä¸º -infï¼‰
    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    alibi_bias.masked_fill_(causal_mask, float('-inf'))

    return alibi_bias.to(dtype)

class ALiBiAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"

        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.dropout = dropout

    def forward(self, x):
        B, L, D = x.shape
        H = self.num_heads
        Dh = self.head_dim

        # æŠ•å½± Q, K, V
        q = self.q_proj(x).view(B, L, H, Dh).transpose(1, 2)  # (B, H, L, Dh)
        k = self.k_proj(x).view(B, L, H, Dh).transpose(1, 2)  # (B, H, L, Dh)
        v = self.v_proj(x).view(B, L, H, Dh).transpose(1, 2)  # (B, H, L, Dh)

        # Scaled dot-product
        scores = torch.matmul(q, k.transpose(-2, -1)) / (Dh ** 0.5)  # (B, H, L, L)

        # æ·»åŠ  ALiBi åç½®
        alibi_bias = build_alibi_bias(H, L, dtype=scores.dtype).to(scores.device)
        scores = scores + alibi_bias.unsqueeze(0)  # å¹¿æ’­ batch ç»´åº¦

        # Softmax + Dropout
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)

        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attn_weights, v)  # (B, H, L, Dh)
        output = output.transpose(1, 2).contiguous().view(B, L, D)
        output = self.out_proj(output)
        return output


# ------------------ æ¼”ç¤º ------------------
if __name__ == "__main__":
    torch.manual_seed(0)
    B, L, D = 2, 8, 128
    H = 8

    x = torch.randn(B, L, D)
    model = ALiBiAttention(embed_dim=D, num_heads=H)

    out = model(x)
    print("Input shape:", x.shape)       # [2, 8, 128]
    print("Output shape:", out.shape)     # [2, 8, 128]

    # å¯è§†åŒ– ALiBi åç½®ï¼ˆç¬¬ä¸€ä¸ª headï¼‰
    bias = build_alibi_bias(H, L)
    print("\nALiBi bias for head 0 (first 4x4):")
    print(bias[0, :4, :4])
```

### è¾“å‡ºç¤ºä¾‹ï¼ˆåç½®éƒ¨åˆ†ï¼‰ï¼š
```
ALiBi bias for head 0 (first 4x4):
tensor([[ 0.0000,    -inf,    -inf,    -inf],
        [-0.5000,  0.0000,    -inf,    -inf],
        [-1.0000, -0.5000,  0.0000,    -inf],
        [-1.5000, -1.0000, -0.5000,  0.0000]])
```

> ğŸ” å¯è§ï¼š
> - å¯¹è§’çº¿ä¸º 0ï¼ˆè‡ªå·±å¯¹é½è‡ªå·±ï¼‰
> - å·¦ä¸‹æ–¹ä¸ºè´Ÿå€¼ï¼Œä¸”éšè·ç¦»çº¿æ€§å‡å°
> - ä¸Šä¸‰è§’ä¸º `-inf`ï¼ˆcausal maskï¼‰

---

## äº”ã€ALiBi vs RoPEï¼šå…³é”®åŒºåˆ«

| ç‰¹æ€§ | RoPE | ALiBi |
|------|------|-------|
| æ˜¯å¦éœ€è¦ä½ç½®ç¼–ç  | âœ… æ˜¯ï¼ˆæ—‹è½¬çŸ©é˜µï¼‰ | âŒ å¦ |
| å¤–æ¨èƒ½åŠ› | ä¾èµ–æ’å€¼ï¼ˆå¦‚ YaRNï¼‰ | **å¤©ç„¶æ”¯æŒä»»æ„é•¿åº¦** |
| è®¡ç®—å¼€é”€ | éœ€è¦å¤æ•°ä¹˜æ³• | ä»…åŠ åç½®ï¼ˆæä½ï¼‰ |
| é€‚ç”¨åœºæ™¯ | ä¸»æµ LLMï¼ˆLLaMA, Qwenï¼‰ | BLOOMã€é•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹ |
| å¯¹ç§°æ€§ | æ”¯æŒåŒå‘ï¼ˆencoderï¼‰ | é€šå¸¸ç”¨äºå•å‘ï¼ˆdecoderï¼‰ |

---

## å…­ã€æ€»ç»“

- **ALiBi = æ— ä½ç½®ç¼–ç  + è·ç¦»çº¿æ€§åç½®**
- **å…¬å¼**ï¼š\(\text{score}_{ij} = \frac{Q_i K_j^\top}{\sqrt{d}} - m_h \cdot |i - j|\)
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒçŸ­ã€æµ‹è¯•é•¿ï¼›æ¶æ„ç®€æ´ï¼›æ¨ç†é«˜æ•ˆ
- **åº”ç”¨**ï¼šBLOOMï¼ˆ176B å‚æ•°ï¼‰ã€AI21 çš„ Jurassic æ¨¡å‹

> ğŸ’¡ å¦‚æœä½ æ­£åœ¨è®¾è®¡ä¸€ä¸ªéœ€è¦ **è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ>32K tokensï¼‰** çš„æ¨¡å‹ï¼ŒALiBi æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚

---
