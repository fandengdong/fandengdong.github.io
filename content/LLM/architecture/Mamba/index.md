---
title: "Mamba - A Transformer-like Architecture for Long Sequence Modeling"
date: 2025-01-13
math: true
---

## ä¸€ã€Mamba çš„è®¾è®¡æ€æƒ³

### èƒŒæ™¯ï¼šTransformer çš„å±€é™æ€§
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(L^2)$ï¼ˆL ä¸ºåºåˆ—é•¿åº¦ï¼‰ï¼Œå¯¹é•¿åºåˆ—ï¼ˆå¦‚åŸºå› ç»„ã€é«˜åˆ†è¾¨ç‡éŸ³é¢‘ã€é•¿æ–‡æœ¬ï¼‰æ•ˆç‡ä½ã€‚
- **çº¿æ€§æ³¨æ„åŠ›**ç­‰è¿‘ä¼¼æ–¹æ³•ç‰ºç‰²äº†å»ºæ¨¡èƒ½åŠ›ã€‚
- **RNN / SSMï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰** å…·æœ‰çº¿æ€§å¤æ‚åº¦ $O(L)$ï¼Œä½†ä¼ ç»Ÿ SSMï¼ˆå¦‚ Linear Time-Invariant SSM, LTI-SSMï¼‰æ˜¯**æ—¶ä¸å˜**çš„ï¼Œæ— æ³•åƒ Transformer é‚£æ ·æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€è°ƒæ•´è¡Œä¸ºã€‚

### Mamba çš„æ ¸å¿ƒåˆ›æ–°
> **Selective State Space Model (SSM)** â€”â€” å°† SSM ä¸è¾“å…¥ç›¸å…³ï¼ˆinput-dependentï¼‰ï¼Œä½¿å…¶å…·å¤‡**ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›**ï¼ŒåŒæ—¶ä¿æŒ**çº¿æ€§å¤æ‚åº¦**ã€‚

å…³é”®ç‚¹ï¼š
- **é€‰æ‹©æ€§ï¼ˆSelectivityï¼‰**ï¼šSSM çš„å‚æ•°ï¼ˆå¦‚ A, B, Cï¼‰ä¸å†æ˜¯å›ºå®šæˆ–ä»…æ—¶é—´ç›¸å…³çš„ï¼Œè€Œæ˜¯ç”±å½“å‰è¾“å…¥ token åŠ¨æ€ç”Ÿæˆã€‚
- **ç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡**ï¼šåˆ©ç”¨ç°ä»£ GPU çš„å¹¶è¡Œç‰¹æ€§ï¼Œé€šè¿‡â€œæ‰«æï¼ˆscanï¼‰+ å¹¶è¡Œå‰ç¼€â€å®ç°é«˜æ•ˆè®­ç»ƒã€‚
- **ç»“æ„ç®€å•**ï¼šæ²¡æœ‰ attentionï¼Œåªæœ‰ MLP + SSM blockã€‚

---

## äºŒã€Mamba æ¶æ„æ¦‚è§ˆ

Mamba Block æ›¿ä»£äº† Transformer ä¸­çš„ Attention + MLP å­å±‚ï¼š

```
Input â†’ LayerNorm â†’ SSM (Selective SSM) â†’ Residual Add â†’ LayerNorm â†’ MLP â†’ Residual Add â†’ Output
```

å…¶ä¸­æœ€æ ¸å¿ƒçš„æ˜¯ **Selective SSM æ¨¡å—**ã€‚

---

## ä¸‰ã€æ•°å­¦åŸç†è¯¦è§£

### 1. ç»å…¸è¿ç»­æ—¶é—´ SSMï¼ˆLTIï¼‰
è¿ç»­å½¢å¼ï¼š
$$
\begin{aligned}
\frac{d}{dt} \mathbf{h}(t) &= \mathbf{A} \mathbf{h}(t) + \mathbf{B} \mathbf{x}(t) \\
\mathbf{y}(t) &= \mathbf{C} \mathbf{h}(t)
\end{aligned}
$$

ç¦»æ•£åŒ–ï¼ˆä½¿ç”¨ Zero-Order Hold, ZOHï¼‰åï¼š
$$
\begin{aligned}
\mathbf{h}_t &= \bar{\mathbf{A}} \mathbf{h}_{t-1} + \bar{\mathbf{B}} \mathbf{x}_t \\
\mathbf{y}_t &= \mathbf{C} \mathbf{h}_t
\end{aligned}
$$
å…¶ä¸­ $\bar{\mathbf{A}} = e^{\Delta \mathbf{A}},\ \bar{\mathbf{B}} = (\int_0^\Delta e^{\tau \mathbf{A}} d\tau) \mathbf{B}$ï¼Œ$\Delta$ æ˜¯æ—¶é—´æ­¥é•¿ã€‚

ä½†è¿™æ˜¯**æ—¶ä¸å˜**çš„ï¼ˆA, B, C å›ºå®šï¼‰ï¼Œæ— æ³•é€‚åº”ä¸åŒè¾“å…¥ã€‚

---

### 2. Selective SSMï¼ˆMamba çš„æ ¸å¿ƒï¼‰

è®© **B, C, Î” æˆä¸ºè¾“å…¥ x çš„å‡½æ•°**ï¼š

$$
\begin{aligned}
\mathbf{z}_t &= \text{MLP}(\mathbf{x}_t) \quad \text{(ç”¨äºé—¨æ§)} \\
\Delta_t, \mathbf{B}_t, \mathbf{C}_t &= \text{Linear}(\mathbf{x}_t) \\
\bar{\mathbf{A}}_t &= \exp(\Delta_t \mathbf{A}) \quad (\mathbf{A} \text{ æ˜¯å¯å­¦ä¹ å¯¹è§’å¤çŸ©é˜µï¼Œé€šå¸¸åˆå§‹åŒ–ä¸ºè´Ÿå®æ•°}) \\
\bar{\mathbf{B}}_t &= (\Delta_t \cdot \mathbf{B}_t) \odot \phi(\Delta_t \mathbf{A}) \quad \text{ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ç”¨ç¦»æ•£åŒ–å…¬å¼ï¼‰}
\end{aligned}
$$

ç„¶åé€’å½’è®¡ç®—éšçŠ¶æ€ï¼š
$$
\mathbf{h}_t = \bar{\mathbf{A}}_t \mathbf{h}_{t-1} + \bar{\mathbf{B}}_t \mathbf{x}_t
$$
è¾“å‡ºï¼š
$$
\mathbf{y}_t = \mathbf{C}_t \mathbf{h}_t
$$

æœ€ååŠ ä¸€ä¸ª **SiLU æ¿€æ´» + é—¨æ§**ï¼š
$$
\text{output}_t = \mathbf{y}_t \odot \sigma(\mathbf{z}_t)
$$

> âœ… **å…³é”®ä¼˜åŠ¿**ï¼šå› ä¸º A æ˜¯å¯¹è§’çŸ©é˜µï¼ˆæˆ–å¯å¯¹è§’åŒ–ï¼‰ï¼Œæ•´ä¸ªé€’å½’å¯ä»¥**å¹¶è¡ŒåŒ–**ï¼ˆé€šè¿‡å…³è”æ‰«æ/parallel scanï¼‰ï¼Œå®ç° O(L) è®­ç»ƒï¼

#### SSMçš„è®¾è®¡æ€æƒ³

##### âœ… èƒŒæ™¯ï¼šSSM æ¥è‡ªäºâ€œè¿ç»­æ—¶é—´ç³»ç»Ÿâ€

çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æœ€åˆæ¥è‡ªæ§åˆ¶ç†è®ºï¼Œæè¿°çš„æ˜¯**è¿ç»­æ—¶é—´åŠ¨æ€ç³»ç»Ÿ**ï¼š

$$
\frac{d}{dt} h(t) = A h(t) + B x(t)
$$

è¿™æ˜¯ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œæè¿°äº†éšçŠ¶æ€ $ h(t) $ éšæ—¶é—´å˜åŒ–çš„æ–¹å¼ã€‚

ä½†æˆ‘ä»¬åœ¨åºåˆ—å»ºæ¨¡ä¸­å¤„ç†çš„æ˜¯**ç¦»æ•£æ—¶é—´ç‚¹** $ t=0,1,2,\dots,L $ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªè¿ç»­ç³»ç»Ÿâ€œç¦»æ•£åŒ–â€ã€‚

---

##### âœ… ç¦»æ•£åŒ–æ–¹æ³•ï¼šZero-Order Hold (ZOH)

åœ¨æ§åˆ¶é¢†åŸŸï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ **ZOH ç¦»æ•£åŒ–**ï¼Œå…¶ç»“æœä¸ºï¼š

$$
h_t = e^{\Delta A} h_{t-1} + \left( \int_0^\Delta e^{\tau A} d\tau \right) B x_t
$$

å…¶ä¸­ï¼š
- $ \Delta $ æ˜¯æ—¶é—´æ­¥é•¿ï¼ˆå›ºå®šæˆ–å¯å˜ï¼‰
- $ e^{\Delta A} $ æ˜¯çŸ©é˜µæŒ‡æ•°ï¼Œè¡¨ç¤ºçŠ¶æ€è¡°å‡/æ¼”åŒ–
- $ \int_0^\Delta e^{\tau A} d\tau $ æ˜¯ç§¯åˆ†é¡¹ï¼Œè¡¨ç¤ºè¾“å…¥å¦‚ä½•å½±å“çŠ¶æ€ï¼š$\Delta \cdot \exp(\tau A) \cdot B $ => $(\Delta \cdot B) \exp(\tau A) $

ğŸ‘‰ æ‰€ä»¥ï¼Œ**`exp(Î”A)` å°±æ˜¯è¿ç»­ç³»ç»Ÿçš„ç¦»æ•£ç‰ˆæœ¬**ï¼

> ğŸ’¡ Mamba æŠŠè¿™ä¸ªç¦»æ•£åŒ–è¿‡ç¨‹ç›´æ¥åµŒå…¥åˆ°äº†æ¨¡å‹é‡Œï¼š  
> - å®ƒè®© $ \Delta_t $ æˆä¸ºè¾“å…¥ $ x_t $ çš„å‡½æ•° â†’ å¯ä»¥**åŠ¨æ€è°ƒæ•´æ—¶é—´æ­¥é•¿**
> - å®ƒè®© $ A $ æ˜¯ä¸€ä¸ª**å¯å­¦ä¹ çš„å¯¹è§’å¤çŸ©é˜µ** â†’ æ§åˆ¶æ¯ä¸ªç»´åº¦çš„çŠ¶æ€è¡°å‡æ¨¡å¼

---

##### ğŸ§  ä¸ºä»€ä¹ˆè¦è®© A æ˜¯â€œå¯¹è§’å¤çŸ©é˜µâ€ï¼Ÿ

##### âœ… 1. å¯¹è§’çŸ©é˜µ â‡’ è®¡ç®—æ•ˆç‡é«˜

å¦‚æœ $ A $ æ˜¯å¯¹è§’çŸ©é˜µï¼Œæ¯”å¦‚ï¼š

$$
A = \text{diag}(\lambda_1, \lambda_2, ..., \lambda_d)
$$

é‚£ä¹ˆï¼š
- $ \exp(A) = \text{diag}(e^{\lambda_1}, e^{\lambda_2}, ...) $
- $ \int_0^\Delta e^{\tau A} d\tau = \text{diag}\left( \frac{e^{\Delta \lambda_i} - 1}{\lambda_i} \right) $

ğŸ‘‰ æ‰€æœ‰è¿ç®—éƒ½å¯ä»¥æŒ‰é€šé“ç‹¬ç«‹è®¡ç®—ï¼Œæ— éœ€çŸ©é˜µä¹˜æ³•ï¼

è¿™ä½¿å¾— SSM çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éƒ½å˜å¾—**æå…¶é«˜æ•ˆ**ï¼Œå¹¶ä¸”å¯ä»¥**å¹¶è¡ŒåŒ–**ã€‚

---

##### âœ… 2. å¤æ•°çŸ©é˜µ â‡’ æ¨¡æ‹ŸæŒ¯è¡è¡Œä¸ºï¼ˆå…³é”®ï¼ï¼‰

å‡è®¾ $ A $ ä¸åªæ˜¯å®æ•°ï¼Œè€Œæ˜¯**å¤æ•°å¯¹è§’çŸ©é˜µ**ï¼Œä¾‹å¦‚ï¼š

$$
\lambda_i = \alpha_i + i\beta_i
$$

é‚£ä¹ˆï¼š
- $ e^{\lambda_i t} = e^{\alpha_i t} \cdot e^{i\beta_i t} = e^{\alpha_i t} \cdot (\cos(\beta_i t) + i\sin(\beta_i t)) $

ğŸ‘‰ è¿™å°±å¼•å…¥äº†**æŒ¯è¡ï¼ˆoscillationï¼‰** è¡Œä¸ºï¼

è¿™æ„å‘³ç€ï¼šMamba çš„ SSM å¯ä»¥æ¨¡æ‹Ÿ**å‘¨æœŸæ€§ä¿¡å·**ã€**æ­£å¼¦æ³¢**ã€**é¢‘ç‡å“åº”**ç­‰å¤æ‚åŠ¨æ€ã€‚

ğŸ§  ç±»æ¯”ï¼šå°±åƒ RNN ä¸­çš„ LSTM å¯ä»¥è®°ä½é•¿æœŸä¾èµ–ï¼Œä½† Mamba çš„ SSM å¯ä»¥é€šè¿‡å¤æ•°ç‰¹å¾å€¼å®ç°â€œè®°å¿†+æŒ¯è¡â€ï¼Œæ›´é€‚åˆå»ºæ¨¡è¯­éŸ³ã€éŸ³ä¹ã€ç”Ÿç‰©ä¿¡å·ç­‰å…·æœ‰å‘¨æœŸæ€§çš„æ•°æ®ã€‚

> ğŸ“Œ å®é™…ä¸Šï¼ŒMamba çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§å¤æ•° A åœ¨è¯­éŸ³ã€åŸºå› ç»„åºåˆ—ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚


---

### 3. å¹¶è¡ŒåŒ–æŠ€å·§ï¼ˆç®€è¿°ï¼‰

è™½ç„¶é€’å½’å½¢å¼æ˜¯ä¸²è¡Œçš„ï¼Œä½† Mamba åˆ©ç”¨ **associative scan** æŠ€å·§å°†é€’å½’è½¬åŒ–ä¸ºå¯å¹¶è¡Œçš„â€œç®—å­ç»„åˆâ€ï¼Œç±»ä¼¼ï¼š
$$
(h_t) = f_t \circ f_{t-1} \circ \cdots \circ f_1 (h_0)
$$
å…¶ä¸­æ¯ä¸ª $f_t(h) = \bar{A}_t h + \bar{B}_t x_t$ æ˜¯ä¸€ä¸ªä»¿å°„å˜æ¢ã€‚é€šè¿‡å®šä¹‰åˆé€‚çš„ç»“åˆå¾‹æ“ä½œï¼Œå¯ç”¨ `torch_scan` æˆ– CUDA kernel å¹¶è¡Œè®¡ç®—ã€‚

> å®é™…å®ç°ä¸­ï¼ŒMamba ä½¿ç”¨äº†å®šåˆ¶ CUDA kernelï¼ˆå¦‚ `selective_scan_cuda`ï¼‰æ¥åŠ é€Ÿã€‚

---

## å››ã€PyTorch ç®€åŒ– Demoï¼ˆCPU å‹å¥½ç‰ˆï¼‰

> æ³¨æ„ï¼šå®Œæ•´ Mamba ä¾èµ– CUDA kernel æ‰é«˜æ•ˆã€‚è¿™é‡Œæˆ‘ä»¬ç”¨ **æœ´ç´ é€’å½’å®ç°**ï¼ˆO(L) æ¨ç†ï¼Œä½†è®­ç»ƒä¸å¯å¹¶è¡Œï¼‰ï¼Œä»…ç”¨äºç†è§£é€»è¾‘ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = d_model * expand

        # Input projections
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)
        
        # Convolution for local context (optional but used in Mamba)
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            bias=True
        )

        # SSM parameters projection (Î”, B, C)
        self.x_proj = nn.Linear(self.d_inner, d_state + d_state + 1, bias=False)  # B, C, Î”
        self.dt_proj = nn.Linear(1, self.d_inner, bias=True)

        # A matrix: learnable diagonal real parts (negative init)
        A = torch.arange(1, d_state + 1, dtype=torch.float32).view(1, -1)
        self.A_log = nn.Parameter(torch.log(A))  # shape (1, d_state)
        self.D = nn.Parameter(torch.ones(self.d_inner))

        # Output projection
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=True)

    def forward(self, x):
        # x: (B, L, D)
        B, L, D = x.shape
        x_and_z = self.in_proj(x)  # (B, L, 2*d_inner)
        x, z = x_and_z.chunk(2, dim=-1)  # each (B, L, d_inner)

        # Optional convolution for local mixing
        x = x.transpose(1, 2)  # (B, d_inner, L)
        x = self.conv1d(x)[..., :L]  # causal padding
        x = x.transpose(1, 2)  # (B, L, d_inner)
        x = F.silu(x)

        # Discretization and SSM
        A = -torch.exp(self.A_log.float())  # (1, d_state)
        y = self.ssm_step(x, A)  # (B, L, d_inner)

        # Gating
        y = y * F.silu(z)

        output = self.out_proj(y)
        return output

    def ssm_step(self, x, A):
        # x: (B, L, d_inner)
        # A: (1, d_state)
        B_, L, D = x.shape
        d_state = A.shape[1]

        # Project to get B, C, log(Î”)
        deltaBC = self.x_proj(x.view(-1, D))  # (B*L, d_state*2 + 1)
        delta, B_proj, C_proj = torch.split(deltaBC, [1, d_state, d_state], dim=-1)
        delta = delta.view(B_, L, 1)  # (B, L, 1)
        B_proj = B_proj.view(B_, L, d_state)  # (B, L, d_state)
        C_proj = C_proj.view(B_, L, d_state)  # (B, L, d_state)

        # Compute Î” from log(Î”) via softplus
        delta = F.softplus(self.dt_proj.weight * delta + self.dt_proj.bias)  # (B, L, d_inner)

        # Expand A to (d_inner, d_state) â€” assume A shared per channel
        A = A.repeat(D, 1)  # (d_inner, d_state)

        # Initialize hidden state
        h = torch.zeros(B_, d_state, device=x.device)  # (B, d_state)
        ys = []

        for t in range(L):
            xt = x[:, t, :]  # (B, d_inner)
            dt = delta[:, t, :]  # (B, d_inner)
            Bt = B_proj[:, t, :]  # (B, d_state)
            Ct = C_proj[:, t, :]  # (B, d_state)

            # Discretize A and B
            Ad = torch.exp(dt.unsqueeze(-1) * A)  # (B, d_inner, d_state)
            Bd = (dt.unsqueeze(-1) * Bt.unsqueeze(1))  # (B, d_inner, d_state)

            # Update hidden state: h = Ad * h + Bd * xt.unsqueeze(-1)
            # But h is (B, d_state), so we need to broadcast
            # We'll compute per-channel SSM
            ut = xt.unsqueeze(-1)  # (B, d_inner, 1)
            h = h.unsqueeze(1)  # (B, 1, d_state)
            h = Ad * h + Bd * ut  # (B, d_inner, d_state)
            h = h.sum(dim=-1)  # ??? â† This is a simplification; real Mamba keeps d_state per channel

            # Actually, standard implementation treats each of d_inner as independent SSM with d_state dim
            # For simplicity, we collapse â€” this demo is conceptual only!

            yt = (h * Ct).sum(dim=-1)  # (B, d_inner)
            ys.append(yt)
            h = h.mean(dim=1)  # crude hack to keep shape

        y = torch.stack(ys, dim=1)  # (B, L, d_inner)
        return y


# Simple test
if __name__ == "__main__":
    model = MambaBlock(d_model=64)
    x = torch.randn(2, 10, 64)
    y = model(x)
    print("Input shape:", x.shape)
    print("Output shape:", y.shape)
```

> âš ï¸ æ³¨æ„ï¼šä¸Šè¿°ä»£ç æ˜¯**æ•™å­¦ç®€åŒ–ç‰ˆ**ï¼ŒçœŸå® Mamba å®ç°æ›´å¤æ‚ï¼Œå°¤å…¶æ˜¯ï¼š
> - æ¯ä¸ª `d_inner` é€šé“æœ‰è‡ªå·±çš„ SSMï¼ˆå³ `d_inner` ä¸ªå¹¶è¡Œ SSMï¼Œæ¯ä¸ªç»´åº¦ä¸º `d_state`ï¼‰
> - ä½¿ç”¨ CUDA kernel å®ç°å¹¶è¡Œæ‰«æ
> - æ­£ç¡®çš„ç¦»æ•£åŒ–ï¼ˆå¦‚ bilinear transformï¼‰
>
> å®˜æ–¹å®ç°è§ï¼šhttps://github.com/state-spaces/mamba

---

## äº”ã€æ€»ç»“

| ç‰¹æ€§ | Transformer | Mamba |
|------|-------------|-------|
| å¤æ‚åº¦ | $O(L^2)$ | $O(L)$ |
| ä¸Šä¸‹æ–‡æ„ŸçŸ¥ | é€šè¿‡ attention | é€šè¿‡ input-dependent SSM |
| å¹¶è¡Œè®­ç»ƒ | å®Œå…¨å¹¶è¡Œ | é€šè¿‡ associative scan å¹¶è¡Œ |
| é•¿åºåˆ—æ€§èƒ½ | å·® | ä¼˜ç§€ |
| ç¡¬ä»¶å‹å¥½æ€§ | é«˜ï¼ˆä½†æ˜¾å­˜ç“¶é¢ˆï¼‰ | æ›´é«˜ï¼ˆçº¿æ€§æ˜¾å­˜ï¼‰ |

Mamba åœ¨è¯­è¨€å»ºæ¨¡ã€DNA åºåˆ—ã€éŸ³é¢‘ç­‰é¢†åŸŸå·²å±•ç°å‡ºè¶…è¶Š Transformer çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨**é•¿ä¸Šä¸‹æ–‡**åœºæ™¯ã€‚

---
